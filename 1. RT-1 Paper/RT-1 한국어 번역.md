# RT-1: Robotics Transformer for Real-World Control at Scale

## 1. 서론

강화학습이나 모방학습을 사용하는 **end-to-end 로봇 학습**은 일반적으로 단일 태스크 또는 멀티태스크 환경에서 태스크별 데이터를 수집하는 방식으로 진행되며, 이 데이터는 로봇이 수행해야 할 태스크에 맞춰 좁은 범위로 구성됩니다. 이러한 워크플로우는 컴퓨터 비전이나 자연어 처리 같은 다른 분야의 전통적인 지도학습 접근 방식과 유사합니다. 과거에는 태스크별로 데이터셋을 수집하고, 레이블링하고, 개별 태스크를 해결하기 위해 배포했으며, 태스크 간의 상호작용은 거의 없었습니다.

최근 몇 년간 비전, 자연어 처리 및 기타 분야에서는 분리되고 소규모인 데이터셋과 모델에서 벗어나, **광범위하고 대규모인 데이터셋으로 사전학습된 대형 범용 모델**로의 전환이 이루어졌습니다. 이러한 모델의 성공 핵심은 개방형 태스크 비의존적 학습과 대규모 데이터셋에 담긴 모든 지식을 흡수할 수 있는 고용량 아키텍처의 결합에 있습니다. 모델이 언어나 인식의 일반적인 패턴을 학습하기 위해 경험을 "스펀지처럼 흡수"할 수 있다면, 이를 개별 태스크에 더 효율적으로 적용할 수 있습니다.

대규모 태스크별 데이터셋의 필요성을 없애는 것은 일반적인 지도학습에서도 매력적이지만, **로보틱스에서는 더욱 중요**합니다. 로보틱스에서 데이터셋을 구축하려면 엔지니어링 집약적인 자율 운영이나 비용이 많이 드는 사람의 시연이 필요할 수 있기 때문입니다.

따라서 우리는 다음과 같은 질문을 던집니다:

> **다양한 로봇 태스크로 구성된 데이터로 단일하고 유능한 대규모 멀티태스크 백본 모델을 학습시킬 수 있는가? 그리고 그러한 모델이 다른 분야에서 관찰된 이점, 즉 새로운 태스크, 환경, 객체에 대한 zero-shot 일반화를 보여줄 수 있는가?**

### 1.1 주요 과제

로보틱스에서 이러한 모델을 구축하는 것은 쉽지 않습니다. 최근 몇 년간 여러 대규모 멀티태스크 로봇 정책이 문헌에서 제안되었지만, 이러한 모델들은 종종 Gato처럼 실제 환경 태스크의 범위가 제한적이거나, 최근의 instruction following 방법들처럼 새로운 태스크로의 일반화보다는 학습 태스크에 초점을 맞추거나, 새로운 태스크에서 비교적 낮은 성능을 보입니다.

두 가지 주요 과제는 **적절한 데이터셋을 구성하는 것**과 **적절한 모델을 설계하는 것**입니다.

#### 과제 1: 데이터 수집과 큐레이션

데이터 수집과 큐레이션은 많은 대규모 머신러닝 프로젝트의 "숨은 영웅"이지만, 로보틱스에서는 이것이 특히 중요합니다. 로보틱스의 데이터셋은 종종 로봇별로 특화되어 있고 수동으로 수집되기 때문입니다. 우리의 평가에서 보여주듯이, 좋은 일반화를 위해서는 다양한 태스크와 설정을 포괄하는 **규모와 다양성을 모두 갖춘 데이터셋**이 필요합니다. 동시에, 데이터셋의 태스크들은 일반화를 가능하게 할 만큼 충분히 잘 연결되어 있어야 합니다. 이를 통해 모델이 구조적으로 유사한 태스크들 간의 패턴을 발견하고, 이러한 패턴을 새로운 방식으로 조합하여 새로운 태스크를 수행할 수 있습니다.

우리는 **17개월에 걸쳐 13대의 로봇 fleet으로 수집한 데이터셋**을 활용합니다. 이 데이터셋은 약 **130,000개의 에피소드**와 **700개 이상의 태스크**를 포함하며, 우리는 평가에서 이 데이터셋의 다양한 측면을 분석합니다.

#### 과제 2: 모델 설계

두 번째 과제는 모델 자체의 설계에 있습니다. 효과적인 로봇 멀티태스크 학습은 고용량 모델을 필요로 하며, **Transformer 모델**은 이 점에서 뛰어납니다. 특히 우리의 경우처럼 언어 지시에 조건화된 많은 태스크를 학습해야 할 때 그렇습니다. 그러나 로봇 컨트롤러는 **실시간으로 실행될 수 있을 만큼 효율적**이어야 하며, 이것이 특히 Transformer에게는 큰 도전입니다.

### 1.2 RT-1 아키텍처

우리는 **RT-1(Robotics Transformer 1)**이라고 부르는 새로운 아키텍처를 제안합니다. 이 아키텍처는 카메라 이미지, 지시문, 모터 명령을 포함한 고차원 입출력을 Transformer가 사용할 수 있는 **컴팩트한 토큰 표현으로 인코딩**함으로써, 런타임에 효율적인 추론을 가능하게 하여 실시간 제어를 실현합니다.

우리의 기여는 RT-1 모델과 실제 환경 로봇 태스크의 대규모 다양한 데이터셋에서 이 모델을 사용한 실험입니다. 우리의 실험은 RT-1이 기존 기법들에 비해 크게 향상된 일반화와 강건성을 보여줄 수 있음을 증명할 뿐만 아니라, 모델과 학습 세트 구성의 많은 설계 선택을 평가하고 분석합니다.

### 1.3 주요 결과

- RT-1은 **700개 이상의 학습 지시문을 97% 성공률**로 수행할 수 있음
- 새로운 태스크, 방해물, 배경에 대해 각각 **25%, 36%, 18% 더 나은 일반화 성능** (차순위 베이스라인 대비)
- 이러한 성능 수준 덕분에 SayCan 프레임워크에서 **최대 50단계에 달하는 매우 긴 시간 범위의 태스크**를 실행할 수 있음
- RT-1은 **시뮬레이션이나 심지어 다른 로봇 유형의 데이터도 통합**할 수 있으며, 원래 태스크에 대한 성능을 유지하면서 새로운 시나리오에 대한 일반화를 향상시킴

---

## 2. 관련 연구

최근 여러 연구들이 로봇 제어를 위한 **Transformer 기반 정책**을 제안했습니다. RT-1처럼 여러 연구들은 Transformer로 처리된 언어 명령을 새로운 태스크에 대한 명세화 및 일반화를 위한 강건한 프레임워크로 사용합니다. 우리 연구는 Transformer의 적용을 한 단계 더 발전시켜, **언어와 비전 관측을 로봇 행동으로 매핑하는 것을 시퀀스 모델링 문제**로 다루며, Transformer를 사용하여 이 매핑을 학습합니다. 이 아이디어는 게임 플레이에서의 성공과 시뮬레이션 로봇 내비게이션, 이동, 조작 환경에서 직접 영감을 받았습니다.

Transformer 기반 정책을 넘어서, 우리 연구의 초점은 **대규모의 일반화 가능하고 강건한 실제 환경 로봇 조작**에 있습니다. 실제 환경 Transformer 기반 로봇 조작에 관한 기존 연구들은 태스크당 시연 세트로부터 효율적으로 태스크를 학습하는 데 초점을 맞춥니다. Behavior Transformer와 Gato는 대규모 로봇 및 비로봇 데이터셋에서 단일 모델을 학습하는 것을 지지합니다. 그러나 이러한 연구들은 실제 환경 로봇 태스크에서 제한적입니다. 예를 들어, Gato는 효과적으로 단일 태스크(색깔 블록 쌓기)만 학습하며, 새로운 태스크나 다양한 실제 환경 설정에 대한 일반화를 평가하지 않습니다.

고용량 Transformer 모델을 사용한 로봇 제어 정책 학습은 비교적 최근의 혁신이지만, 로보틱스는 **멀티태스크 및 언어 조건화 학습의 오랜 역사**를 가지고 있으며, RT-1은 이러한 기반 위에 구축됩니다. 우리 연구는 멀티태스크, 언어 조건화 로봇 학습의 힘을 더욱 뒷받침하는 증거를 추가하며, 더 큰 규모와 더 다양한 행동, 객체, 장면으로 실험 결과를 제시하고, 상당히 더 큰 규모에서 로봇 학습을 가능하게 하는 새로운 아키텍처와 설계 선택을 제안합니다.

---

## 3. 사전 지식

### 3.1 로봇 학습

우리는 **비전으로부터 언어 조건화 태스크를 해결하기 위한 로봇 정책**을 학습하는 것을 목표로 합니다. 

**형식적 정의:**
- 타임스텝 t = 0에서, 정책 π는 언어 지시문 i와 초기 이미지 관측 x₀를 받습니다
- 정책은 행동 분포 π(· | i, x₀)를 생성하고, 행동 a₀가 샘플링되어 로봇에 적용됩니다
- 정책은 학습된 분포 π(· | i, {xⱼ}ᵗⱼ₌₀)에서 샘플링하여 반복적으로 행동 aₜ를 생성합니다
- 시작 스텝 t = 0부터 종료 스텝 T까지의 전체 상호작용 i, {(xⱼ, aⱼ)}ᵀⱼ₌₀를 **에피소드**라고 합니다
- 에피소드가 끝나면, 에이전트는 로봇이 지시문 i를 수행했는지 여부를 나타내는 이진 보상 r ∈ {0, 1}을 받습니다

**목표:** 지시문 분포, 초기 상태 x₀, 전이 역학에 대한 기대값에서 평균 보상을 최대화하는 정책 π를 학습

### 3.2 Transformer

RT-1은 **Transformer를 사용하여 정책 π를 파라미터화**합니다. 일반적으로 말해서, Transformer는 **self-attention 레이어와 완전 연결 신경망의 조합**을 사용하여 입력 시퀀스 {ξₕ}ᴴₕ₌₀를 출력 시퀀스 {yₖ}ᴷₖ₌₀로 매핑하는 시퀀스 모델입니다. 

Transformer는 원래 텍스트 시퀀스를 위해 설계되었으며, 각 입력 ξⱼ와 출력 yₖ는 텍스트 토큰을 나타내지만, 이미지와 다른 모달리티로도 확장되었습니다. 우리는 먼저 입력 i, {xⱼ}ᵗⱼ₌₀를 시퀀스 {ξₕ}ᴴₕ₌₀로, 행동 출력 aₜ를 시퀀스 {yₖ}ᴷₖ₌₀로 매핑한 후 Transformer를 사용하여 매핑 {ξₕ}ᴴₕ₌₀ → {yₖ}ᴷₖ₌₀를 학습함으로써 π를 파라미터화합니다.

### 3.3 모방 학습

모방 학습 방법은 **시연 데이터셋 D에서 정책 π를 학습**합니다. 구체적으로, 우리는 모두 성공적인(즉, 최종 보상이 1인) 에피소드들의 데이터셋 D = {(i⁽ⁿ⁾, {(x⁽ⁿ⁾ₜ, a⁽ⁿ⁾ₜ)}ᵀ⁽ⁿ⁾ₜ₌₀)}ᴺₙ₌₀에 접근할 수 있다고 가정합니다. 

우리는 **behavioral cloning**을 사용하여 π를 학습하며, 이는 이미지와 언어 지시문이 주어졌을 때 행동의 음의 로그 우도를 최소화함으로써 π를 최적화합니다.

---

## 4. 시스템 개요

이 연구의 목표는 **대량의 데이터를 흡수하고 효과적으로 일반화할 수 있는 범용 로봇 학습 시스템**을 구축하고 시연하는 것입니다. 

### 4.1 하드웨어 플랫폼

우리는 Everyday Robots의 **모바일 매니퓰레이터**를 사용하며, 이 로봇은:
- 7 자유도 팔
- 2개의 손가락 그리퍼
- 모바일 베이스

를 가지고 있습니다(그림 2(d) 참조).

### 4.2 실험 환경

데이터를 수집하고 우리의 방법을 평가하기 위해, 우리는 **세 가지 주방 기반 환경**을 사용합니다:

1. **학습 환경** (그림 2(a)): 부분적인 조리대로 구성되어 있으며 대규모 데이터 수집을 위해 구축
2. **실제 사무실 주방 1** (그림 2(b)): 학습 환경과 유사한 조리대를 가지지만, 조명, 배경, 전체 주방 구조가 다름
3. **실제 사무실 주방 2** (그림 2(c)): 예를 들어 서랍 대신 캐비닛이 있거나 싱크대가 보일 수 있음

우리는 이러한 다른 환경들에 걸쳐 정책의 성능을 평가하며, 정책의 성능과 일반화 능력을 측정합니다.

### 4.3 학습 데이터

우리의 학습 데이터는 **사람이 제공한 시연**으로 구성되며, 각 에피소드에 로봇이 방금 수행한 지시문의 텍스트 설명을 주석으로 달습니다. 

**지시문 구조:**
- 동사: "pick", "open", "place upright" 등
- 명사: "coke can", "apple", "drawer" 등

**데이터셋 규모:**
- **130,000개 이상의 개별 시연**
- **700개 이상의 고유한 태스크 지시문**

### 4.4 RT-1 아키텍처 개요

우리 시스템의 주요 기여 중 하나는 네트워크 아키텍처인 **Robotics Transformer 1(RT-1)**입니다. RT-1은:
- 대량의 데이터를 흡수
- 효과적으로 일반화
- **실시간 속도로 행동 출력** (실용적인 로봇 제어를 위해)

**입력:** 짧은 이미지 시퀀스 + 자연어 지시문  
**출력:** 각 타임스텝에서 로봇에 대한 행동

**아키텍처 구성 요소:**
1. **FiLM 조건화된 ImageNet 사전학습 합성곱 네트워크**: 이미지와 텍스트 처리
2. **Token Learner**: 컴팩트한 토큰 집합 계산
3. **Transformer**: 토큰에 대한 어텐션 수행 및 이산화된 행동 토큰 생성

**행동 구성:**
- 팔 움직임: 7개 차원 (x, y, z, roll, pitch, yaw, 그리퍼 열림)
- 베이스 움직임: 3개 차원 (x, y, yaw)
- 모드 전환: 이산 차원 (팔 제어, 베이스 제어, 에피소드 종료)

RT-1은 **폐루프 제어**를 수행하며 "terminate" 행동을 출력하거나 사전 설정된 타임스텝 제한에 도달할 때까지 **3Hz로 행동을 명령**합니다.

---

## 5. RT-1: Robotics Transformer

이 섹션에서는 이미지, 텍스트, 행동을 토큰화하는 방법을 설명하고, RT-1 모델 아키텍처를 논의합니다. 그런 다음 실시간 제어에 필요한 런타임 속도를 달성하는 방법을 설명합니다. 마지막으로, 데이터 수집 절차와 우리 데이터셋의 스킬 및 지시문을 설명합니다.

### 5.1 모델

우리 모델은 **Transformer 아키텍처**를 기반으로 구축되며, 이미지 히스토리와 태스크 설명을 입력으로 받아 **토큰화된 행동을 직접 출력**합니다(그림 1a와 그림 3의 상세 그림 참조).

#### 5.1.1 지시문 및 이미지 토큰화

RT-1 아키텍처는 이미지와 언어 지시문의 **데이터 효율적이고 컴팩트한 토큰화**에 의존합니다.

**이미지 처리:**
- **ImageNet 사전학습된 EfficientNet-B3** 모델 사용
- 입력: 해상도 300×300의 6개 이미지
- 출력: 9×9×512 형태의 공간 특징 맵
- 81개의 시각 토큰으로 평탄화

**언어 지시문 통합:**
- 사전학습된 **Universal Sentence Encoder**를 통해 지시문 임베딩
- **항등 초기화 FiLM 레이어**를 통해 이미지 인코더를 조건화
- FiLM 아핀 변환을 생성하는 dense 레이어(fc와 hC)의 가중치를 **0으로 초기화**하여, FiLM 레이어가 처음에는 항등 함수로 작동

**토큰화 결과:**
- 총 **1,600만 개의 파라미터**
- 26개 레이어의 MBConv 블록과 FiLM 레이어
- **81개의 vision-language 토큰** 출력

#### 5.1.2 TokenLearner

RT-1이 어텐션해야 하는 토큰 수를 더욱 압축하여 **추론 속도를 높이기** 위해, RT-1은 **TokenLearner**를 사용합니다.

**기능:**
- 많은 수의 토큰을 훨씬 적은 수의 토큰으로 매핑하는 것을 학습
- 요소별 어텐션 모듈
- 정보에 기반하여 이미지 토큰을 소프트 선택
- 81개의 시각 토큰 → **8개의 최종 토큰**으로 서브샘플링

#### 5.1.3 Transformer

**입력:**
- 이미지당 8개의 토큰 × 6개 이미지 = 48개의 토큰
- 위치 인코딩 추가

**구조:**
- 8개의 self-attention 레이어
- 총 **1,900만 개의 파라미터**
- Decoder-only 시퀀스 모델

**출력:**
- 행동 토큰

#### 5.1.4 행동 토큰화

**이산화 방법:**
- 각 행동 차원: **256개의 빈**으로 이산화
- 빈은 각 변수의 범위 내에서 균일하게 분포

**행동 차원:**
- 팔 움직임: 7개 변수 (x, y, z, roll, pitch, yaw, 그리퍼 열림)
- 베이스 움직임: 3개 변수 (x, y, yaw)
- 모드: 이산 변수 (팔, 베이스, 에피소드 종료 제어)

#### 5.1.5 손실 함수

표준 **범주형 교차 엔트로피 목표**와 **인과적 마스킹** 사용

#### 5.1.6 추론 속도

**요구 사항:**
- 실제 로봇에서 **실시간으로 실행**
- 최소 **3Hz 제어 주파수**
- 모델의 추론 시간 예산: **100ms 미만**

**최적화 기법:**
1. **TokenLearner 사용**: 토큰 수 감소 → **2.4배 가속**
2. **토큰 재사용**: 한 번만 계산하고 후속 윈도우에 재사용 → **1.7배 가속**

### 5.2 데이터

우리의 목표는 **높은 성능, 새로운 태스크에 대한 일반화, 그리고 방해물과 배경에 대한 강건성**을 보여주는 시스템을 구축하는 것입니다.

**데이터셋 규모:**
- 17개월에 걸쳐 13대의 로봇 fleet으로 수집
- 약 **130,000개의 로봇 시연**
- 로봇 교실이라고 부르는 일련의 사무실 주방 세그먼트에서 수집

#### 5.2.1 스킬 및 지시문

| 스킬 | 개수 | 설명 | 예시 지시문 |
|------|------|------|-------------|
| Pick Object | 130 | 객체를 표면에서 들어올리기 | pick iced tea can |
| Move Object Near Object | 337 | 첫 번째 객체를 두 번째 객체 근처로 이동 | move pepsi can near rxbar blueberry |
| Place Object Upright | 8 | 긴 객체를 세우기 | place water bottle upright |
| Knock Object Over | 8 | 긴 객체를 쓰러뜨리기 | knock redbull can over |
| Open Drawer | 3 | 캐비닛 서랍 열기 | open the top drawer |
| Close Drawer | 3 | 캐비닛 서랍 닫기 | close the middle drawer |
| Place Object into Receptacle | 84 | 객체를 용기에 넣기 | place brown chip bag into white bowl |
| Pick Object from Receptacle and Place on Counter | 162 | 위치에서 객체를 집어 조리대에 놓기 | pick green jalapeno chip bag from paper bowl and place on counter |
| 섹션 6.3 및 6.4 태스크 | 9 | 현실적이고 긴 지시문을 위해 학습된 스킬 | open the large glass jar of pistachios, pull napkin out of dispenser, grab scooper |
| **총계** | **744** | | |

**표 1:** RT-1을 위해 수집된 스킬 목록과 설명 및 예시 지시문

**현재 스킬 세트:**
- 집기, 놓기
- 서랍 열고 닫기
- 서랍에서 물건 꺼내고 넣기
- 긴 물건 세우기, 쓰러뜨리기
- 냅킨 뽑기, 병 열기

**확장성:**
- 새로운 지시문을 추가할 때 특정 스킬에 대한 가정을 하지 않음
- 시스템은 쉽게 확장 가능
- 기능을 개선하기 위해 더 다양한 데이터를 지속적으로 제공 가능

---

## 6. 실험

우리의 실험은 다음 질문들에 답하고자 합니다:

1. RT-1이 많은 수의 지시문을 수행하고, 새로운 태스크, 객체, 환경에 zero-shot으로 일반화하는 것을 학습할 수 있는가? (섹션 6.2)
2. 시뮬레이션 데이터나 다른 로봇의 데이터와 같은 이종 데이터 소스를 통합하여 결과 모델을 더욱 발전시킬 수 있는가? (섹션 6.3)
3. 다양한 방법들이 긴 시간 범위 로봇 시나리오에 어떻게 일반화하는가? (섹션 6.4)
4. 데이터 양과 데이터 다양성의 변화에 따라 일반화 메트릭이 어떻게 변하는가? (섹션 6.5)
5. 모델 설계에서 중요하고 실용적인 결정은 무엇이며 이것이 성능과 일반화에 어떻게 영향을 미치는가? (부록 섹션 D.4)

### 6.1 실험 설정

이 섹션 전체에서 우리는 두 가지 baseline 최신 아키텍처인 **Gato**와 **BC-Z**와 비교합니다. 중요하게도, 이 두 가지 모두 섹션 5.2에서 자세히 설명한 우리 데이터(우리 시스템의 중요한 부분)로 학습되었습니다.

#### Baseline 모델 설명

**Gato:**
- RT-1과 유사하게 Transformer 아키텍처 기반
- 차이점:
  - 언어 개념 없이 이미지 토큰을 계산
  - 각 이미지 토큰 임베딩은 각 이미지 패치에 대해 별도로 계산
  - 사전학습된 텍스트 임베딩을 사용하지 않음
  - TokenLearner와 자기회귀 행동 제거 미포함
- 모델 크기: 3,700만 파라미터 (원래 12억 → RT-1과 유사한 크기로 제한)

**BC-Z:**
- ResNet 아키텍처 기반
- SayCan에서 사용됨
- 차이점:
  - 이전 타임스텝을 사용하지 않는 피드포워드 모델
  - 이산 행동 토큰 대신 연속 행동 사용
- **BC-Z XL**: RT-1과 유사한 파라미터 수를 가진 더 큰 버전

#### 평가 환경

**세 가지 환경:**
1. **학습 환경** (그림 2(a)): 부분적인 조리대
2. **실제 사무실 주방 1** (그림 2(b)): 조명, 배경, 구조가 다름
3. **실제 사무실 주방 2** (그림 2(c)): 서랍 대신 캐비닛, 싱크대 등

#### 평가 카테고리

**1. 학습된 태스크 성능**
- 학습 세트에서 샘플링된 지시문 평가
- 200개 이상의 태스크 테스트:
  - 객체 집기: 36개
  - 객체 쓰러뜨리기: 35개
  - 세우기: 35개
  - 객체 이동: 48개
  - 서랍 열고 닫기: 18개
  - 서랍에서 꺼내고 넣기: 36개

**2. 보지 못한 태스크 일반화**
- 21개의 새로운, 보지 못한 지시문 테스트
- 각 객체와 스킬의 최소 일부 인스턴스가 학습 세트에 있었지만 새로운 방식으로 조합

**3. 강건성**
- **방해물 강건성**: 30개의 실제 환경 태스크
- **배경 강건성**: 22개의 태스크
  - 새로운 주방 (다른 조명과 배경 시각)
  - 다른 조리대 표면 (예: 무늬가 있는 테이블보)

**4. 긴 시간 범위 시나리오**
- 두 개의 실제 주방에서 15개의 긴 시간 범위 지시문
- 약 10개의 고유한 단계로 구성된 스킬 시퀀스
- SayCan 시스템 사용

**평가 규모:**
- 3,000회 이상의 실제 환경 시도
- 지금까지 로봇 학습 시스템의 **가장 대규모 평가 중 하나**

### 6.2 RT-1이 많은 수의 지시문을 수행하고, 새로운 태스크, 객체, 환경에 일반화하는 것을 학습할 수 있는가?

첫 번째 질문에 답하기 위해, 이전에 제안된 모델과 비교하여 RT-1의 전반적인 성능, 일반화, 강건성 능력을 분석합니다.

#### 주요 결과

| 모델 | 학습된 태스크 | 보지 못한 태스크 | 방해물 | 배경 |
|------|------------|----------------|--------|------|
| Gato | 65% | 52% | 43% | 35% |
| BC-Z | 72% | 19% | 47% | 41% |
| BC-Z XL | 56% | 43% | 23% | 35% |
| **RT-1 (ours)** | **97%** | **76%** | **83%** | **59%** |

**표 2:** RT-1과 baseline의 학습된 태스크, 보지 못한 태스크에 대한 일반화, 방해물과 배경에 대한 강건성 전반적 성능

#### 분석

**학습된 태스크:**
- RT-1은 200개 이상의 지시문 중 **97% 성공률**
- BC-Z보다 25%, Gato보다 32% 더 높음

**보지 못한 태스크:**
- 새로운 지시문의 **76%를 수행**
- 차순위 baseline보다 24% 더 높음
- 정책의 자연어 조건화 덕분에 가능

**강건성:**
- 방해물 강건성 태스크: **83% 성공률** (차순위 대비 +36%)
- 배경 강건성 태스크: **59% 성공률** (차순위 대비 +18%)

#### 현실적인 지시문에 대한 일반화

**실제 주방 시나리오:**
- 서랍에 여러 간식 재입고
- 사람이 쓰러뜨린 조미료 병 정리
- 오렌지와 냅킨으로 간식 준비
- 주방의 여러 장소에서 잃어버린 물건 가져오기

**일반화 수준:**
- **L1**: 새로운 조리대 레이아웃과 조명 조건
- **L2**: 추가로 보지 못한 방해물 객체
- **L3**: 추가로 급격히 새로운 태스크 설정, 새로운 태스크 객체 또는 보지 못한 위치의 객체

| 모델 | 전체 | L1 | L2 | L3 |
|------|------|-----|-----|-----|
| Gato | 30% | 63% | 25% | 0% |
| BC-Z | 45% | 38% | 50% | 50% |
| BC-Z XL | 55% | 63% | 75% | 38% |
| **RT-1 (ours)** | **70%** | **88%** | **75%** | **50%** |

**표 3:** 현실적인 일반화 시나리오 - 세 가지 일반화 수준에 걸쳐 현실적인 Google 주방 시나리오에서 모델 성공률 비교

### 6.3 시뮬레이션이나 다른 로봇의 데이터와 같은 이종 데이터 소스를 통합하여 결과 모델을 더욱 발전시킬 수 있는가?

다음으로, 매우 이종적인 데이터를 활용하기 위한 RT-1의 한계를 탐구합니다. RT-1이 매우 다른 데이터 소스로부터 통합하고 학습할 수 있으며, 이 데이터에 내재된 다양한 태스크에 걸쳐 원래 태스크 성능을 희생하지 않고 그러한 데이터로부터 개선될 수 있음을 시연합니다.

#### 6.3.1 시뮬레이션 데이터 흡수

**실험 설정:**
- 모든 실제 시연 데이터 + 로봇이 실제 환경에서 본 적 없는 객체를 포함하는 추가 시뮬레이션 데이터

| 모델 | 학습 데이터 | 실제 객체<br>(학습된 스킬) | 시뮬레이션 객체<br>(학습된 스킬) | 시뮬레이션 객체<br>(보지 못한 스킬) |
|------|----------|-------------------|--------------------------|----------------------------|
| RT-1 | Real Only | 92% | 23% | 7% |
| RT-1 | Real + Sim | 90% *(-2%)* | 87% *(+64%)* | 33% *(+26%)* |

**표 4:** RT-1에 시뮬레이션 데이터 통합 실험 결과

**분석:**
- 시뮬레이션 데이터 추가해도 **실제 객체 성능에 영향 없음** (-2% 미미한 감소)
- 시뮬레이션에서만 본 객체: 23% → **87% (+64%)** 크게 증가
- 보지 못한 지시문: 7% → **33% (+26%)** 증가
- **인상적인 도메인 전이** 시연

#### 6.3.2 다른 로봇의 데이터 흡수

**실험 설정:**
- **Kuka IIWA** + **Everyday Robots 모바일 매니퓰레이터**
- Kuka 데이터: QT-Opt에서 수집된 209,000개의 bin-picking 에피소드

| 모델 | 학습 데이터 | Classroom 평가 | Bin-picking 평가 |
|------|----------|---------------|-----------------|
| RT-1 | Kuka + EDR 데이터 | 90% *(-2%)* | 39% *(+17%)* |
| RT-1 | EDR only 데이터 | 92% | 22% |
| RT-1 | Kuka only 데이터 | 0% | 0% |

**표 5:** 두 개의 다른 로봇에서 데이터 혼합 실험 결과

**분석:**
- 원래 태스크 성능: **단 2% 감소** (Classroom 평가)
- Bin-picking 평가: 22% → **39% (+17%, 거의 2배)**
- Kuka only 데이터: **0% 성능** (다른 로봇 형태에서 행동 전이 어려움 확인)
- **효과적인 다중 로봇 전이** 시연
- RT-1의 흡수 속성이 **다른 로봇의 경험을 관찰하여 새로운 스킬 습득**하는 능력 포함

### 6.4 다양한 방법들이 긴 시간 범위 로봇 시나리오에 어떻게 일반화하는가?

다음 실험 세트에서 우리 방법이 긴 시간 범위의 현실적인 주방 설정에서 사용될 수 있을 만큼 충분히 일반화하는지 평가합니다. 이 질문에 답하기 위해, 두 개의 다른 실제 주방에서 **SayCan 프레임워크** 내에서 RT-1과 다양한 baseline을 실행합니다.

#### SayCan 프레임워크의 장점

- 상위 수준 지시문을 수행하기 위해 많은 하위 수준 지시문을 결합
- 가능한 상위 수준 지시문의 수는 스킬과 함께 **조합적으로 증가**
- 긴 시간 범위 태스크의 성공률은 태스크 길이에 따라 **기하급수적으로 감소** → 조작 스킬의 높은 성공률이 특히 중요
- 모바일 조작 태스크는 내비게이션과 조작을 모두 필요 → 베이스 위치에 대한 정책의 강건성이 중요

#### 결과

| 모델 | Kitchen1 SayCan 태스크<br>계획 | Kitchen1 SayCan 태스크<br>실행 | Kitchen2 SayCan 태스크<br>실행 |
|------|---------------------------|---------------------------|---------------------------|
| Original SayCan* | 73% | 47% | - |
| SayCan w/ Gato | 87% | 33% | 0% |
| SayCan w/ BC-Z | 87% | 53% | 13% |
| **SayCan w/ RT-1 (ours)** | **87%** | **67%** | **67%** |

**표 6:** Kitchen1과 Kitchen2에서의 SayCan 스타일 긴 시간 범위 태스크  
(*Original SayCan 평가는 약간 다른 프롬프트를 사용하므로 계획 성공률이 더 낮습니다.)

#### 분석

**Kitchen1:**
- 모든 방법 (Original SayCan 제외): 계획 성공률 87%
- RT-1: 실행 성공률 **67%** (가장 좋은 성능)

**Kitchen2 (훨씬 더 도전적):**
- Robot Classroom 학습 장면이 Kitchen1을 모델로 함
- Gato: **0% 성공률** (어떤 긴 시간 범위 태스크도 완료 못함)
- BC-Z: **13% 성공률**
- RT-1: **67% 성공률** (Kitchen1에서 Kitchen2로 눈에 띄게 감소하지 않음)

**놀라운 결과:**
- Kitchen2에서 보지 못한 서랍을 조작 가능
- SayCan-RT1을 사용하여 **최대 50단계에 달하는 초장기 시간 범위 태스크** 계획 및 실행 가능

### 6.5 데이터 양과 데이터 다양성의 변화에 따라 일반화 메트릭이 어떻게 변하는가?

이전 연구들이 모델 파라미터 수에 따른 Transformer 기반 모델의 스케일링 능력을 보여주었지만, 많은 로보틱스 연구에서 모델 크기는 종종 주요 병목이 아니며, 최대 크기는 실제 로봇에서 그러한 모델을 실행하기 위한 지연 요구 사항에 의해 제한됩니다. 

대신, 이 연구에서 우리는 전통적으로 데이터가 제한된 로봇 학습 분야에서 중요한 역할을 하는 **데이터셋 크기와 다양성의 영향**을 ablation하는 데 초점을 맞춥니다.

#### 실험 설정

**더 작은 데이터:**
- 가장 많은 데이터를 가진 태스크에서 데이터를 제거
- 동일한 태스크 다양성을 가진 더 작은 데이터셋 생성
- 태스크당 예제 수 제한:
  - 200개 (데이터의 51%)
  - 100개 (데이터의 37%)
  - 50개 (데이터의 22.5%)

**더 좁은 데이터:**
- 가장 적은 데이터를 가진 태스크 제거
- 전체 데이터의 97% 유지하면서 태스크의 75%만 유지

#### 결과

| 모델 | % 태스크 | % 데이터 | 학습된<br>태스크 | 전체 | 보지 못한<br>태스크 | 방해물 | 배경 |
|------|---------|---------|-------------|------|---------------|--------|------|
| **더 작은 데이터** | | | | | | | |
| RT-1 (ours) | 100 | 100 | 97 | 73 | 76 | 83 | 59 |
| RT-1 | 100 | 51 | 71 | 50 | 52 | 39 | 59 |
| RT-1 | 100 | 37 | 55 | 46 | 57 | 35 | 47 |
| RT-1 | 100 | 22 | 59 | 29 | 14 | 31 | 41 |
| **더 좁은 데이터** | | | | | | | |
| RT-1 (ours) | 100 | 100 | 97 | 73 | 76 | 83 | 59 |
| RT-1 | 75 | 97 | 86 | 54 | 67 | 42 | 53 |

**표 7:** RT-1의 다양한 데이터 ablation

#### 분석

**데이터셋 크기 감소:**
- 성능이 감소하는 일반적인 추세
- 일반화가 **더 급격히 감소**

**데이터셋을 더 좁게 만들 때:**
- 특히 일반화 측면에서 **훨씬 더 급격한 성능 감소**
- 데이터의 97% 유지하면서 태스크의 25% 제거 → 데이터셋 크기를 49%까지 줄인 것과 동등한 일반화 성능

#### 핵심 교훈

> **데이터 다양성이 데이터 양보다 더 중요하다**

---

## 7. 결론, 한계 및 향후 연구

### 7.1 결론

우리는 **대량의 데이터를 효과적으로 흡수하고 데이터 양과 다양성에 따라 스케일링할 수 있는** 로봇 학습 방법인 **Robotics Transformer 1(RT-1)**을 제시했습니다.

**학습 데이터:**
- 17개월에 걸쳐 13대의 로봇으로 수집
- 130,000개 이상의 에피소드
- 대규모 시연 데이터셋

**주요 성과:**
1. **700개 이상의 지시문을 97% 성공률**로 수행
2. 이전에 출판된 baseline보다 **새로운 태스크, 객체, 환경에 효과적으로 일반화**
3. 원래 태스크 성능을 희생하지 않고 **시뮬레이션과 다른 로봇 형태의 이종 데이터를 성공적으로 흡수**
4. **최대 50단계의 매우 긴 시간 범위 태스크** 실행 가능 (SayCan 프레임워크)

### 7.2 한계

RT-1이 데이터 흡수 모델을 갖춘 대규모 로봇 학습을 향한 유망한 단계를 제시하지만, 여러 한계가 있습니다:

1. **모방 학습의 한계**
   - 시연자의 성능을 능가하지 못할 수 있음
   - 모방 학습 접근 방식 클래스의 도전 상속

2. **일반화의 제약**
   - 새로운 지시문에 대한 일반화는 **이전에 본 개념의 조합으로 제한**
   - 이전에 본 적 없는 완전히 새로운 동작에 일반화 불가

3. **태스크 복잡도**
   - 크지만 매우 정교하지 않은 조작 태스크 세트

### 7.3 향후 연구 방향

**1. 지시문 세트 확장**
- RT-1이 가능하게 하고 일반화하는 지시문 세트를 계속 확장
- 비전문가가 **방향성 데이터 수집과 모델 프롬프팅**을 통해 로봇을 학습시킬 수 있는 방법 개발
- 로봇 스킬의 수를 더 빠르게 스케일링

**2. 강건성 개선**
- 현재 RT-1은 특히 방해물 객체에 상당히 강건
- **환경 다양성을 크게 늘려** 배경과 환경에 대한 강건성을 더욱 개선

**3. 성능 향상**
- **스케일러블 어텐션과 메모리**를 통해 RT-1의 반응 속도와 맥락 유지 개선

**4. 오픈소스 공개**
- 연구 커뮤니티가 이 연구 위에 구축할 수 있도록 **RT-1의 코드를 오픈소스로 공개**
- 로봇 학습을 스케일업하기 위한 미래 연구를 위한 귀중한 자원 제공

---

