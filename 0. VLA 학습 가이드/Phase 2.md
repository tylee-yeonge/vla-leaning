# VLA í•™ìŠµ ê°€ì´ë“œ - Phase 2 ìˆ˜ì •ë³¸ (AMR Component í†µí•©)

## ğŸ“‹ Phase 2 ìˆ˜ì • ìš”ì•½

### ë³€ê²½ ì‚¬í•­

| í•­ëª© | ê¸°ì¡´ | ìˆ˜ì • | ì´ìœ  |
|------|------|------|------|
| ê¸°ê°„ | 3-6ê°œì›” | 2-3ê°œì›” | ROS2 ê²½í—˜ìœ¼ë¡œ ì¶•ì†Œ ê°€ëŠ¥ |
| PyTorch ì‹¬í™” | í•„ìˆ˜ | ì„ íƒì /ì¶•ì†Œ | ê¸°ë³¸ì€ Phase 1ì—ì„œ ì™„ë£Œ |
| CNN ê¹Šì´ íŒŒê¸° | í•„ìˆ˜ | ì¶•ì†Œ | ViT ì¤‘ì‹¬ìœ¼ë¡œ ì „í™˜ |
| RL ê¸°ì´ˆ | í•„ìˆ˜ | ìµœì†Œí™” | VLAëŠ” BC ì¤‘ì‹¬ |
| ìˆ˜í•™ | ë³„ë„ ì§„í–‰ | 3Blue1Brown ì§„í–‰ì¤‘ í™œìš© | ì´ë¯¸ í•™ìŠµ ì¤‘ |

### Phase 2 ìœ„ì¹˜ ì¬ì •ì˜

```
ê¸°ì¡´ ë¡œë“œë§µ:
Phase 1 (í•„ìˆ˜) â†’ Phase 2 (í•„ìˆ˜) â†’ Phase 3 â†’ Phase 4

ìˆ˜ì • ë¡œë“œë§µ:
Phase 1 (í•„ìˆ˜) â†’ Phase 2 (ì„ íƒì  ë³´ê°•) â†’ Phase 3 â†’ Phase 4
                      â†‘
                í•„ìš”í•œ ë¶€ë¶„ë§Œ ì„ íƒì ìœ¼ë¡œ
```

---

## ğŸ¯ Phase 2 ëª©í‘œ (ìˆ˜ì •)

- [x] Phase 1ì—ì„œ ë¶€ì¡±í–ˆë˜ ë¶€ë¶„ë§Œ ì„ íƒì ìœ¼ë¡œ ì±„ìš°ê¸°
- [x] Transformer/ViT ì‹¬í™” (í•µì‹¬)
- [x] Imitation Learning ì‹¬í™” (í•µì‹¬)
- [x] PyTorch ê³ ê¸‰ ê¸°ë²• (í•„ìš”ì‹œ)
- [ ] ~~CNN ì™„ì „ ì´í•´~~ â†’ ê¸°ë³¸ë§Œ (ViT ì¤‘ì‹¬)
- [ ] ~~RL ê¸°ì´ˆ~~ â†’ ê°œë…ë§Œ (BC ì¤‘ì‹¬)

---

## ğŸ“… ìˆ˜ì •ëœ ì¼ì •

```
ê¸°ì¡´ (3-6ê°œì›”):
Month 3-4: Deep Learning ì œëŒ€ë¡œ (8ì£¼)
Month 5:   Transformer & Multi-modal (4ì£¼)
Month 6:   Imitation Learning & RL (4ì£¼)

ìˆ˜ì • (2-3ê°œì›”):
Week 1-3:  ì„ íƒì  PyTorch ì‹¬í™” (í•„ìš”ì‹œë§Œ)
Week 4-6:  Transformer & ViT ì‹¬í™” (í•µì‹¬)
Week 7-9:  Imitation Learning ì‹¬í™” (í•µì‹¬)
Week 10-12: ìˆ˜í•™ ë³‘í–‰ + ì •ë¦¬
```

---

## Week 1-3: ì„ íƒì  PyTorch ì‹¬í™”

### ë³€ê²½ ë‚´ìš©

#### ìœ ì§€: í•µì‹¬ ê³ ê¸‰ ê¸°ë²•ë§Œ

```python
# ê¼­ ì•Œì•„ì•¼ í•  ê²ƒë§Œ:
# 1. Custom Dataset/DataLoader (AMR ë°ì´í„°ìš©)
# 2. Mixed Precision Training (GPU íš¨ìœ¨)
# 3. Gradient Checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)

# ë‚˜ë¨¸ì§€ëŠ” í•„ìš”í•  ë•Œ ì°¸ì¡°
```

#### ì¶•ì†Œ ë˜ëŠ” ì œê±°

| ê¸°ì¡´ ë‚´ìš© | ìˆ˜ì • | ì´ìœ  |
|----------|------|------|
| Custom Loss Functions | ìœ ì§€ | ë„í‚¹ íƒœìŠ¤í¬ìš© ì»¤ìŠ¤í…€ loss |
| LR Scheduling | ì¶•ì†Œ | OneCycleLRë§Œ ì•Œë©´ ì¶©ë¶„ |
| Gradient Clipping | ìœ ì§€ | í•™ìŠµ ì•ˆì •ì„± |
| GPU ë©”ëª¨ë¦¬ ìµœì í™” | ìœ ì§€ | Jetson ë°°í¬ ëŒ€ë¹„ |

#### ì¶”ê°€: AMR ë°ì´í„°ì…‹ êµ¬í˜„

```python
class AMRDockingDataset(Dataset):
    """
    Phase 1ì—ì„œ ìˆ˜ì§‘í•œ ë„í‚¹ ë°ì´í„°ìš©
    
    íŠ¹ì§•:
    - ROS bag â†’ HDF5 ë³€í™˜
    - ì´ë¯¸ì§€ + cmd_vel + odom
    - Action normalization
    """
    
    def __init__(self, data_dir, transform=None):
        self.episodes = self.load_hdf5(data_dir)
        self.compute_action_statistics()
    
    def compute_action_statistics(self):
        """ë„í‚¹ cmd_vel í†µê³„ (ì •ê·œí™”ìš©)"""
        all_actions = []
        for ep in self.episodes:
            all_actions.extend(ep['cmd_vels'])
        
        self.action_mean = np.mean(all_actions, axis=0)
        self.action_std = np.std(all_actions, axis=0)
```

---

## Week 4-6: Transformer & ViT ì‹¬í™” (í•µì‹¬)

### ìœ ì§€/ê°•í™” ë‚´ìš©

#### ê°•í™”: Attention ë©”ì»¤ë‹ˆì¦˜

```python
# AMR ê´€ì  ì¶”ê°€ ì§ˆë¬¸:
# - ë„í‚¹ ì‹œ ì–´ë””ì— attentionì´ ì§‘ì¤‘ë˜ë‚˜?
# - ì¶©ì „ í¬íŠ¸ vs ë°°ê²½
# - Attention ì‹œê°í™”ë¡œ ë””ë²„ê¹… ê°€ëŠ¥?
```

#### ê°•í™”: Vision Transformer ì‹¬í™”

```python
class ViTForAMR(nn.Module):
    """
    AMR ë„í‚¹ìš© ViT ë¶„ì„
    
    ì£¼ìš” í¬ì¸íŠ¸:
    - Patch size ì˜í–¥ (16 vs 32)
    - [CLS] token vs mean pooling
    - Fine-tuning vs Feature extraction
    """
    
    def visualize_attention(self, image):
        """ë„í‚¹ ì‹œ attention íŒ¨í„´ ì‹œê°í™”"""
        # ì–´ë””ë¥¼ ë³´ëŠ”ì§€ í™•ì¸
        # â†’ ì¶©ì „ í¬íŠ¸ì— ì§‘ì¤‘ë˜ì–´ì•¼ í•¨
```

#### ì¶”ê°€: Multi-modal Fusion (AMR ê´€ì )

```python
class AMRMultiModalFusion(nn.Module):
    """
    AMRìš© ë©€í‹°ëª¨ë‹¬ ìœµí•©
    
    ì…ë ¥:
    - RGB ì´ë¯¸ì§€ (ë„í‚¹ ì¹´ë©”ë¼)
    - LiDAR ì êµ° ë˜ëŠ” ê±°ë¦¬ ì •ë³´
    - Proprioception (odom, joint states)
    
    ê¸°ì¡´ AMRê³¼ ì°¨ì´:
    - ê¸°ì¡´: LiDAR ì¤‘ì‹¬ + ì¹´ë©”ë¼ ë³´ì¡°
    - VLA: ì¹´ë©”ë¼ ì¤‘ì‹¬ + LiDAR ë³´ì¡°
    """
    
    def forward(self, rgb, lidar_range, odom):
        # Vision features
        vis_feat = self.vision_encoder(rgb)
        
        # LiDAR features (ê°„ë‹¨íˆ)
        lidar_feat = self.lidar_encoder(lidar_range)
        
        # Proprioception
        proprio_feat = self.proprio_encoder(odom)
        
        # Fusion
        fused = self.fusion(vis_feat, lidar_feat, proprio_feat)
        
        return fused
```

---

## Week 7-9: Imitation Learning ì‹¬í™” (í•µì‹¬)

### ìœ ì§€/ê°•í™” ë‚´ìš©

#### ê°•í™”: Behavioral Cloning ì‹¬í™”

```python
# AMR ë„í‚¹ì— íŠ¹í™”ëœ BC ê³ ë ¤ì‚¬í•­

class DockingBC:
    """
    ë„í‚¹ BC íŠ¹ìˆ˜ ê³ ë ¤ì‚¬í•­:
    
    1. Covariate Shift
       - ì „ë¬¸ê°€ëŠ” ì„±ê³µ ê²½ë¡œë§Œ
       - ì‹¤ì œë¡œëŠ” ì˜¤ì°¨ ë°œìƒ
       â†’ Data Augmentationìœ¼ë¡œ ë³´ì™„
    
    2. Multi-modal Action
       - ì ‘ê·¼ ë‹¨ê³„: ë¹ ë¥¸ ì†ë„
       - ë„í‚¹ ë‹¨ê³„: ëŠë¦° ì •ë°€ ì†ë„
       â†’ Mixture Density Network ê³ ë ¤
    
    3. Action Delay
       - ì´ë¯¸ì§€ â†’ ì¶”ë¡  â†’ ì‹¤í–‰ latency
       â†’ Action chunkingìœ¼ë¡œ ë³´ì™„
    """
```

#### ê°•í™”: Action Chunking

```python
class ActionChunkingPolicy(nn.Module):
    """
    ì—¬ëŸ¬ timestep actionì„ í•œë²ˆì— ì˜ˆì¸¡
    
    AMR ë„í‚¹ì—ì„œ ì¥ì :
    - ë¶€ë“œëŸ¬ìš´ trajectory
    - Latency ë³´ìƒ
    - Temporal consistency
    
    ì„¤ì •:
    - chunk_size = 10 (0.5ì´ˆ @ 20Hz)
    - ë§¤ inferenceë§ˆë‹¤ ì²« action ì‹¤í–‰
    - ë‚˜ë¨¸ì§€ëŠ” bufferì— ë³´ê´€
    """
    
    def __init__(self, chunk_size=10):
        self.chunk_size = chunk_size
        self.action_buffer = deque(maxlen=chunk_size)
    
    def forward(self, obs):
        # 10ê°œ action ì˜ˆì¸¡
        actions = self.policy(obs)  # (B, 10, 3)
        return actions
    
    def get_action(self, obs):
        if len(self.action_buffer) == 0:
            # ìƒˆë¡œ ì˜ˆì¸¡
            actions = self.forward(obs)
            for a in actions[0]:
                self.action_buffer.append(a)
        
        return self.action_buffer.popleft()
```

#### ì¶”ê°€: DAgger (ì„ íƒì )

```python
"""
DAgger: Dataset Aggregation

BCì˜ covariate shift ë¬¸ì œ í•´ê²°

AMR ë„í‚¹ì—ì„œ:
1. BCë¡œ ì´ˆê¸° policy í•™ìŠµ
2. Policyë¡œ ë„í‚¹ ì‹œë„
3. ì „ë¬¸ê°€ê°€ ê°œì…í•˜ì—¬ êµì •
4. ìƒˆ ë°ì´í„° ì¶”ê°€
5. ì¬í•™ìŠµ
6. ë°˜ë³µ

í˜„ì‹¤ì  ì ‘ê·¼:
- ì‹œë®¬ë ˆì´ì…˜ì—ì„œ DAgger ì ìš©
- ì‹¤ì œ ë¡œë´‡ì—ì„œëŠ” BCë¡œ ì¶©ë¶„í•  ìˆ˜ë„
"""
```

### ì¶•ì†Œ/ì œê±° ë‚´ìš©

#### ì¶•ì†Œ: RL ê¸°ì´ˆ

```python
"""
ê¸°ì¡´: REINFORCE, PPO êµ¬í˜„
ìˆ˜ì •: ê°œë…ë§Œ ì´í•´

ì´ìœ :
- VLAëŠ” BC ì¤‘ì‹¬
- RL fine-tuningì€ Phase 4ì—ì„œ ì„ íƒì 
- ì‹œê°„ íš¨ìœ¨í™”

ì•Œì•„ì•¼ í•  ê²ƒ:
- Policy Gradient ê°œë…
- BCì™€ RLì˜ ì°¨ì´
- ì–¸ì œ RLì´ í•„ìš”í•œì§€
"""
```

---

## Week 10-12: ìˆ˜í•™ ë³‘í–‰ + ì •ë¦¬

### ë³€ê²½ ë‚´ìš©

#### ìœ ì§€: Linear Algebra (ì§„í–‰ì¤‘ í™œìš©)

```markdown
3Blue1Brown ì‹œë¦¬ì¦ˆ ê³„ì† ì§„í–‰

VLA ê´€ë ¨ í•µì‹¬:
- Eigenvalues â†’ PCA, Attention
- Matrix decomposition â†’ ëª¨ë¸ ì••ì¶•
- Linear transformation â†’ Layer ì´í•´
```

#### ì¶•ì†Œ: Probability & Statistics

```markdown
ê¸°ì¡´: ê¹Šì´ìˆëŠ” í™•ë¥ ë¡ 
ìˆ˜ì •: VLAì— í•„ìš”í•œ ê²ƒë§Œ

í•„ìˆ˜:
- Gaussian distribution (action modeling)
- Cross-entropy (classification)
- KL divergence (ê°œë…ë§Œ)

ë‚˜ë¨¸ì§€ëŠ” í•„ìš”í•  ë•Œ ì°¸ì¡°
```

#### ì¶”ê°€: Phase 3 ì¤€ë¹„ ì²´í¬

```markdown
â–¡ Isaac Sim ì„¤ì¹˜ ê°€ëŠ¥í•œì§€ í™•ì¸ (RTX 4070, 32GB RAM)
â–¡ íšŒì‚¬ AMR URDF í™•ë³´ ê°€ëŠ¥í•œì§€ í™•ì¸
â–¡ GPU ì„œë²„ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
â–¡ Phase 1 ë„í‚¹ ë°ì´í„° ì •ë¦¬
```

---

## Phase 2 ì™„ë£Œ ì²´í¬ (ìˆ˜ì •)

```
ê¸°ì¡´:
âœ… Deep Learning ê¸°ì´ˆ íƒ„íƒ„ (80%)
âœ… PyTorch ììœ ìì¬ (85%)
âœ… CNN ì™„ì „ ì´í•´ (80%)
âœ… Transformer ì™„ì „ ì´í•´ (90%)
âœ… Multi-modal learning ì´í•´ (80%)
âœ… Imitation Learning ì‹¬í™” (70%)
âœ… RL ê¸°ì´ˆ ì´í•´ (60%)
âœ… ìˆ˜í•™ ê¸°ì´ˆ ì¶©ë¶„ (70%)

ìˆ˜ì •:
âœ… PyTorch ê³ ê¸‰ ê¸°ë²• (í•„ìš”í•œ ê²ƒë§Œ) (70%)
âœ… Transformer/ViT ì‹¬í™” (í•µì‹¬) (85%)
âœ… Multi-modal fusion ì´í•´ (AMR ê´€ì ) (75%)
âœ… Imitation Learning ì‹¬í™” (í•µì‹¬) (80%)
âœ… Action Chunking ì´í•´ (80%)
â¬œ RL ê¸°ì´ˆ â†’ ê°œë…ë§Œ (40%)
âœ… ìˆ˜í•™ ì§„í–‰ì¤‘ (3B1B ì™„ì£¼ ëª©í‘œ)
âœ… Phase 3 ì¤€ë¹„ ì™„ë£Œ
```

---

## í•™ìŠµ ì‹œê°„ ì¶”ì •

### ì£¼ë‹¹ 10-12ì‹œê°„ ê¸°ì¤€

| ì£¼ì°¨ | ë‚´ìš© | ì˜ˆìƒ ì‹œê°„ |
|------|------|----------|
| 1-3 | PyTorch ì‹¬í™” (ì„ íƒì ) | 15h |
| 4-6 | Transformer/ViT ì‹¬í™” | 20h |
| 7-9 | Imitation Learning ì‹¬í™” | 18h |
| 10-12 | ìˆ˜í•™ + ì •ë¦¬ | 12h |

**ì´: ~65ì‹œê°„ (ì•½ 2-2.5ê°œì›”)**

### ìœ ì—°í•œ ì§„í–‰

```markdown
## ë¹¨ë¦¬ ëë‚  ìˆ˜ ìˆëŠ” ê²½ìš°
- PyTorch ì´ë¯¸ ì¶©ë¶„ â†’ Week 1-3 ê±´ë„ˆë›°ê¸°
- Transformer ë…¼ë¬¸ ì½ê¸° ìˆ˜ì›” â†’ Week 4-6 ì¶•ì†Œ

## ëŠ¦ì–´ì§ˆ ìˆ˜ ìˆëŠ” ê²½ìš°
- ê¸°ì´ˆê°€ ë¶€ì¡±í•˜ë‹¤ ëŠë¼ë©´ ì²œì²œíˆ
- íšŒì‚¬ ì—…ë¬´ ë°”ìœ ì‹œê¸°

â†’ Phase 3 ì‹œì‘ì´ ì¤‘ìš”í•˜ë¯€ë¡œ 80% ì´í•´ë¡œ ë„˜ì–´ê°€ë„ OK
```

---

## Phase 1 â†’ Phase 2 â†’ Phase 3 ì—°ê²°

```
Phase 1 ì™„ë£Œ ì‹œì :
- Mini VLA ë™ì‘ (Gazebo)
- ROS2 Component ì„¤ê³„
- 70% ì´í•´ë„

Phase 2ì—ì„œ:
- ë¶€ì¡±í•œ ë¶€ë¶„ ì„ íƒì  ë³´ê°•
- í•µì‹¬ (Transformer, IL) ì‹¬í™”
- 85% ì´í•´ë„

Phase 3 ì‹œì‘ ì‹œ:
- Isaac Sim ë³¸ê²© ì‚¬ìš©
- ë” ë³µì¡í•œ VLA ê°œë°œ
- Production-ready ëª©í‘œ
```