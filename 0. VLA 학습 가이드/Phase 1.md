# VLA í•™ìŠµ ê°€ì´ë“œ - Phase 1

## ëª©ì°¨
- [ğŸ¯ Phase 1: Top-Down ë¹ ë¥¸ ëŒíŒŒ (1-2ê°œì›”)](#-phase-1-top-down-ë¹ ë¥¸-ëŒíŒŒ-1-2ê°œì›”)
- [Week 1-2: VLA ë§›ë³´ê¸°](#week-1-2-vla-ë§›ë³´ê¸°)
  - [Week 1: ë…¼ë¬¸ê³¼ ì‹¤í–‰](#week-1-ë…¼ë¬¸ê³¼-ì‹¤í–‰)
    - [Day 1-3: RT-1 ë…¼ë¬¸ ì²« ì½ê¸°](#day-1-3-rt-1-ë…¼ë¬¸-ì²«-ì½ê¸°)
    - [Day 4-5: ì˜ìƒìœ¼ë¡œ ì§ê´€](#day-4-5-ì˜ìƒìœ¼ë¡œ-ì§ê´€)
    - [Day 6-7: LeRobot ì‹¤í–‰í•´ë³´ê¸°](#day-6-7-lerobot-ì‹¤í–‰í•´ë³´ê¸°)
- [Week 2: ìµœì†Œí•œì˜ ê¸°ì´ˆë§Œ](#week-2-ìµœì†Œí•œì˜-ê¸°ì´ˆë§Œ)
  - [PyTorch ì†ì„± (3ì¼)](#pytorch-ì†ì„±-3ì¼)
  - [Transformer ê°œë…ë§Œ (2ì¼)](#transformer-ê°œë…ë§Œ-2ì¼)
  - [Behavioral Cloning (2ì¼)](#behavioral-cloning-2ì¼)
- [Week 3-4: Mini VLA ë§Œë“¤ê¸°](#week-3-4-mini-vla-ë§Œë“¤ê¸°)
  - [í”„ë¡œì íŠ¸: PyBulletë¡œ ë¸”ë¡ ë°€ê¸°](#í”„ë¡œì íŠ¸-pybulletë¡œ-ë¸”ë¡-ë°€ê¸°)
    - [Week 3: í™˜ê²½ êµ¬ì¶•](#week-3-í™˜ê²½-êµ¬ì¶•)
    - [Week 4: í•™ìŠµ ë° í‰ê°€](#week-4-í•™ìŠµ-ë°-í‰ê°€)
- [Week 5-8: LeRobot ë§ˆìŠ¤í„°](#week-5-8-lerobot-ë§ˆìŠ¤í„°)
  - [Week 5-6: ë‹¤ì–‘í•œ Policy ì‹¤í—˜](#week-5-6-ë‹¤ì–‘í•œ-policy-ì‹¤í—˜)
  - [Week 7-8: RT-1 í•µì‹¬ ìš”ì†Œ êµ¬í˜„](#week-7-8-rt-1-í•µì‹¬-ìš”ì†Œ-êµ¬í˜„)
- [Phase 1 ì™„ë£Œ ì²´í¬](#phase-1-ì™„ë£Œ-ì²´í¬)



## ğŸ¯ Phase 1: Top-Down ë¹ ë¥¸ ëŒíŒŒ (1-2ê°œì›”)

### ëª©í‘œ
- VLAê°€ ë­”ì§€ ê° ì¡ê¸°
- ê°„ë‹¨í•œ VLA ì§ì ‘ ë§Œë“¤ì–´ë³´ê¸°
- "ë‚˜ë„ í•  ìˆ˜ ìˆë‹¤" ìì‹ ê°

---

## Week 1-2: VLA ë§›ë³´ê¸°

### Week 1: ë…¼ë¬¸ê³¼ ì‹¤í–‰

#### Day 1-3: RT-1 ë…¼ë¬¸ ì²« ì½ê¸°

**ëª©í‘œ: 30% ì´í•´ë„ë¡œ ì½ê¸°**
```
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] Abstract 3ë²ˆ ì½ê¸°
- [ ] Figure ì „ë¶€ ë³´ê¸°
- [ ] ëª¨ë¥´ëŠ” ìš©ì–´ ë¦¬ìŠ¤íŠ¸ì—…
- [ ] ì „ì²´ íë¦„ë§Œ íŒŒì•…

ì‹œê°„: í•˜ë£¨ 1ì‹œê°„ Ã— 3ì¼
```

**ì½ëŠ” ë°©ë²•:**
1. Abstractë§Œ 3ë²ˆ ì½ê¸°
2. Introduction ì •ë…
3. Figure ì „ë¶€ ë³´ê¸° (ê·¸ë¦¼ì´ í•µì‹¬!)
4. Method í›‘ì–´ë³´ê¸° (ëª¨ë¥´ëŠ” ê±° ë©”ëª¨ë§Œ)
5. Results ê²°ê³¼ë§Œ ë³´ê¸°

**ì˜ˆìƒ ë°˜ì‘:**
```
"Transformerê°€ ë­ì§€?"
"Tokenì´ ë­ì•¼?"
"FiLM layerëŠ”?"
"Behavioral Cloning?"

â†’ ì •ìƒì…ë‹ˆë‹¤! ê³„ì† ì§„í–‰í•˜ì„¸ìš”
```

---

#### Day 4-5: ì˜ìƒìœ¼ë¡œ ì§ê´€

**ì¶”ì²œ ì˜ìƒ:**

1. **RT-1 Official Video**
   - YouTube ê²€ìƒ‰: "RT-1 Robotics Transformer"
   - ë¡œë´‡ì´ ì‹¤ì œë¡œ ë­˜ í•˜ëŠ”ì§€ í™•ì¸
   - ì‹œê°„: 3ë¶„

2. **ì„¤ëª… ì˜ìƒ**
   - "RT-1 Explained" ê²€ìƒ‰
   - ì—¬ëŸ¬ ê°œ ë³´ê¸° (ê°ì ì„¤ëª… ë°©ì‹ ë‹¤ë¦„)
   - ì‹œê°„: 30ë¶„-1ì‹œê°„

3. **Conference Talk**
   - CoRL 2022 presentation ê²€ìƒ‰
   - ì—°êµ¬ìê°€ ì§ì ‘ ì„¤ëª…
   - ì‹œê°„: 20ë¶„

**í•™ìŠµ í¬ì¸íŠ¸:**
- [ ] VLAì˜ ì…ë ¥/ì¶œë ¥ ì´í•´
- [ ] ì–´ë–¤ ë¬¸ì œë¥¼ í‘¸ëŠ”ì§€ ì´í•´
- [ ] ì™œ ì¤‘ìš”í•œì§€ ì´í•´

**ì‹œê°„: 2-3ì‹œê°„**

---

#### Day 6-7: LeRobot ì‹¤í–‰í•´ë³´ê¸°

**ëª©í‘œ: ì½”ë“œ í•œ ì¤„ë„ ì´í•´ ëª» í•´ë„ ì¼ë‹¨ ëŒë ¤ë³´ê¸°!**
```bash
# ì„¤ì¹˜
pip install lerobot

# ë°ì´í„° ì‹œê°í™”
python -m lerobot.scripts.visualize_dataset \
    --repo-id lerobot/pusht

# í•™ìŠµ ì‹¤í–‰
python -m lerobot.scripts.train \
    --dataset lerobot/pusht \
    --policy act \
    --num-epochs 10
```

**ê´€ì°°í•  ê²ƒ:**
- [ ] ë­ê°€ ì…ë ¥ì¸ê°€? (ì´ë¯¸ì§€)
- [ ] ë­ê°€ ì¶œë ¥ì¸ê°€? (action)
- [ ] í•™ìŠµì´ ë­˜ í•˜ëŠ”ê°€? (loss ê°ì†Œ)
- [ ] ê²°ê³¼ê°€ ë­”ê°€? (ë¡œë´‡ ì›€ì§ì„)

**ì‹œê°„: 4-6ì‹œê°„**

---

**Week 1 ì™„ë£Œ ì²´í¬:**
```
âœ… VLAê°€ ëŒ€ì¶© ë­”ì§€ ì•
âœ… ë…¼ë¬¸ 1í¸ ë´¤ìŒ (ì´í•´ 30%)
âœ… ì½”ë“œ ëŒë ¤ë´¤ìŒ (ì´í•´ 10%)
âŒ ê¹Šì€ ì´í•´ëŠ” ì—†ìŒ

â†’ ì´ê²Œ ì •ìƒ! ê³„ì† ì§„í–‰
```

---

## Week 2: ìµœì†Œí•œì˜ ê¸°ì´ˆë§Œ

### PyTorch ì†ì„± (3ì¼)

**Day 1: Tensor ê¸°ë³¸**
```python
import torch

# Tensor ìƒì„±
x = torch.randn(2, 3)
y = torch.zeros(2, 3)
z = torch.ones(2, 3)

# Shape ì¡°ì‘
x.shape  # torch.Size([2, 3])
x.view(3, 2)  # reshape
x.transpose(0, 1)

# ì¤‘ìš”í•œ ê²ƒë§Œ ì•Œë©´ ë¨!
```

**Day 2: nn.Module**
```python
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(10, 20)
        self.layer2 = nn.Linear(20, 5)
    
    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = self.layer2(x)
        return x

# ì´ íŒ¨í„´ë§Œ ì•Œë©´ ë¨
```

**Day 3: Training Loop**
```python
# ì´ íŒ¨í„´ì´ ì „ë¶€!
for epoch in range(num_epochs):
    for batch_data, batch_labels in dataloader:
        # Forward
        outputs = model(batch_data)
        loss = criterion(outputs, batch_labels)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

**ìë£Œ:**
- PyTorch ê³µì‹ íŠœí† ë¦¬ì–¼: "Deep Learning with PyTorch: A 60 Minute Blitz"

**ì‹œê°„: í•˜ë£¨ 1ì‹œê°„ Ã— 3ì¼**

---

### Transformer ê°œë…ë§Œ (2ì¼)

**í•„ìˆ˜ ê°œë…:**
- Self-Attentionì´ ë­”ì§€
- Query, Key, Value
- ViTê°€ ì´ë¯¸ì§€ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€

**ìë£Œ:**
- "The Illustrated Transformer" ë¸”ë¡œê·¸ (í•„ë…!)
- 3Blue1Brown "Attention" ì˜ìƒ

**ê¹Šì´ ì¡°ì ˆ:**
- âŒ ìˆ˜ì‹ ìœ ë„ â†’ ë„ˆë¬´ ê¹ŠìŒ
- âŒ ì™„ë²½í•œ ì´í•´ â†’ ì‹œê°„ ë‚­ë¹„
- âœ… ê°œë…ê³¼ ì§ê´€ â†’ ì¶©ë¶„í•¨

**ì‹œê°„: í•˜ë£¨ 1ì‹œê°„ Ã— 2ì¼**

---

### Behavioral Cloning (2ì¼)

**í•µì‹¬ ê°œë…:**
```python
# BC = Supervised Learning for Actions

# ì „í†µì  Supervised Learning:
# ì…ë ¥: ì´ë¯¸ì§€
# ì¶œë ¥: ë¼ë²¨ (ê³ ì–‘ì´/ê°•ì•„ì§€)
# ì†ì‹¤: Cross-entropy

# Behavioral Cloning:
# ì…ë ¥: Observation (ì´ë¯¸ì§€)
# ì¶œë ¥: Action (joint angles, gripper)
# ì†ì‹¤: MSE (Mean Squared Error)

# â†’ ì™„ì „íˆ ë˜‘ê°™ì€ ì›ë¦¬!
```

**ì½”ë“œë¡œ ì´í•´:**
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# BCëŠ” ê·¸ëƒ¥ íšŒê·€(Regression)
class SimpleBC(nn.Module):
    def __init__(self, obs_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )
    
    def forward(self, obs):
        return self.net(obs)

# í•™ìŠµ ë£¨í”„
model = SimpleBC(obs_dim=100, action_dim=7)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for obs, expert_action in dataloader:
    # Forward
    predicted_action = model(obs)
    
    # Loss (MSE)
    loss = F.mse_loss(predicted_action, expert_action)
    
    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# ì´ê²Œ BCì˜ ì „ë¶€!
```

**ì‹œê°„: í•˜ë£¨ 1ì‹œê°„ Ã— 2ì¼**

---

**Week 2 ì™„ë£Œ ì²´í¬:**
```
âœ… PyTorch ê¸°ë³¸ (50%)
âœ… Transformer ê°œë… (30%)
âœ… BC ì´í•´ (70%)

â†’ Mini VLA ë§Œë“¤ ì¤€ë¹„ ë¨!
```

---

## Week 3-4: Mini VLA ë§Œë“¤ê¸°

### í”„ë¡œì íŠ¸: PyBulletë¡œ ë¸”ë¡ ë°€ê¸°

#### Week 3: í™˜ê²½ êµ¬ì¶•

**Day 1-2: PyBullet ê¸°ë³¸**
```python
import pybullet as p
import pybullet_data
import time

# ì—°ê²°
p.connect(p.GUI)
p.setAdditionalSearchPath(pybullet_data.getDataPath())
p.setGravity(0, 0, -10)

# í™˜ê²½ ë¡œë“œ
plane = p.loadURDF("plane.urdf")
robot = p.loadURDF("kuka_iiwa/model.urdf", [0, 0, 0])
table = p.loadURDF("table/table.urdf", [0.5, 0, 0])
block = p.loadURDF("cube.urdf", [0.7, 0, 0.7], globalScaling=0.05)

# ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
for i in range(1000):
    p.stepSimulation()
    time.sleep(1./240.)

p.disconnect()
```

**ì²´í¬:**
- [ ] PyBullet GUI ì—´ë¦¼
- [ ] ë¡œë´‡ íŒ” ë³´ì„
- [ ] ë¸”ë¡ ë–¨ì–´ì§€ëŠ” ê²ƒ í™•ì¸

---

**Day 3-4: ì¹´ë©”ë¼ ì¶”ê°€**
```python
import numpy as np
from PIL import Image

def get_camera_image():
    # ì¹´ë©”ë¼ ì„¤ì •
    view_matrix = p.computeViewMatrix(
        cameraEyePosition=[1, 1, 1],
        cameraTargetPosition=[0.5, 0, 0.5],
        cameraUpVector=[0, 0, 1]
    )
    
    projection_matrix = p.computeProjectionMatrixFOV(
        fov=60,
        aspect=1.0,
        nearVal=0.1,
        farVal=100.0
    )
    
    # ì´ë¯¸ì§€ ìº¡ì²˜
    width, height = 224, 224
    img_arr = p.getCameraImage(
        width, height,
        view_matrix,
        projection_matrix,
        renderer=p.ER_BULLET_HARDWARE_OPENGL
    )
    
    # RGB ì´ë¯¸ì§€ ì¶”ì¶œ
    rgb = np.array(img_arr[2]).reshape(height, width, 4)[:, :, :3]
    return rgb

# í…ŒìŠ¤íŠ¸
for i in range(100):
    p.stepSimulation()
    
    if i % 10 == 0:
        img = get_camera_image()
        print(f"Image shape: {img.shape}")
```

**ì²´í¬:**
- [ ] ì´ë¯¸ì§€ ìº¡ì²˜ ê°€ëŠ¥
- [ ] Shape (224, 224, 3) í™•ì¸

---

**Day 5-7: Teleoperation ë°ì´í„° ìˆ˜ì§‘**
```python
import pickle
from pynput import keyboard

class DataCollector:
    def __init__(self):
        self.episodes = []
        self.current_episode = {'obs': [], 'actions': []}
        self.recording = False
        self.current_action = np.zeros(7)  # 7-DOF robot
    
    def start_episode(self):
        self.recording = True
        self.current_episode = {'obs': [], 'actions': []}
        print("ğŸ”´ Recording started")
    
    def stop_episode(self):
        if self.recording:
            self.episodes.append(self.current_episode.copy())
            self.recording = False
            print(f"âœ… Episode saved ({len(self.current_episode['obs'])} frames)")
    
    def add_step(self, obs, action):
        if self.recording:
            self.current_episode['obs'].append(obs)
            self.current_episode['actions'].append(action.copy())
    
    def save(self, filename='data.pkl'):
        with open(filename, 'wb') as f:
            pickle.dump(self.episodes, f)
        print(f"ğŸ’¾ Saved {len(self.episodes)} episodes")

# í‚¤ë³´ë“œ ì œì–´
collector = DataCollector()
current_joint_velocities = np.zeros(7)

def on_press(key):
    global current_joint_velocities
    try:
        # ì¡°ì¸íŠ¸ ì œì–´ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        if key.char == 'w':
            current_joint_velocities[0] = 0.5
        elif key.char == 's':
            current_joint_velocities[0] = -0.5
        # ... ë‹¤ë¥¸ í‚¤ ë§¤í•‘
        
        # ë…¹í™” ì œì–´
        elif key.char == 'r':
            collector.start_episode()
        elif key.char == 't':
            collector.stop_episode()
    except AttributeError:
        pass

def on_release(key):
    global current_joint_velocities
    current_joint_velocities = np.zeros(7)
    if key == keyboard.Key.esc:
        return False

# ë©”ì¸ ë£¨í”„
listener = keyboard.Listener(on_press=on_press, on_release=on_release)
listener.start()

while listener.is_alive():
    # ë¡œë´‡ ì œì–´
    for i in range(7):
        p.setJointMotorControl2(
            robot, i,
            p.VELOCITY_CONTROL,
            targetVelocity=current_joint_velocities[i]
        )
    
    p.stepSimulation()
    
    # ë°ì´í„° ìˆ˜ì§‘
    obs = get_camera_image()
    collector.add_step(obs, current_joint_velocities)

collector.save('demonstrations.pkl')
```

**ëª©í‘œ:**
- [ ] 10+ ì—í”¼ì†Œë“œ ìˆ˜ì§‘
- [ ] ê° ì—í”¼ì†Œë“œ 50+ í”„ë ˆì„

---

#### Week 4: í•™ìŠµ ë° í‰ê°€

**Day 1-3: ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ**
```python
# mini_vla.py
import torch
import torch.nn as nn
from transformers import ViTModel

class MiniVLA(nn.Module):
    """
    ì´ˆê°„ë‹¨ VLA ëª¨ë¸
    - Vision: Pre-trained ViT
    - Policy: MLP
    """
    
    def __init__(self, action_dim=7):
        super().__init__()
        
        # Vision Encoder (ViT-Base)
        self.vision = ViTModel.from_pretrained('google/vit-base-patch16-224')
        
        # Policy Head
        self.policy = nn.Sequential(
            nn.Linear(768, 256),  # ViT hidden_size
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, images):
        """
        Args:
            images: (B, 3, 224, 224)
        Returns:
            actions: (B, action_dim)
        """
        # Vision encoding
        vision_outputs = self.vision(images)
        
        # [CLS] token ì‚¬ìš©
        image_features = vision_outputs.last_hidden_state[:, 0]
        
        # Policy
        actions = self.policy(image_features)
        
        return actions

# Dataset
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

class RobotDataset(Dataset):
    def __init__(self, episodes_file):
        with open(episodes_file, 'rb') as f:
            self.episodes = pickle.load(f)
        
        # ëª¨ë“  (obs, action) ìŒ ì¶”ì¶œ
        self.data = []
        for episode in self.episodes:
            for obs, action in zip(episode['obs'], episode['actions']):
                self.data.append((obs, action))
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        obs, action = self.data[idx]
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        obs_tensor = self.transform(obs)
        action_tensor = torch.FloatTensor(action)
        
        return obs_tensor, action_tensor

# í•™ìŠµ
def train_mini_vla():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ë°ì´í„° ë¡œë“œ
    dataset = RobotDataset('demonstrations.pkl')
    dataloader = DataLoader(
        dataset,
        batch_size=32,
        shuffle=True,
        num_workers=2
    )
    
    # ëª¨ë¸
    model = MiniVLA(action_dim=7).to(device)
    
    # Optimizer & Loss
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()
    
    # í•™ìŠµ
    num_epochs = 50
    
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        
        for obs, actions in dataloader:
            obs = obs.to(device)
            actions = actions.to(device)
            
            # Forward
            pred_actions = model(obs)
            loss = criterion(pred_actions, actions)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (epoch + 1) % 10 == 0:
            torch.save(model.state_dict(), f'checkpoint_epoch{epoch+1}.pt')
    
    print("ğŸ‰ Training complete!")

if __name__ == '__main__':
    train_mini_vla()
```

---

**Day 4-7: í‰ê°€ ë° ê°œì„ **
```python
# evaluate.py
def evaluate_model():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ëª¨ë¸ ë¡œë“œ
    model = MiniVLA(action_dim=7).to(device)
    model.load_state_dict(torch.load('checkpoint_epoch50.pt'))
    model.eval()
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])
    
    # PyBullet ì´ˆê¸°í™”
    p.connect(p.GUI)
    p.setAdditionalSearchPath(pybullet_data.getDataPath())
    p.setGravity(0, 0, -10)
    
    success_count = 0
    num_episodes = 10
    
    for episode in range(num_episodes):
        print(f"\nEpisode {episode + 1}/{num_episodes}")
        
        # í™˜ê²½ ë¦¬ì…‹
        p.resetSimulation()
        plane = p.loadURDF("plane.urdf")
        robot = p.loadURDF("kuka_iiwa/model.urdf", [0, 0, 0])
        block = p.loadURDF("cube.urdf", [0.7, 0, 0.7], globalScaling=0.05)
        
        # ëª©í‘œ ìœ„ì¹˜
        goal_pos = [0.3, 0, 0.5]
        
        for step in range(100):
            # ê´€ì¸¡
            rgb_image = get_camera_image()
            obs_tensor = transform(rgb_image).unsqueeze(0).to(device)
            
            # ì˜ˆì¸¡
            with torch.no_grad():
                action = model(obs_tensor)
                action = action.cpu().numpy()[0]
            
            # ì‹¤í–‰
            for i in range(7):
                p.setJointMotorControl2(
                    robot, i,
                    p.VELOCITY_CONTROL,
                    targetVelocity=action[i]
                )
            
            p.stepSimulation()
            
            # ì„±ê³µ ì²´í¬
            block_pos, _ = p.getBasePositionAndOrientation(block)
            distance = np.linalg.norm(np.array(block_pos) - np.array(goal_pos))
            
            if distance < 0.1:  # 10cm ì´ë‚´
                print(f"  âœ… Success at step {step}")
                success_count += 1
                break
        else:
            print(f"  âŒ Failed")
    
    p.disconnect()
    
    success_rate = success_count / num_episodes
    print(f"\nğŸ“Š Success Rate: {success_rate * 100:.1f}%")

if __name__ == '__main__':
    evaluate_model()
```

**ê¸°ëŒ€ ê²°ê³¼:**
- ì²« ì‹œë„: 20-40% ì„±ê³µë¥  (ì •ìƒ!)
- ë°ì´í„°/ëª¨ë¸ ê°œì„  í›„: 50-60%

---

## Week 5-8: LeRobot ë§ˆìŠ¤í„°

### Week 5-6: ë‹¤ì–‘í•œ Policy ì‹¤í—˜
```bash
# ACT Policy
python -m lerobot.scripts.train \
    --dataset lerobot/pusht \
    --policy act \
    --batch-size 32 \
    --num-epochs 100

# Diffusion Policy
python -m lerobot.scripts.train \
    --dataset lerobot/pusht \
    --policy diffusion \
    --batch-size 32 \
    --num-epochs 100

# ì„±ëŠ¥ ë¹„êµ
python -m lerobot.scripts.eval \
    --policy act \
    --checkpoint path/to/checkpoint

python -m lerobot.scripts.eval \
    --policy diffusion \
    --checkpoint path/to/checkpoint
```

**ë¹„êµí•  ê²ƒ:**
- [ ] í•™ìŠµ ì†ë„
- [ ] ìµœì¢… ì„±ëŠ¥
- [ ] ì•ˆì •ì„±
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

---

### Week 7-8: RT-1 í•µì‹¬ ìš”ì†Œ êµ¬í˜„
```python
# rt1_components.py

import torch
import torch.nn as nn

class FiLMLayer(nn.Module):
    """
    Feature-wise Linear Modulation
    ì–¸ì–´ ì„ë² ë”©ìœ¼ë¡œ vision featureë¥¼ ì¡°ì ˆ
    """
    def __init__(self, feature_dim, condition_dim):
        super().__init__()
        self.scale = nn.Linear(condition_dim, feature_dim)
        self.shift = nn.Linear(condition_dim, feature_dim)
    
    def forward(self, features, condition):
        """
        Args:
            features: (B, N, feature_dim) - vision tokens
            condition: (B, condition_dim) - language embedding
        """
        gamma = self.scale(condition).unsqueeze(1)  # (B, 1, feature_dim)
        beta = self.shift(condition).unsqueeze(1)
        
        return gamma * features + beta

class TokenLearner(nn.Module):
    """
    Adaptive token selection
    ë§ì€ token â†’ ì ì€ token (íš¨ìœ¨ì„±)
    """
    def __init__(self, num_tokens, input_dim):
        super().__init__()
        self.num_tokens = num_tokens
        self.attention = nn.Sequential(
            nn.Linear(input_dim, input_dim),
            nn.Tanh(),
            nn.Linear(input_dim, num_tokens)
        )
    
    def forward(self, tokens):
        """
        Args:
            tokens: (B, N, D) - input tokens
        Returns:
            selected: (B, num_tokens, D)
        """
        # Attention weights
        attn_weights = self.attention(tokens)  # (B, N, num_tokens)
        attn_weights = torch.softmax(attn_weights, dim=1)
        
        # Weighted sum
        attn_weights = attn_weights.transpose(1, 2)  # (B, num_tokens, N)
        selected = torch.bmm(attn_weights, tokens)  # (B, num_tokens, D)
        
        return selected

class SimpleRT1(nn.Module):
    """
    RT-1ì˜ ê°„ì†Œí™” ë²„ì „
    """
    def __init__(self, action_dim=7, num_tokens=8):
        super().__init__()
        
        # Vision Encoder
        from transformers import ViTModel
        self.vision = ViTModel.from_pretrained('google/vit-base-patch16-224')
        vision_dim = 768
        
        # Language Encoder (ê°„ë‹¨íˆ)
        self.language = nn.Embedding(vocab_size=1000, embedding_dim=512)
        
        # FiLM layer
        self.film = FiLMLayer(vision_dim, 512)
        
        # Token Learner
        self.token_learner = TokenLearner(num_tokens, vision_dim)
        
        # Transformer Decoder
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=vision_dim,
            nhead=8,
            dim_feedforward=2048,
            dropout=0.1
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=4)
        
        # Action Head
        self.action_head = nn.Linear(vision_dim, action_dim)
    
    def forward(self, images, instructions):
        """
        Args:
            images: (B, 3, 224, 224)
            instructions: (B, seq_len) - token ids
        Returns:
            actions: (B, action_dim)
        """
        # Vision
        vision_out = self.vision(images).last_hidden_state  # (B, 197, 768)
        
        # Language
        lang_embed = self.language(instructions).mean(dim=1)  # (B, 512)
        
        # FiLM conditioning
        conditioned = self.film(vision_out, lang_embed)  # (B, 197, 768)
        
        # Token selection
        selected_tokens = self.token_learner(conditioned)  # (B, 8, 768)
        
        # Decoder
        query = selected_tokens.mean(dim=1, keepdim=True)  # (B, 1, 768)
        decoded = self.decoder(
            query.transpose(0, 1),
            selected_tokens.transpose(0, 1)
        ).transpose(0, 1).squeeze(1)
        
        # Action
        actions = self.action_head(decoded)  # (B, action_dim)
        
        return actions

# í…ŒìŠ¤íŠ¸
if __name__ == '__main__':
    model = SimpleRT1()
    
    images = torch.randn(2, 3, 224, 224)
    instructions = torch.randint(0, 1000, (2, 10))
    
    actions = model(images, instructions)
    print(f"Actions shape: {actions.shape}")
```

**ì²´í¬:**
- [ ] FiLM layer ì´í•´ ë° êµ¬í˜„
- [ ] TokenLearner êµ¬í˜„
- [ ] ì „ì²´ ëª¨ë¸ ì¡°ë¦½

---

## Phase 1 ì™„ë£Œ ì²´í¬
```
âœ… VLA ì „ì²´ ê·¸ë¦¼ ì´í•´ (70%)
âœ… ê°„ë‹¨í•œ VLA ë§Œë“¤ì–´ë´„
âœ… ì‹¤íŒ¨ì™€ ì„±ê³µ ê²½í—˜
âœ… "í•  ìˆ˜ ìˆë‹¤" ìì‹ ê°
âœ… ë¶€ì¡±í•œ ë¶€ë¶„ ëª…í™•íˆ íŒŒì•…
âœ… RT-1 í•µì‹¬ ìš”ì†Œ ì´í•´

â†’ ì´ì œ ì œëŒ€ë¡œ ë°°ìš¸ ì¤€ë¹„!
```