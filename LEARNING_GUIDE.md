# VLA Top-Down í•™ìŠµ ê°€ì´ë“œ

## ğŸ¯ í•™ìŠµ ì² í•™

### Top-Down vs Bottom-Up
```
âŒ Bottom-Up (ì „í†µì , ë¹„íš¨ìœ¨):
ì„ í˜•ëŒ€ìˆ˜ â†’ ë¯¸ì ë¶„ â†’ ìµœì í™” â†’ ML â†’ DL â†’ CV â†’ NLP â†’ RL â†’ VLA
â””â”€ 6ê°œì›”+ ì§€ë‚˜ë„ VLA ì‹œì‘ ëª» í•¨

âœ… Top-Down (íš¨ìœ¨ì , ì¶”ì²œ):
VLA ë…¼ë¬¸ ì½ê¸° â†’ ë§‰íˆëŠ” ë¶€ë¶„ë§Œ í•™ìŠµ â†’ êµ¬í˜„ â†’ ë°˜ë³µ
â””â”€ 1ì£¼ì¼ ë§Œì— VLA ì½”ë“œ ëŒë¦¼
```

**í•µì‹¬ ì›ì¹™:**
1. í° ê·¸ë¦¼ì„ ë¨¼ì € ë³¸ë‹¤
2. ì´í•´ë„ 30%ë©´ ì§„í–‰í•œë‹¤
3. í•„ìš”í•œ ê²ƒë§Œ ê¹Šê²Œ íŒë‹¤
4. ì‹¤ìŠµì´ 80%ë‹¤

---

## ğŸ“… 8ì£¼ í•™ìŠµ ë¡œë“œë§µ

### Week 1: VLAì˜ ì „ì²´ ê·¸ë¦¼ ë³´ê¸°

#### Day 1-2: RT-1 ë…¼ë¬¸ ì²« ì½ê¸°

**ëª©í‘œ:** 70% ì´í•´ ë¶ˆê°€ëŠ¥í•´ë„ ìƒê´€ì—†ìŒ, "VLAê°€ ë­”ì§€" ê°ë§Œ ì¡ê¸°

**ë…¼ë¬¸:**
- "RT-1: Robotics Transformer for Real-World Control at Scale"
- [arXiv ë§í¬](https://arxiv.org/abs/2212.06817)

**ì½ëŠ” ë°©ë²•:**
1. Abstractë§Œ 3ë²ˆ ì½ê¸°
2. Introduction ì •ë…
3. Figure ì „ë¶€ ë³´ê¸° (ê·¸ë¦¼ì´ í•µì‹¬!)
4. Method í›‘ì–´ë³´ê¸° (ëª¨ë¥´ëŠ” ê±° ë©”ëª¨ë§Œ)
5. Results ê²°ê³¼ë§Œ ë³´ê¸°

**ë©”ëª¨í•  ê²ƒ:**
- [ ] ì´í•´ ì•ˆ ë˜ëŠ” ìš©ì–´ ë¦¬ìŠ¤íŠ¸ì—…
- [ ] ê¶ê¸ˆí•œ ì  ì ê¸°
- [ ] ê·¸ë¦¼ ë³´ë©´ì„œ ì§ê´€ ì¡ê¸°

**ì‹œê°„:** 3-4ì‹œê°„

**ì˜ˆìƒ ë°˜ì‘:**
```
"Transformerê°€ ë­ì§€?"
"Tokenì´ ë­ì•¼?"
"FiLM layerëŠ”?"
"Behavioral Cloning?"

â†’ ì •ìƒì…ë‹ˆë‹¤! ê³„ì† ì§„í–‰í•˜ì„¸ìš”
```

---

#### Day 3-4: ì˜ìƒìœ¼ë¡œ ì§ê´€ ì¡ê¸°

**ì¶”ì²œ ì˜ìƒ:**

1. **RT-1 Official Video**
   - YouTube ê²€ìƒ‰: "RT-1 Robotics Transformer"
   - ë¡œë´‡ì´ ì‹¤ì œë¡œ ë­˜ í•˜ëŠ”ì§€ í™•ì¸
   - ì‹œê°„: 3ë¶„

2. **ì„¤ëª… ì˜ìƒ**
   - "RT-1 Explained" ê²€ìƒ‰
   - ì—¬ëŸ¬ ê°œ ë³´ê¸° (ê°ì ì„¤ëª… ë°©ì‹ ë‹¤ë¦„)
   - ì‹œê°„: 30ë¶„-1ì‹œê°„

3. **Conference Talk**
   - CoRL 2022 presentation ê²€ìƒ‰
   - ì—°êµ¬ìê°€ ì§ì ‘ ì„¤ëª…
   - ì‹œê°„: 20ë¶„

**í•™ìŠµ í¬ì¸íŠ¸:**
- [ ] VLAì˜ ì…ë ¥/ì¶œë ¥ ì´í•´
- [ ] ì–´ë–¤ ë¬¸ì œë¥¼ í‘¸ëŠ”ì§€ ì´í•´
- [ ] ì™œ ì¤‘ìš”í•œì§€ ì´í•´

**ì‹œê°„:** 2-3ì‹œê°„

---

#### Day 5-7: HuggingFace LeRobot ì‹¤í–‰í•´ë³´ê¸°

**ëª©í‘œ:** ì½”ë“œ í•œ ì¤„ë„ ì´í•´ ëª» í•´ë„ ì¼ë‹¨ ëŒë ¤ë³´ê¸°!

**ì„¤ì¹˜:**
```bash
pip install lerobot
```

**ì²« ì‹¤í–‰ - ë°ì´í„° ì‹œê°í™”:**
```bash
python -m lerobot.scripts.visualize_dataset \
    --repo-id lerobot/pusht
```
â†’ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ í™•ì¸

**ë‘ ë²ˆì§¸ - í•™ìŠµ ì‹¤í–‰:**
```bash
python -m lerobot.scripts.train \
    --dataset lerobot/pusht \
    --policy act
```
â†’ í•™ìŠµ ëŒë ¤ë³´ê¸° (ì´í•´ ì•ˆ ë¼ë„ ë¨)

**ê´€ì°°í•  ê²ƒ:**
- [ ] ë­ê°€ ì…ë ¥ì¸ê°€? (ì´ë¯¸ì§€)
- [ ] ë­ê°€ ì¶œë ¥ì¸ê°€? (action)
- [ ] í•™ìŠµì´ ë­˜ í•˜ëŠ”ê°€? (loss ê°ì†Œ)
- [ ] ê²°ê³¼ê°€ ë­”ê°€? (ë¡œë´‡ ì›€ì§ì„)

**ì‹œê°„:** 4-6ì‹œê°„

---

**Week 1 ì™„ë£Œ ì²´í¬:**
```
âœ… VLAê°€ ëŒ€ì¶© ë­”ì§€ ì•
âœ… ë…¼ë¬¸ 1í¸ ë´¤ìŒ (ì´í•´ 30%)
âœ… ì½”ë“œ ëŒë ¤ë´¤ìŒ (ì´í•´ 10%)
âŒ ê¹Šì€ ì´í•´ëŠ” ì—†ìŒ

â†’ ì´ê²Œ ì •ìƒ! ê³„ì† ì§„í–‰
```

---

### Week 2-3: ë§‰íˆëŠ” ë¶€ë¶„ë§Œ ì§‘ì¤‘ í•™ìŠµ

#### Part 1: Transformer ìµœì†Œí•œë§Œ (3-4ì¼)

**ëª©í‘œ:** VLAì— í•„ìš”í•œ ë§Œí¼ë§Œ

**ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ê²ƒ:**

1. **Self-Attention ê°œë… (2ì‹œê°„)**
   - Query, Key, Valueê°€ ë­”ì§€
   - Attention weight ê³„ì‚°
   - ì§ê´€: "ì–´ë””ë¥¼ ë³¼ì§€ ê²°ì •"

2. **Transformer êµ¬ì¡° (2ì‹œê°„)**
   - Encoder-Decoder êµ¬ì¡°
   - Position encoding
   - Multi-head attention

3. **Vision Transformer (2ì‹œê°„)**
   - ì´ë¯¸ì§€ë¥¼ ì–´ë–»ê²Œ í† í°ìœ¼ë¡œ?
   - Patch embedding
   - [CLS] tokenì˜ ì˜ë¯¸

**í•™ìŠµ ìë£Œ:**
- "The Illustrated Transformer" (ë¸”ë¡œê·¸) - í•„ë…!
- 3Blue1Brown "Attention" ì˜ìƒ
- Andrej Karpathy "Let's build GPT" (ì„ íƒ)

**ê¹Šì´ ì¡°ì ˆ:**
- âŒ ìˆ˜ì‹ ìœ ë„ â†’ ë„ˆë¬´ ê¹ŠìŒ
- âŒ ì™„ë²½í•œ ì´í•´ â†’ ì‹œê°„ ë‚­ë¹„
- âœ… ê°œë…ê³¼ ì§ê´€ â†’ ì¶©ë¶„í•¨

**ê°„ë‹¨í•œ ì‹¤ìŠµ:**
```python
import torch
import torch.nn.functional as F

# Attention ë©”ì»¤ë‹ˆì¦˜ ì´í•´í•˜ê¸°
Q = torch.randn(1, 10, 64)  # Query
K = torch.randn(1, 10, 64)  # Key
V = torch.randn(1, 10, 64)  # Value

# Attention weight ê³„ì‚°
attention_scores = Q @ K.transpose(-2, -1) / 8
attention_weights = F.softmax(attention_scores, dim=-1)

# Weighted sum
output = attention_weights @ V

print("Attention weights shape:", attention_weights.shape)
print("Output shape:", output.shape)

# ì§ê´€: attention_weightsê°€ "ì–´ë””ë¥¼ ë³¼ì§€" ê²°ì •
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Attentionì˜ ì§ê´€ ì´í•´
- [ ] Transformer êµ¬ì¡° ê·¸ë¦¼ ê·¸ë¦´ ìˆ˜ ìˆìŒ
- [ ] ViTê°€ ì–´ë–»ê²Œ ì´ë¯¸ì§€ ì²˜ë¦¬í•˜ëŠ”ì§€ ì´í•´

---

#### Part 2: Behavioral Cloning (2-3ì¼)

**ëª©í‘œ:** VLAì˜ í•™ìŠµ ë°©ë²• ì´í•´

**í•µì‹¬ ê°œë…:**
```
BC = Supervised Learning for Actions

ì „í†µì  Supervised Learning:
ì…ë ¥: ì´ë¯¸ì§€
ì¶œë ¥: ë¼ë²¨ (ê³ ì–‘ì´/ê°•ì•„ì§€)
ì†ì‹¤: Cross-entropy

Behavioral Cloning:
ì…ë ¥: Observation (ì´ë¯¸ì§€)
ì¶œë ¥: Action (joint angles, gripper)
ì†ì‹¤: MSE (Mean Squared Error)

â†’ ì™„ì „íˆ ë˜‘ê°™ì€ ì›ë¦¬!
```

**ì½”ë“œë¡œ ì´í•´:**
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# BCëŠ” ê·¸ëƒ¥ íšŒê·€(Regression)
class SimpleBC(nn.Module):
    def __init__(self, obs_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )
    
    def forward(self, obs):
        return self.net(obs)

# í•™ìŠµ ë£¨í”„
model = SimpleBC(obs_dim=100, action_dim=7)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for obs, expert_action in dataloader:
    # Forward
    predicted_action = model(obs)
    
    # Loss (MSE)
    loss = F.mse_loss(predicted_action, expert_action)
    
    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    print(f"Loss: {loss.item()}")

# ì´ê²Œ BCì˜ ì „ë¶€!
```

**BC vs RL ë¹„êµ:**
```
Behavioral Cloning:
âœ… ê°„ë‹¨í•¨ (Supervised Learning)
âœ… ì•ˆì •ì  í•™ìŠµ
âœ… Expert demonstration í•„ìš”
âŒ Expertë³´ë‹¤ ëª»í•¨

Reinforcement Learning:
âœ… Expert ì—†ì´ í•™ìŠµ ê°€ëŠ¥
âœ… ìŠ¤ìŠ¤ë¡œ ë” ë‚˜ì•„ì§ˆ ìˆ˜ ìˆìŒ
âŒ ë³µì¡í•¨
âŒ í•™ìŠµ ë¶ˆì•ˆì •

VLAëŠ” ëŒ€ë¶€ë¶„ BC ì‚¬ìš©!
```

**ê¹Šì´ ì¡°ì ˆ:**
- âŒ RL ì „ì²´ â†’ ë‹¹ì¥ ë¶ˆí•„ìš”
- âŒ Policy gradient â†’ ë‚˜ì¤‘ì—
- âŒ DAgger â†’ ì‹¬í™”
- âœ… BCë§Œ â†’ ì§€ê¸ˆì€ ì¶©ë¶„!

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] BCì˜ ì›ë¦¬ ì™„ì „ ì´í•´
- [ ] BCì™€ ì¼ë°˜ SLì˜ ì°¨ì´ ì´í•´
- [ ] ê°„ë‹¨í•œ BC ì½”ë“œ ì‘ì„± ê°€ëŠ¥

---

#### Part 3: PyTorch í•„ìˆ˜ë§Œ (3-4ì¼)

**ëª©í‘œ:** VLA ì½”ë“œ ì½ì„ ìˆ˜ ìˆì„ ì •ë„ë§Œ

**í•„ìˆ˜ ê°œë…:**

**1. Tensor ê¸°ë³¸ (1ì‹œê°„)**
```python
import torch

# Tensor ìƒì„±
x = torch.randn(2, 3)
y = torch.zeros(2, 3)
z = torch.ones(2, 3)

# Shape ì¡°ì‘
x.shape  # torch.Size([2, 3])
x.view(3, 2)  # reshape
x.transpose(0, 1)

# ì¤‘ìš”í•œ ê²ƒë§Œ ì•Œë©´ ë¨!
```

**2. nn.Module (1ì‹œê°„)**
```python
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(10, 20)
        self.layer2 = nn.Linear(20, 5)
    
    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = self.layer2(x)
        return x

# ì´ íŒ¨í„´ë§Œ ì•Œë©´ ë¨
```

**3. DataLoader (1ì‹œê°„)**
```python
from torch.utils.data import Dataset, DataLoader

class MyDataset(Dataset):
    def __init__(self, data):
        self.data = data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

dataset = MyDataset(my_data)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# Batch ê°œë…ë§Œ ì´í•´í•˜ë©´ ë¨
```

**4. Training Loop (1ì‹œê°„)**
```python
# ì´ íŒ¨í„´ì´ ì „ë¶€!
for epoch in range(num_epochs):
    for batch_data, batch_labels in dataloader:
        # Forward
        outputs = model(batch_data)
        loss = criterion(outputs, batch_labels)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

**5. GPU ì‚¬ìš© (30ë¶„)**
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = model.to(device)
data = data.to(device)

# ì´ê²ƒë§Œ ì•Œë©´ ë¨
```

**í•™ìŠµ ìë£Œ:**
- PyTorch ê³µì‹ íŠœí† ë¦¬ì–¼: "Deep Learning with PyTorch: A 60 Minute Blitz"
- https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html

**ê¹Šì´ ì¡°ì ˆ:**
- âŒ Advanced ê¸°ëŠ¥ â†’ ë‚˜ì¤‘ì—
- âŒ Custom CUDA â†’ í•„ìš” ì—†ìŒ
- âŒ ë¶„ì‚° í•™ìŠµ â†’ í•„ìš” ì—†ìŒ
- âœ… ê¸°ë³¸ë§Œ â†’ ì¶©ë¶„í•¨!

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Tensor ì¡°ì‘ ê°€ëŠ¥
- [ ] nn.Module ì‘ì„± ê°€ëŠ¥
- [ ] Training loop ì´í•´
- [ ] GPU ì‚¬ìš© ê°€ëŠ¥

---

**Week 2-3 ì™„ë£Œ ì²´í¬:**
```
âœ… Transformer ê°œë… ì´í•´ (60%)
âœ… BC ì™„ì „ ì´í•´ (90%)
âœ… PyTorch ê¸°ë³¸ ê°€ëŠ¥ (70%)
âœ… VLA ë…¼ë¬¸ ë‹¤ì‹œ ì½ìœ¼ë©´ 80% ì´í•´

â†’ ì´ì œ ì§ì ‘ ë§Œë“¤ ì¤€ë¹„ ë¨!
```

---

### Week 4-6: ê°„ë‹¨í•œ VLA ì§ì ‘ ë§Œë“¤ê¸°

#### í”„ë¡œì íŠ¸: "Mini VLA"

**ëª©í‘œ:** RT-1ì˜ ì´ˆê°„ë‹¨ ë²„ì „ êµ¬í˜„

**ìŠ¤í™:**
- í™˜ê²½: PyBullet (Isaac Simë³´ë‹¤ ì‰¬ì›€)
- ì‘ì—…: ë¡œë´‡ íŒ”ë¡œ ë¸”ë¡ ë°€ê¸°
- ëª¨ë¸: ViT-Tiny + MLP

---

#### Week 4: í™˜ê²½ ì…‹ì—…

**Day 1-2: PyBullet ì„¤ì¹˜ ë° ê¸°ë³¸**
```bash
pip install pybullet
```
```python
# basic_pybullet.py
import pybullet as p
import pybullet_data
import time

# ì—°ê²°
p.connect(p.GUI)
p.setAdditionalSearchPath(pybullet_data.getDataPath())
p.setGravity(0, 0, -10)

# í™˜ê²½ ë¡œë“œ
plane = p.loadURDF("plane.urdf")
robot = p.loadURDF("kuka_iiwa/model.urdf", [0, 0, 0])
table = p.loadURDF("table/table.urdf", [0.5, 0, 0])
block = p.loadURDF("cube.urdf", [0.7, 0, 0.7], globalScaling=0.05)

# ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
for i in range(1000):
    p.stepSimulation()
    time.sleep(1./240.)

p.disconnect()
```

**ì²´í¬:**
- [ ] PyBullet GUI ì—´ë¦¼
- [ ] ë¡œë´‡ íŒ” ë³´ì„
- [ ] ë¸”ë¡ ë–¨ì–´ì§€ëŠ” ê²ƒ í™•ì¸

---

**Day 3-4: ì¹´ë©”ë¼ ì¶”ê°€**
```python
# camera_setup.py
import numpy as np
from PIL import Image

def get_camera_image():
    # ì¹´ë©”ë¼ ì„¤ì •
    view_matrix = p.computeViewMatrix(
        cameraEyePosition=[1, 1, 1],
        cameraTargetPosition=[0.5, 0, 0.5],
        cameraUpVector=[0, 0, 1]
    )
    
    projection_matrix = p.computeProjectionMatrixFOV(
        fov=60,
        aspect=1.0,
        nearVal=0.1,
        farVal=100.0
    )
    
    # ì´ë¯¸ì§€ ìº¡ì²˜
    width, height = 224, 224
    img_arr = p.getCameraImage(
        width, height,
        view_matrix,
        projection_matrix,
        renderer=p.ER_BULLET_HARDWARE_OPENGL
    )
    
    # RGB ì´ë¯¸ì§€ ì¶”ì¶œ
    rgb = np.array(img_arr[2]).reshape(height, width, 4)[:, :, :3]
    return rgb

# í…ŒìŠ¤íŠ¸
for i in range(100):
    p.stepSimulation()
    
    if i % 10 == 0:
        img = get_camera_image()
        print(f"Image shape: {img.shape}")
```

**ì²´í¬:**
- [ ] ì´ë¯¸ì§€ ìº¡ì²˜ ê°€ëŠ¥
- [ ] Shape (224, 224, 3) í™•ì¸

---

**Day 5-7: Teleoperation ë°ì´í„° ìˆ˜ì§‘**
```python
# teleop_collect.py
import pybullet as p
import numpy as np
import pickle
from pynput import keyboard

class DataCollector:
    def __init__(self):
        self.episodes = []
        self.current_episode = {'obs': [], 'actions': []}
        self.recording = False
        self.current_action = np.zeros(7)  # 7-DOF robot
    
    def start_episode(self):
        self.recording = True
        self.current_episode = {'obs': [], 'actions': []}
        print("ğŸ”´ Recording started")
    
    def stop_episode(self):
        if self.recording:
            self.episodes.append(self.current_episode.copy())
            self.recording = False
            print(f"âœ… Episode saved ({len(self.current_episode['obs'])} frames)")
    
    def add_step(self, obs, action):
        if self.recording:
            self.current_episode['obs'].append(obs)
            self.current_episode['actions'].append(action.copy())
    
    def save(self, filename='data.pkl'):
        with open(filename, 'wb') as f:
            pickle.dump(self.episodes, f)
        print(f"ğŸ’¾ Saved {len(self.episodes)} episodes")

# í‚¤ë³´ë“œ ì œì–´
collector = DataCollector()
current_joint_velocities = np.zeros(7)

def on_press(key):
    global current_joint_velocities
    try:
        # ì¡°ì¸íŠ¸ ì œì–´ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        if key.char == 'w':
            current_joint_velocities[0] = 0.5
        elif key.char == 's':
            current_joint_velocities[0] = -0.5
        # ... ë‹¤ë¥¸ í‚¤ ë§¤í•‘
        
        # ë…¹í™” ì œì–´
        elif key.char == 'r':
            collector.start_episode()
        elif key.char == 't':
            collector.stop_episode()
    except AttributeError:
        pass

def on_release(key):
    global current_joint_velocities
    current_joint_velocities = np.zeros(7)
    if key == keyboard.Key.esc:
        return False

# ë©”ì¸ ë£¨í”„
listener = keyboard.Listener(on_press=on_press, on_release=on_release)
listener.start()

while listener.is_alive():
    # ë¡œë´‡ ì œì–´
    for i in range(7):
        p.setJointMotorControl2(
            robot, i,
            p.VELOCITY_CONTROL,
            targetVelocity=current_joint_velocities[i]
        )
    
    p.stepSimulation()
    
    # ë°ì´í„° ìˆ˜ì§‘
    obs = get_camera_image()
    collector.add_step(obs, current_joint_velocities)

collector.save('demonstrations.pkl')
```

**ëª©í‘œ:**
- [ ] 10+ ì—í”¼ì†Œë“œ ìˆ˜ì§‘
- [ ] ê° ì—í”¼ì†Œë“œ 50+ í”„ë ˆì„

---

#### Week 5: Mini VLA ëª¨ë¸ êµ¬í˜„

**Day 1-3: ëª¨ë¸ ì •ì˜**
```python
# mini_vla.py
import torch
import torch.nn as nn
from transformers import ViTModel, ViTConfig

class MiniVLA(nn.Module):
    """
    ì´ˆê°„ë‹¨ VLA ëª¨ë¸
    - Vision: Pre-trained ViT
    - Policy: MLP
    """
    
    def __init__(self, action_dim=7):
        super().__init__()
        
        # Vision Encoder (ViT-Tiny ì‚¬ìš©)
        config = ViTConfig(
            image_size=224,
            patch_size=16,
            num_channels=3,
            hidden_size=192,  # Tiny
            num_hidden_layers=12,
            num_attention_heads=3,
        )
        self.vision = ViTModel(config)
        
        # ë˜ëŠ” Pre-trained ì‚¬ìš©
        # self.vision = ViTModel.from_pretrained('google/vit-base-patch16-224')
        
        # Policy Head
        self.policy = nn.Sequential(
            nn.Linear(192, 256),  # ViT hidden_size
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, images):
        """
        Args:
            images: (B, 3, 224, 224)
        Returns:
            actions: (B, action_dim)
        """
        # Vision encoding
        vision_outputs = self.vision(images)
        
        # [CLS] token ì‚¬ìš©
        image_features = vision_outputs.last_hidden_state[:, 0]
        
        # Policy
        actions = self.policy(image_features)
        
        return actions

# í…ŒìŠ¤íŠ¸
if __name__ == '__main__':
    model = MiniVLA(action_dim=7)
    
    # ë”ë¯¸ ì…ë ¥
    dummy_images = torch.randn(4, 3, 224, 224)
    
    # Forward
    actions = model(dummy_images)
    
    print(f"Input shape: {dummy_images.shape}")
    print(f"Output shape: {actions.shape}")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")
```

**ì²´í¬:**
- [ ] ëª¨ë¸ ì •ì˜ ì™„ë£Œ
- [ ] Forward pass ì‘ë™
- [ ] ì¶œë ¥ shape í™•ì¸

---

**Day 4-5: ë°ì´í„° ì¤€ë¹„**
```python
# dataset.py
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pickle
from torchvision import transforms

class RobotDataset(Dataset):
    def __init__(self, episodes_file):
        with open(episodes_file, 'rb') as f:
            self.episodes = pickle.load(f)
        
        # ëª¨ë“  (obs, action) ìŒ ì¶”ì¶œ
        self.data = []
        for episode in self.episodes:
            for obs, action in zip(episode['obs'], episode['actions']):
                self.data.append((obs, action))
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        obs, action = self.data[idx]
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        obs_tensor = self.transform(obs)
        action_tensor = torch.FloatTensor(action)
        
        return obs_tensor, action_tensor

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == '__main__':
    dataset = RobotDataset('demonstrations.pkl')
    dataloader = DataLoader(
        dataset,
        batch_size=32,
        shuffle=True,
        num_workers=2
    )
    
    # í…ŒìŠ¤íŠ¸
    for obs, actions in dataloader:
        print(f"Batch obs shape: {obs.shape}")
        print(f"Batch actions shape: {actions.shape}")
        break
```

**ì²´í¬:**
- [ ] Dataset í´ë˜ìŠ¤ ì‘ë™
- [ ] DataLoaderë¡œ ë°°ì¹˜ ë¡œë“œ ê°€ëŠ¥
- [ ] ì´ë¯¸ì§€ ì •ê·œí™” í™•ì¸

---

**Day 6-7: í•™ìŠµ ë£¨í”„**
```python
# train.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from mini_vla import MiniVLA
from dataset import RobotDataset
import wandb  # ì„ íƒì‚¬í•­

def train_mini_vla():
    # ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # ë°ì´í„° ë¡œë“œ
    full_dataset = RobotDataset('demonstrations.pkl')
    
    # Train/Val split
    train_size = int(0.9 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size]
    )
    
    train_loader = DataLoader(
        train_dataset, batch_size=32, shuffle=True, num_workers=2
    )
    val_loader = DataLoader(
        val_dataset, batch_size=32, shuffle=False, num_workers=2
    )
    
    # ëª¨ë¸
    model = MiniVLA(action_dim=7).to(device)
    
    # Optimizer & Loss
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()
    
    # í•™ìŠµ
    num_epochs = 50
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0.0
        
        for obs, actions in train_loader:
            obs = obs.to(device)
            actions = actions.to(device)
            
            # Forward
            pred_actions = model(obs)
            loss = criterion(pred_actions, actions)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        train_loss /= len(train_loader)
        
        # Validation
        model.eval()
        val_loss = 0.0
        
        with torch.no_grad():
            for obs, actions in val_loader:
                obs = obs.to(device)
                actions = actions.to(device)
                
                pred_actions = model(obs)
                loss = criterion(pred_actions, actions)
                
                val_loss += loss.item()
        
        val_loss /= len(val_loader)
        
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"  Train Loss: {train_loss:.4f}")
        print(f"  Val Loss: {val_loss:.4f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_model.pt')
            print(f"  âœ… Best model saved!")
    
    print("\nğŸ‰ Training complete!")

if __name__ == '__main__':
    train_mini_vla()
```

**ì‹¤í–‰:**
```bash
python train.py
```

**ì²´í¬:**
- [ ] í•™ìŠµ ì‹œì‘ë¨
- [ ] Lossê°€ ê°ì†Œí•¨
- [ ] best_model.pt ì €ì¥ë¨

---

#### Week 6: í‰ê°€ ë° ê°œì„ 

**Day 1-3: ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í‰ê°€**
```python
# evaluate.py
import torch
import pybullet as p
import pybullet_data
import numpy as np
from mini_vla import MiniVLA
from camera_setup import get_camera_image
from torchvision import transforms

def evaluate_model():
    # ë””ë°”ì´ìŠ¤
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ëª¨ë¸ ë¡œë“œ
    model = MiniVLA(action_dim=7).to(device)
    model.load_state_dict(torch.load('best_model.pt'))
    model.eval()
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])
    
    # PyBullet ì´ˆê¸°í™”
    p.connect(p.GUI)
    p.setAdditionalSearchPath(pybullet_data.getDataPath())
    p.setGravity(0, 0, -10)
    
    success_count = 0
    num_episodes = 10
    
    for episode in range(num_episodes):
        print(f"\nEpisode {episode + 1}/{num_episodes}")
        
        # í™˜ê²½ ë¦¬ì…‹
        p.resetSimulation()
        plane = p.loadURDF("plane.urdf")
        robot = p.loadURDF("kuka_iiwa/model.urdf", [0, 0, 0])
        block = p.loadURDF("cube.urdf", [0.7, 0, 0.7], globalScaling=0.05)
        
        # ëª©í‘œ ìœ„ì¹˜ (ì˜ˆì‹œ)
        goal_pos = [0.3, 0, 0.5]
        
        for step in range(100):
            # ê´€ì¸¡
            rgb_image = get_camera_image()
            obs_tensor = transform(rgb_image).unsqueeze(0).to(device)
            
            # ì˜ˆì¸¡
            with torch.no_grad():
                action = model(obs_tensor)
                action = action.cpu().numpy()[0]
            
            # ì‹¤í–‰
            for i in range(7):
                p.setJointMotorControl2(
                    robot, i,
                    p.VELOCITY_CONTROL,
                    targetVelocity=action[i]
                )
            
            p.stepSimulation()
            
            # ì„±ê³µ ì²´í¬ (ë¸”ë¡ì´ ëª©í‘œ ìœ„ì¹˜ ê·¼ì²˜ì— ìˆëŠ”ì§€)
            block_pos, _ = p.getBasePositionAndOrientation(block)
            distance = np.linalg.norm(np.array(block_pos) - np.array(goal_pos))
            
            if distance < 0.1:  # 10cm ì´ë‚´
                print(f"  âœ… Success at step {step}")
                success_count += 1
                break
        else:
            print(f"  âŒ Failed")
    
    p.disconnect()
    
    success_rate = success_count / num_episodes
    print(f"\nğŸ“Š Success Rate: {success_rate * 100:.1f}%")

if __name__ == '__main__':
    evaluate_model()
```

**ì‹¤í–‰:**
```bash
python evaluate.py
```

**ê¸°ëŒ€ ê²°ê³¼:**
- ì²« ì‹œë„: 0-20% ì„±ê³µë¥  (ì •ìƒ!)
- ë°ì´í„°/ëª¨ë¸ ê°œì„  í›„: 40-60%

---

**Day 4-7: ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ & ê°œì„ **

**ë¶„ì„ ì²´í¬ë¦¬ìŠ¤íŠ¸:**

1. **ë°ì´í„° ë¬¸ì œ?**
   - [ ] ì—í”¼ì†Œë“œ ìˆ˜ ì¶©ë¶„í•œê°€? (ìµœì†Œ 20+)
   - [ ] ë‹¤ì–‘í•œ ì‹œì‘ ìœ„ì¹˜ì—ì„œ ìˆ˜ì§‘í–ˆë‚˜?
   - [ ] Expert demonstration í’ˆì§ˆì€?
   
   ê°œì„ :
```python
   # ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘
   # ë‹¤ì–‘í•œ ì´ˆê¸° ì¡°ê±´ì—ì„œ
   # ë” ì¼ê´€ëœ demonstration
```

2. **ëª¨ë¸ ë¬¸ì œ?**
   - [ ] ëª¨ë¸ì´ ë„ˆë¬´ ì‘ì€ê°€?
   - [ ] Overfitting ë°œìƒ?
   - [ ] Lossê°€ ì¶©ë¶„íˆ ë‚®ì•„ì¡Œë‚˜?
   
   ê°œì„ :
```python
   # ëª¨ë¸ í¬ê¸° ì¦ê°€
   self.policy = nn.Sequential(
       nn.Linear(192, 512),  # 256 â†’ 512
       nn.ReLU(),
       nn.Linear(512, 256),
       nn.ReLU(),
       nn.Linear(256, action_dim)
   )
```

3. **Sim-to-Sim Gap?**
   - [ ] í•™ìŠµ í™˜ê²½ = í‰ê°€ í™˜ê²½?
   - [ ] ì´ˆê¸° ì¡°ê±´ ë‹¤ë¥¸ê°€?
   
   ê°œì„ :
```python
   # í•™ìŠµ ì‹œ ë‹¤ì–‘í•œ ì´ˆê¸° ì¡°ê±´
   # Domain randomization
```

**ê°œì„  ì‚¬ì´í´:**
```
ë¶„ì„ â†’ ê°€ì„¤ â†’ ì‹¤í—˜ â†’ í‰ê°€ â†’ ë°˜ë³µ
```

---

**Week 4-6 ì™„ë£Œ ì²´í¬:**
```
âœ… ë‚˜ë§Œì˜ VLA ëª¨ë¸ ì™„ì„±!
âœ… ë°ì´í„° ìˆ˜ì§‘ â†’ í•™ìŠµ â†’ í‰ê°€ ì „ì²´ ê²½í—˜
âœ… ì‹¤íŒ¨ë„ ê²½í—˜ (ë§¤ìš° ì¤‘ìš”!)
âœ… VLA ì „ì²´ íŒŒì´í”„ë¼ì¸ ì´í•´
âœ… ë””ë²„ê¹… ëŠ¥ë ¥ í–¥ìƒ

â†’ ì´ì œ ë³µì¡í•œ ê²ƒë„ í•  ìˆ˜ ìˆìŒ!
```

---

### Week 7-8: RT-1 ë‹¤ì‹œ ì½ê³  LeRobot ë§ˆìŠ¤í„°

#### Week 7: RT-1 ë…¼ë¬¸ ì¬êµ¬í˜„

**Day 1-2: ë…¼ë¬¸ ë‹¤ì‹œ ì½ê¸°**

**ì´ì œ ë…¼ë¬¸ì´ ë‹¤ë¥´ê²Œ ë³´ì…ë‹ˆë‹¤:**
```
Week 1ì— ì½ì—ˆì„ ë•Œ:
"ë­” ì†Œë¦°ì§€ í•˜ë‚˜ë„ ëª¨ë¥´ê² ë„¤..."

Week 7ì— ë‹¤ì‹œ ì½ìœ¼ë©´:
"ì•„! ì´ê²Œ ì´ëŸ° ëœ»ì´ì—ˆêµ¬ë‚˜!"
"ë‚´ê°€ ë§Œë“  ê±°ë‘ ë¹„ìŠ·í•œë°?"
"ì—¬ê¸°ê°€ ë‹¤ë¥´ë„¤, ì´ë˜ì„œ ì„±ëŠ¥ì´ ì¢‹êµ¬ë‚˜"

â†’ ê°™ì€ ë…¼ë¬¸, ì™„ì „íˆ ë‹¤ë¥¸ ì´í•´ë„!
```

**ì£¼ëª©í•  ë¶€ë¶„:**
- [ ] FiLM layers: ì–¸ì–´ë¡œ vision conditioning
- [ ] TokenLearner: íš¨ìœ¨ì ì¸ attention
- [ ] Action chunking: ì—¬ëŸ¬ timestep ì˜ˆì¸¡
- [ ] EfficientNet: Vision backbone

---

**Day 3-7: RT-1 í•µì‹¬ ìš”ì†Œ êµ¬í˜„**
```python
# rt1_components.py

import torch
import torch.nn as nn

class FiLMLayer(nn.Module):
    """
    Feature-wise Linear Modulation
    ì–¸ì–´ ì„ë² ë”©ìœ¼ë¡œ vision featureë¥¼ ì¡°ì ˆ
    """
    def __init__(self, feature_dim, condition_dim):
        super().__init__()
        self.scale = nn.Linear(condition_dim, feature_dim)
        self.shift = nn.Linear(condition_dim, feature_dim)
    
    def forward(self, features, condition):
        """
        Args:
            features: (B, N, feature_dim) - vision tokens
            condition: (B, condition_dim) - language embedding
        """
        gamma = self.scale(condition).unsqueeze(1)  # (B, 1, feature_dim)
        beta = self.shift(condition).unsqueeze(1)
        
        return gamma * features + beta

class TokenLearner(nn.Module):
    """
    Adaptive token selection
    ë§ì€ token â†’ ì ì€ token (íš¨ìœ¨ì„±)
    """
    def __init__(self, num_tokens, input_dim):
        super().__init__()
        self.num_tokens = num_tokens
        self.attention = nn.Sequential(
            nn.Linear(input_dim, input_dim),
            nn.Tanh(),
            nn.Linear(input_dim, num_tokens)
        )
    
    def forward(self, tokens):
        """
        Args:
            tokens: (B, N, D) - input tokens
        Returns:
            selected: (B, num_tokens, D)
        """
        # Attention weights
        attn_weights = self.attention(tokens)  # (B, N, num_tokens)
        attn_weights = torch.softmax(attn_weights, dim=1)
        
        # Weighted sum
        attn_weights = attn_weights.transpose(1, 2)  # (B, num_tokens, N)
        selected = torch.bmm(attn_weights, tokens)  # (B, num_tokens, D)
        
        return selected

class SimpleRT1(nn.Module):
    """
    RT-1ì˜ ê°„ì†Œí™” ë²„ì „
    """
    def __init__(self, action_dim=7, num_tokens=8):
        super().__init__()
        
        # Vision Encoder
        from transformers import ViTModel
        self.vision = ViTModel.from_pretrained('google/vit-base-patch16-224')
        vision_dim = 768
        
        # Language Encoder (ê°„ë‹¨íˆ)
        self.language = nn.Embedding(vocab_size=1000, embedding_dim=512)
        
        # FiLM layer
        self.film = FiLMLayer(vision_dim, 512)
        
        # Token Learner
        self.token_learner = TokenLearner(num_tokens, vision_dim)
        
        # Transformer Decoder
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=vision_dim,
            nhead=8,
            dim_feedforward=2048,
            dropout=0.1
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=4)
        
        # Action Head
        self.action_head = nn.Linear(vision_dim, action_dim)
    
    def forward(self, images, instructions):
        """
        Args:
            images: (B, 3, 224, 224)
            instructions: (B, seq_len) - token ids
        Returns:
            actions: (B, action_dim)
        """
        # Vision
        vision_out = self.vision(images).last_hidden_state  # (B, 197, 768)
        
        # Language
        lang_embed = self.language(instructions).mean(dim=1)  # (B, 512)
        
        # FiLM conditioning
        conditioned = self.film(vision_out, lang_embed)  # (B, 197, 768)
        
        # Token selection
        selected_tokens = self.token_learner(conditioned)  # (B, 8, 768)
        
        # Decoder (ê°„ë‹¨íˆ)
        query = selected_tokens.mean(dim=1, keepdim=True)  # (B, 1, 768)
        decoded = self.decoder(
            query.transpose(0, 1),
            selected_tokens.transpose(0, 1)
        ).transpose(0, 1).squeeze(1)
        
        # Action
        actions = self.action_head(decoded)  # (B, action_dim)
        
        return actions

# í…ŒìŠ¤íŠ¸
if __name__ == '__main__':
    model = SimpleRT1()
    
    images = torch.randn(2, 3, 224, 224)
    instructions = torch.randint(0, 1000, (2, 10))
    
    actions = model(images, instructions)
    print(f"Actions shape: {actions.shape}")
```

**ì²´í¬:**
- [ ] FiLM layer ì´í•´ ë° êµ¬í˜„
- [ ] TokenLearner êµ¬í˜„
- [ ] ì „ì²´ ëª¨ë¸ ì¡°ë¦½

---

#### Week 8: LeRobot ë§ˆìŠ¤í„°

**Day 1-2: LeRobot ì½”ë“œ ì™„ì „ ë¶„ì„**
```bash
# LeRobot ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/huggingface/lerobot.git
cd lerobot

# êµ¬ì¡° íŒŒì•…
tree -L 2 lerobot/

# í•µì‹¬ íŒŒì¼ë“¤:
# - lerobot/common/policies/  # ë‹¤ì–‘í•œ policy êµ¬í˜„
# - lerobot/common/datasets/  # ë°ì´í„°ì…‹
# - lerobot/scripts/         # í•™ìŠµ/í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
```

**ë¶„ì„í•  íŒŒì¼:**
1. `lerobot/common/policies/act/modeling_act.py`
   - ACT (Action Chunking Transformer) êµ¬í˜„
   
2. `lerobot/common/policies/diffusion/modeling_diffusion.py`
   - Diffusion Policy êµ¬í˜„
   
3. `lerobot/common/datasets/lerobot_dataset.py`
   - ë°ì´í„° í¬ë§· ì´í•´

**ì²´í¬:**
- [ ] ACT ì•„í‚¤í…ì²˜ ì´í•´
- [ ] Diffusion Policy ê°œë… ì´í•´
- [ ] LeRobot ë°ì´í„° í¬ë§· ì´í•´

---

**Day 3-5: ë‹¤ì–‘í•œ Policy ì‹¤í—˜**
```bash
# 1. ACT Policy
python -m lerobot.scripts.train \
    --dataset lerobot/pusht \
    --policy act \
    --batch-size 32 \
    --num-epochs 100

# 2. Diffusion Policy
python -m lerobot.scripts.train \
    --dataset lerobot/pusht \
    --policy diffusion \
    --batch-size 32 \
    --num-epochs 100

# 3. ì„±ëŠ¥ ë¹„êµ
python -m lerobot.scripts.eval \
    --policy act \
    --checkpoint path/to/checkpoint

python -m lerobot.scripts.eval \
    --policy diffusion \
    --checkpoint path/to/checkpoint
```

**ë¹„êµí•  ê²ƒ:**
- [ ] í•™ìŠµ ì†ë„
- [ ] ìµœì¢… ì„±ëŠ¥
- [ ] ì•ˆì •ì„±
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

---

**Day 6-7: ìì‹ ì˜ ë°ì´í„°ë¡œ ì ìš©**
```python
# convert_to_lerobot.py
"""
Mini VLA ë°ì´í„° â†’ LeRobot í˜•ì‹ ë³€í™˜
"""

from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
from datasets import Dataset, Features, Image, Sequence, Value
import pickle
import numpy as np
from PIL import Image as PILImage

def convert_demonstrations():
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    with open('demonstrations.pkl', 'rb') as f:
        episodes = pickle.load(f)
    
    # LeRobot í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    data_dict = {
        'observation.image': [],
        'action': [],
        'episode_index': [],
        'frame_index': [],
        'timestamp': [],
    }
    
    for ep_idx, episode in enumerate(episodes):
        for frame_idx, (obs, action) in enumerate(
            zip(episode['obs'], episode['actions'])
        ):
            # ì´ë¯¸ì§€ë¥¼ PIL Imageë¡œ
            img = PILImage.fromarray(obs)
            
            data_dict['observation.image'].append(img)
            data_dict['action'].append(action.tolist())
            data_dict['episode_index'].append(ep_idx)
            data_dict['frame_index'].append(frame_idx)
            data_dict['timestamp'].append(frame_idx * 0.1)  # ì˜ˆì‹œ
    
    # HuggingFace Dataset ìƒì„±
    features = Features({
        'observation.image': Image(),
        'action': Sequence(Value('float32'), length=7),
        'episode_index': Value('int64'),
        'frame_index': Value('int64'),
        'timestamp': Value('float32'),
    })
    
    dataset = Dataset.from_dict(data_dict, features=features)
    
    # ì €ì¥
    dataset.save_to_disk('./my_robot_dataset')
    
    print(f"âœ… Converted {len(episodes)} episodes")
    print(f"   Total frames: {len(dataset)}")

if __name__ == '__main__':
    convert_demonstrations()
```
```bash
# LeRobotìœ¼ë¡œ í•™ìŠµ
python -m lerobot.scripts.train \
    --dataset ./my_robot_dataset \
    --policy act \
    --output-dir ./outputs

# í‰ê°€
python -m lerobot.scripts.eval \
    --policy act \
    --checkpoint ./outputs/checkpoints/last.pt
```

**ì²´í¬:**
- [ ] ë°ì´í„° ë³€í™˜ ì„±ê³µ
- [ ] LeRobotìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥
- [ ] ì„±ëŠ¥ì´ ìì‹ ì˜ Mini VLAë³´ë‹¤ ë‚˜ì€ì§€ í™•ì¸

---

**Week 7-8 ì™„ë£Œ ì²´í¬:**
```
âœ… RT-1 ì™„ì „ ì´í•´ (90%)
âœ… RT-1 í•µì‹¬ ìš”ì†Œ êµ¬í˜„
âœ… LeRobot ì½”ë“œ ì½ì„ ìˆ˜ ìˆìŒ
âœ… ë‹¤ì–‘í•œ policy ë¹„êµ ê°€ëŠ¥
âœ… ë³¸ì¸ ë°ì´í„°ì— LeRobot ì ìš©

â†’ ì´ì œ VLA "í•  ì¤„ ì•„ëŠ”" ìˆ˜ì¤€!
```

---

## ğŸ“Š 8ì£¼ ì™„ë£Œ í›„ ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì§€ì‹
- [ ] VLAê°€ ë­”ì§€ ëª…í™•íˆ ì„¤ëª… ê°€ëŠ¥
- [ ] RT-1/RT-2 ë…¼ë¬¸ ì´í•´ (80%+)
- [ ] Transformer ê°œë… ì´í•´ (60%+)
- [ ] Behavioral Cloning ì™„ì „ ì´í•´ (90%+)
- [ ] Vision Transformer ì´í•´ (70%+)

### ê¸°ìˆ 
- [ ] PyTorchë¡œ ëª¨ë¸ ììœ ë¡­ê²Œ ì‘ì„±
- [ ] ê°„ë‹¨í•œ VLA ì§ì ‘ êµ¬í˜„ ì™„ë£Œ
- [ ] LeRobot ì½”ë“œ ì½ê³  ìˆ˜ì • ê°€ëŠ¥
- [ ] ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ êµ¬ì¶• ê°€ëŠ¥
- [ ] ë°ì´í„° ìˆ˜ì§‘ â†’ í•™ìŠµ â†’ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì´í•´

### ê²½í—˜
- [ ] ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” VLA ë§Œë“¤ì–´ë´„
- [ ] ì‹¤íŒ¨ì™€ ë””ë²„ê¹… ê²½í—˜
- [ ] ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ê²½í—˜
- [ ] ë‹¤ì–‘í•œ policy ë¹„êµ ê²½í—˜

### ë§ˆì¸ë“œì…‹
- [ ] 100% ì´í•´ ì•ˆ í•´ë„ ì§„í–‰í•˜ëŠ” ìì‹ ê°
- [ ] ë§‰í˜€ë„ ê³„ì† ì „ì§„í•˜ëŠ” ìŠµê´€
- [ ] ì‹¤ìŠµ ì¤‘ì‹¬ í•™ìŠµ ì²´í™”
- [ ] Top-Down ë°©ì‹ì˜ íš¨ìœ¨ì„± ì²´ê°

---

## ğŸš€ 9ì£¼ì°¨ ì´í›„: Isaac Simìœ¼ë¡œ í™•ì¥

### Week 9-12: Isaac Sim ì „í™˜

**Week 9: Isaac Sim ê¸°ì´ˆ**
- Isaac Sim ì„¤ì¹˜
- ê¸°ë³¸ í™˜ê²½ êµ¬ì¶•
- ë¡œë´‡ ë¡œë“œ ë° ì œì–´
- ì¹´ë©”ë¼ ì…‹ì—…

**Week 10: ëª¨ë°”ì¼ ë§¤ë‹ˆí“°ë ˆì´í„°**
- ì´ë™ ë¡œë´‡ base
- ë§¤ë‹ˆí“°ë ˆì´í„° ì¥ì°©
- í†µí•© ì œì–´
- ROS2 ì—°ë™

**Week 11: ë¬¼ë¥˜ í™˜ê²½**
- ì°½ê³  í™˜ê²½ êµ¬ì„±
- íŒ”ë ˆíŠ¸/ë°•ìŠ¤ ì¶”ê°€
- ì¡°ëª… ì„¤ì •
- í˜„ì‹¤ì ì¸ í™˜ê²½

**Week 12: ì²« ë¬¼ë¥˜ VLA**
- Pallet grasping task
- ë°ì´í„° ìˆ˜ì§‘ (100+ episodes)
- VLA í•™ìŠµ
- í‰ê°€ (ëª©í‘œ: 60%+ ì„±ê³µë¥ )

### Week 13-18: ê³ ë„í™”

**Week 13-15: í”„ë¡œì íŠ¸ 2**
- Box sorting VLA
- Multi-task learning
- ROS2 ì™„ì „ í†µí•©

**Week 16-18: í¬íŠ¸í´ë¦¬ì˜¤**
- í”„ë¡œì íŠ¸ 3 (ì„ íƒ)
- GitHub ì •ë¦¬
- ë¸”ë¡œê·¸ ê¸€ ì‘ì„±
- ë°ëª¨ ë¹„ë””ì˜¤

---

## ğŸ’¡ í•™ìŠµ íŒ

### 1. ë§‰í˜€ë„ ê³„ì† ì§„í–‰
```
âŒ "Transformer ì™„ë²½íˆ ì´í•´í•˜ê³  ë‹¤ìŒ"
   â†’ 3ì£¼ ë‚­ë¹„

âœ… "Transformer ëŒ€ì¶© ì•Œê³  ì¼ë‹¨ ì§„í–‰"
   â†’ ë‚˜ì¤‘ì— í•„ìš”í•˜ë©´ ë‹¤ì‹œ ë°°ì›€
   â†’ í›¨ì”¬ íš¨ìœ¨ì !
```

### 2. ì´í•´ë„ 30%ë©´ ì¶©ë¶„
```
1ì£¼ì°¨: 10% â†’ "VLAê°€ ë­”ì§€ ëª¨ë¥´ê² ë‹¤"
2ì£¼ì°¨: 30% â†’ "ëŒ€ì¶© ë­”ì§€ ì•Œê² ëŠ”ë°..."
4ì£¼ì°¨: 60% â†’ "ì•„, ì´ë˜ì„œ ì´ë ‡ê²Œ í•˜ëŠ”êµ¬ë‚˜"
8ì£¼ì°¨: 80% â†’ "ë…¼ë¬¸ ë‹¤ì‹œ ì½ìœ¼ë‹ˆ ë‹¤ ë³´ì´ë„¤"

ì™„ë²½í•œ ì´í•´: í‰ìƒ ì•ˆ ì˜´, í•„ìš”ë„ ì—†ìŒ!
```

### 3. ì‹¤ìŠµì´ 80%
```
ì‹œê°„ ë°°ë¶„:
- ë…¼ë¬¸ ì½ê¸°: 10%
- ì´ë¡  ê³µë¶€: 10%
- ì½”ë”©: 40%
- ì‹¤í—˜: 30%
- ë””ë²„ê¹…: 10%

â†’ ì½ê¸° 20%, ì†ìœ¼ë¡œ 80%!
```

### 4. ì‘ì€ ì„±ê³µ ì¶•í•˜í•˜ê¸°
```
âœ… ì½”ë“œê°€ ëŒì•„ê° â†’ ì¶•í•˜!
âœ… Lossê°€ ê°ì†Œí•¨ â†’ ì¶•í•˜!
âœ… ë¡œë´‡ì´ ì¡°ê¸ˆ ì›€ì§ì„ â†’ ì¶•í•˜!
âœ… í•œ ë²ˆì´ë¼ë„ ì„±ê³µ â†’ ì¶•í•˜!

â†’ ë™ê¸°ë¶€ì—¬ ìœ ì§€ê°€ ê°€ì¥ ì¤‘ìš”!
```

### 5. ì¹œêµ¬ í™œìš©
```
í˜¼ì:
- ë§‰íˆë©´ í•˜ë£¨ ì¢…ì¼ í—¤ë§´
- ì˜ëª»ëœ ë°©í–¥ìœ¼ë¡œ ë©°ì¹ 
- ë™ê¸°ë¶€ì—¬ í•˜ë½

ì¹œêµ¬ì™€:
- ë§‰íˆë©´ ì¦‰ì‹œ ì§ˆë¬¸
- ì˜¬ë°”ë¥¸ ë°©í–¥ ì•ˆë‚´
- í•¨ê»˜ ì§„í–‰

â†’ í•™ìŠµ ì†ë„ 3ë°° ì°¨ì´!
```

---

## â“ FAQ

### Q1: "ì •ë§ 8ì£¼ë§Œì— ê°€ëŠ¥í•´?"

**A: ì¡°ê±´ë¶€ YES**

âœ… ê°€ëŠ¥í•œ ê²½ìš°:
- í”„ë¡œê·¸ë˜ë° ê¸°ë³¸ ìˆìŒ
- í•˜ë£¨ 2-3ì‹œê°„ íˆ¬ì
- 100% ì´í•´ ì•ˆ í•´ë„ ì§„í–‰
- ë§‰í˜€ë„ ê³„ì† ì „ì§„

âŒ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°:
- ì™„ë²½ì£¼ì˜ì
- ìˆœì„œëŒ€ë¡œ í•´ì•¼ í•˜ëŠ” ì„±ê²©
- í•˜ë£¨ 30ë¶„ë§Œ íˆ¬ì

---

### Q2: "ì´ë¡ ì´ ë¹ˆì•½í•˜ì§€ ì•Šë‚˜?"

**A: ëª©ì ì— ë”°ë¼ ë‹¤ë¦„**

VLA ì—°êµ¬ì ëª©í‘œ:
- Bottom-Up í•„ìš” (ë°•ì‚¬ ê³¼ì •)
- ìˆ˜í•™/ì´ë¡  ê¹Šì´ í•„ìˆ˜

VLA í™œìš©ì ëª©í‘œ (ë‹¹ì‹ ):
- Top-Down ì¶©ë¶„ (ì‹¤ë¬´)
- ì´ë¡ ì€ í•„ìš”í•œ ë§Œí¼ë§Œ

---

### Q3: "ë‚˜ì¤‘ì— êµ¬ë© ì•ˆ ë‚˜?"

**A: êµ¬ë©ì€ ìƒê¹€, í•˜ì§€ë§Œ...**

Top-Down:
- í° êµ¬ë© ì—†ìŒ (ì „ì²´ ê·¸ë¦¼ O)
- ì‘ì€ êµ¬ë© ìˆìŒ (ì„¸ë¶€ ì´ë¡ )
- í•„ìš”í•  ë•Œ ì±„ìš°ë©´ ë¨

Bottom-Up:
- ì‘ì€ êµ¬ë© ì—†ìŒ (ì™„ë²½í•œ ê¸°ì´ˆ)
- í° êµ¬ë© ê°€ëŠ¥ (ì‹¤ì „ ê°ê° X)
- 18ê°œì›” í›„ì—ë„ ì‹œì‘ ëª» í•¨

â†’ Top-Downì´ ë” ì‹¤ìš©ì !

---

### Q4: "ìœ¡ì•„í•˜ë©´ì„œ ê°€ëŠ¥í•œê°€?"

**A: ì¡°ì • í•„ìš”**

ì´ìƒì : 8ì£¼ (í•˜ë£¨ 2-3ì‹œê°„)
ìœ¡ì•„ ì¤‘: 12-16ì£¼ (í•˜ë£¨ 1ì‹œê°„)

ì „ëµ:
- í•˜ë£¨ 30ë¶„ì´ë¼ë„ ë§¤ì¼
- ì£¼ë§ì— ì§‘ì¤‘ (2-3ì‹œê°„)
- ì•„ì´ ì¬ìš¸ ë•Œ 1ì‹œê°„
- ì¹œêµ¬ì™€ ì˜¨ë¼ì¸ ìŠ¤í„°ë””

â†’ ëŠë ¤ë„ ê´œì°®ìŒ, ë°©í–¥ì´ ì¤‘ìš”!

---

## ğŸ¯ ìµœì¢… ë©”ì‹œì§€

### Top-Down í•™ìŠµì˜ ë³¸ì§ˆ
```
ì „í†µì :
ê¸°ì´ˆ â†’ ì‘ìš© â†’ ì‹¤ì „
â””â”€ ê¸°ì´ˆì—ë§Œ 6ê°œì›”

Top-Down:
ì‹¤ì „ â†’ í•„ìš”í•œ ê¸°ì´ˆ â†’ ê¹Šì´
â””â”€ 1ì£¼ì¼ì— ì „ì²´ ëŒë¦¼
â””â”€ 8ì£¼ë©´ ë§Œë“¤ì–´ë´„
â””â”€ ì¬ë¯¸ìˆì–´ì„œ ê³„ì†í•¨!
```

### ë‹¹ì‹ ì˜ 18ê°œì›”
```
Bottom-Up:
"18ê°œì›” í›„ì—ë„ ì´ë¡ ë§Œ..."

Top-Down:
"8ì£¼ í›„ ê°„ë‹¨í•œ VLA ì‘ë™" âœ…
"12ì£¼ í›„ ë¬¼ë¥˜ í”„ë¡œì íŠ¸ ì‹œì‘" âœ…
"18ê°œì›” í›„ í¬íŠ¸í´ë¦¬ì˜¤ 3ê°œ" âœ…
```

---

## ğŸ“ í•™ìŠµ ê¸°ë¡ í…œí”Œë¦¿

### ì£¼ê°„ íšŒê³ 
```markdown
## Week X íšŒê³ 

### ì™„ë£Œí•œ ê²ƒ
- [ ] 

### ë°°ìš´ ê²ƒ
- 

### ì–´ë ¤ì› ë˜ ê²ƒ
- 

### ë‹¤ìŒ ì£¼ ê³„íš
- [ ] 

### ì¹œêµ¬ì—ê²Œ ì§ˆë¬¸í•  ê²ƒ
- 
```

### í”„ë¡œì íŠ¸ ë…¸íŠ¸
```markdown
## í”„ë¡œì íŠ¸: [ì´ë¦„]

### ëª©í‘œ
- 

### ë°ì´í„°
- ì—í”¼ì†Œë“œ ìˆ˜:
- í”„ë ˆì„ ìˆ˜:
- í™˜ê²½:

### ëª¨ë¸
- ì•„í‚¤í…ì²˜:
- íŒŒë¼ë¯¸í„° ìˆ˜:

### ê²°ê³¼
- í•™ìŠµ Loss:
- ê²€ì¦ Loss:
- ì„±ê³µë¥ :

### êµí›ˆ
- 
```

---

## ğŸš€ ì‹œì‘í•˜ê¸°

### ì§€ê¸ˆ ë‹¹ì¥ (30ë¶„)
```bash
# 1. ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ
wget https://arxiv.org/pdf/2212.06817.pdf

# 2. LeRobot ì„¤ì¹˜
pip install lerobot

# 3. ì²« ì‹¤í–‰
python -m lerobot.scripts.visualize_dataset \
    --repo-id lerobot/pusht
```

### ì´ë²ˆ ì£¼ (Week 1)

- [ ] RT-1 ë…¼ë¬¸ ì½ê¸° (3ì‹œê°„)
- [ ] ì˜ìƒ ë³´ê¸° (2ì‹œê°„)
- [ ] LeRobot ëŒë ¤ë³´ê¸° (4ì‹œê°„)
- [ ] í•™ìŠµ ë…¸íŠ¸ ì‘ì„±
- [ ] ì¹œêµ¬ì—ê²Œ ì§„í–‰ ìƒí™© ê³µìœ 

### ì¹œêµ¬ì—ê²Œ ë¬¼ì–´ë³¼ ê²ƒ
```
1. "RT-1 ë…¼ë¬¸ì—ì„œ ì´ ë¶€ë¶„ì´ ì´í•´ ì•ˆ ê°€ëŠ”ë°..."
2. "LeRobotì´ ì´ë ‡ê²Œ ì‘ë™í•˜ëŠ” ê²Œ ë§ì•„?"
3. "ë‹¤ìŒ ì£¼ì— ë­ ê³µë¶€í•˜ë©´ ì¢‹ì„ê¹Œ?"
```

---