# VLA í•™ìŠµ ê°€ì´ë“œ - Phase 2

## ëª©ì°¨
- [ğŸ“… Phase 2: Bottom-Up ê¸°ì´ˆ íƒ„íƒ„íˆ (3-6ê°œì›”)](#-phase-2-bottom-up-ê¸°ì´ˆ-íƒ„íƒ„íˆ-3-6ê°œì›”)
- [Month 3-4: Deep Learning ì œëŒ€ë¡œ](#month-3-4-deep-learning-ì œëŒ€ë¡œ)
  - [Week 1-2: PyTorch ì‹¬í™”](#week-1-2-pytorch-ì‹¬í™”)
  - [Week 3-4: CNN ê¹Šì´ íŒŒê¸°](#week-3-4-cnn-ê¹Šì´-íŒŒê¸°)
  - [Week 5-6: Computer Vision í•µì‹¬](#week-5-6-computer-vision-í•µì‹¬)
- [Month 5: Transformer & Multi-modal](#month-5-transformer--multi-modal)
  - [Week 1: Attention ë©”ì»¤ë‹ˆì¦˜ ì™„ì „ ì •ë³µ](#week-1-attention-ë©”ì»¤ë‹ˆì¦˜-ì™„ì „-ì •ë³µ)
  - [Week 2: Transformer Encoder & Decoder](#week-2-transformer-encoder--decoder)
  - [Week 3: Vision Transformer (ViT)](#week-3-vision-transformer-vit)
  - [Week 4-6: Multi-modal Learning](#week-4-6-multi-modal-learning)
- [Month 6: Imitation Learning & RL ê¸°ì´ˆ](#month-6-imitation-learning--rl-ê¸°ì´ˆ)
  - [Week 1-2: Imitation Learning ì‹¬í™”](#week-1-2-imitation-learning-ì‹¬í™”)
  - [Week 3-4: RL ê¸°ì´ˆ (ìµœì†Œí•œ)](#week-3-4-rl-ê¸°ì´ˆ-ìµœì†Œí•œ)
- [ìˆ˜í•™ ê¸°ì´ˆ (Phase 2 ì „ì²´ ë³‘í–‰)](#ìˆ˜í•™-ê¸°ì´ˆ-phase-2-ì „ì²´-ë³‘í–‰)
- [Phase 2 ì™„ë£Œ ì²´í¬](#phase-2-ì™„ë£Œ-ì²´í¬)

## ğŸ“… Phase 2: Bottom-Up ê¸°ì´ˆ íƒ„íƒ„íˆ (3-6ê°œì›”)

### ëª©í‘œ
- Phase 1ì—ì„œ ë¶€ì¡±í–ˆë˜ ë¶€ë¶„ ì²´ê³„ì ìœ¼ë¡œ ì±„ìš°ê¸°
- ìˆ˜í•™, ë”¥ëŸ¬ë‹, CV ê¸°ì´ˆ ì œëŒ€ë¡œ
- ë…¼ë¬¸ ì½ì„ ìˆ˜ ìˆëŠ” ì‹¤ë ¥
- Imitation Learning & RL ê¸°ì´ˆ

---

## Month 3-4: Deep Learning ì œëŒ€ë¡œ

### Week 1-2: PyTorch ì‹¬í™”

**Phase 1ì—ì„œ ê¸°ë³¸ë§Œ â†’ ì´ì œ ì œëŒ€ë¡œ**

#### Custom Dataset & DataLoader
```python
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

class CustomRobotDataset(Dataset):
    """
    ë³µì¡í•œ ì „ì²˜ë¦¬ê°€ í¬í•¨ëœ Dataset
    """
    def __init__(self, data_dir, transform=None, augmentation=None):
        self.data_dir = data_dir
        self.transform = transform
        self.augmentation = augmentation
        
        # ë°ì´í„° ë¡œë“œ
        self.episodes = self.load_episodes(data_dir)
        
        # í†µê³„ ê³„ì‚°
        self.compute_statistics()
    
    def load_episodes(self, data_dir):
        # ë³µì¡í•œ ë¡œë”© ë¡œì§
        pass
    
    def compute_statistics(self):
        """
        ì •ê·œí™”ë¥¼ ìœ„í•œ í†µê³„ ê³„ì‚°
        """
        all_actions = []
        for episode in self.episodes:
            all_actions.extend(episode['actions'])
        
        all_actions = np.array(all_actions)
        self.action_mean = all_actions.mean(axis=0)
        self.action_std = all_actions.std(axis=0)
    
    def __len__(self):
        return sum(len(ep['obs']) for ep in self.episodes)
    
    def __getitem__(self, idx):
        # Episodeì™€ frame index ì°¾ê¸°
        episode_idx, frame_idx = self.get_episode_frame(idx)
        
        # ë°ì´í„° ì¶”ì¶œ
        obs = self.episodes[episode_idx]['obs'][frame_idx]
        action = self.episodes[episode_idx]['actions'][frame_idx]
        
        # Augmentation (ì„ íƒì )
        if self.augmentation:
            obs, action = self.augmentation(obs, action)
        
        # Transform
        if self.transform:
            obs = self.transform(obs)
        
        # Normalization
        action = (action - self.action_mean) / (self.action_std + 1e-8)
        
        return {
            'observation': torch.FloatTensor(obs),
            'action': torch.FloatTensor(action),
            'episode_idx': episode_idx,
            'frame_idx': frame_idx
        }

# íš¨ìœ¨ì ì¸ DataLoader ì„¤ì •
dataloader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,        # CPU ì½”ì–´ í™œìš©
    pin_memory=True,      # GPU ì „ì†¡ ë¹ ë¥´ê²Œ
    prefetch_factor=2,    # ë¯¸ë¦¬ ë¡œë“œ
    persistent_workers=True  # Worker ì¬ì‚¬ìš©
)
```

---

#### Custom Loss Functions
```python
class CustomLoss(nn.Module):
    """
    ë³µì¡í•œ ì»¤ìŠ¤í…€ Loss
    """
    def __init__(self, position_weight=1.0, velocity_weight=0.5):
        super().__init__()
        self.position_weight = position_weight
        self.velocity_weight = velocity_weight
    
    def forward(self, pred, target, mask=None):
        # Position loss
        pos_loss = F.mse_loss(pred['position'], target['position'], reduction='none')
        
        # Velocity loss
        vel_loss = F.mse_loss(pred['velocity'], target['velocity'], reduction='none')
        
        # Combined
        loss = self.position_weight * pos_loss + self.velocity_weight * vel_loss
        
        # Masking (for variable length sequences)
        if mask is not None:
            loss = loss * mask
            loss = loss.sum() / mask.sum()
        else:
            loss = loss.mean()
        
        return loss

# Smooth L1 Loss (Huber Loss)
class SmoothL1Loss(nn.Module):
    """
    Outlierì— robustí•œ loss
    """
    def __init__(self, beta=1.0):
        super().__init__()
        self.beta = beta
    
    def forward(self, pred, target):
        diff = torch.abs(pred - target)
        loss = torch.where(
            diff < self.beta,
            0.5 * diff ** 2 / self.beta,
            diff - 0.5 * self.beta
        )
        return loss.mean()
```

---

#### Learning Rate Scheduling
```python
# 1. Warmup + Cosine Annealing
from torch.optim.lr_scheduler import CosineAnnealingLR

class WarmupCosineScheduler:
    def __init__(self, optimizer, warmup_epochs, total_epochs, min_lr=1e-6):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.min_lr = min_lr
        self.base_lr = optimizer.param_groups[0]['lr']
        self.current_epoch = 0
    
    def step(self):
        if self.current_epoch < self.warmup_epochs:
            # Linear warmup
            lr = self.base_lr * (self.current_epoch + 1) / self.warmup_epochs
        else:
            # Cosine annealing
            progress = (self.current_epoch - self.warmup_epochs) / \
                      (self.total_epochs - self.warmup_epochs)
            lr = self.min_lr + (self.base_lr - self.min_lr) * \
                 0.5 * (1 + np.cos(np.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        self.current_epoch += 1
        return lr

# 2. OneCycleLR (ì¶”ì²œ!)
from torch.optim.lr_scheduler import OneCycleLR

scheduler = OneCycleLR(
    optimizer,
    max_lr=1e-3,
    epochs=num_epochs,
    steps_per_epoch=len(dataloader),
    pct_start=0.3,  # Warmup 30%
    anneal_strategy='cos',
    div_factor=25,  # initial_lr = max_lr/25
    final_div_factor=1e4  # final_lr = initial_lr/1e4
)

# ì‚¬ìš©
for epoch in range(num_epochs):
    for batch in dataloader:
        # Training step
        optimizer.step()
        scheduler.step()  # Batchë§ˆë‹¤ í˜¸ì¶œ!
```

---

#### Gradient Clipping & Accumulation
```python
# Gradient Clipping
torch.nn.utils.clip_grad_norm_(
    model.parameters(),
    max_norm=1.0  # Gradient norm ì œí•œ
)

# Gradient Accumulation (í° batch size ì‹œë®¬ë ˆì´ì…˜)
accumulation_steps = 4
optimizer.zero_grad()

for i, batch in enumerate(dataloader):
    # Forward
    output = model(batch)
    loss = criterion(output, batch['target'])
    
    # Normalize loss
    loss = loss / accumulation_steps
    
    # Backward
    loss.backward()
    
    # Update every accumulation_steps
    if (i + 1) % accumulation_steps == 0:
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        optimizer.zero_grad()
```

---

#### Mixed Precision Training
```python
from torch.cuda.amp import autocast, GradScaler

# Scaler ì´ˆê¸°í™”
scaler = GradScaler()

for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        
        # Mixed precision forward
        with autocast():
            output = model(batch['obs'])
            loss = criterion(output, batch['action'])
        
        # Scaled backward
        scaler.scale(loss).backward()
        
        # Unscale gradients & clip
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # Step
        scaler.step(optimizer)
        scaler.update()

# íš¨ê³¼: ë©”ëª¨ë¦¬ 50% ì ˆê°, ì†ë„ 1.5-2ë°°
```

---

#### GPU ë©”ëª¨ë¦¬ ìµœì í™”
```python
# 1. Gradient Checkpointing
import torch.utils.checkpoint as checkpoint

class CheckpointedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = HeavyLayer()
        self.layer2 = HeavyLayer()
        self.layer3 = HeavyLayer()
    
    def forward(self, x):
        # Checkpointìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        x = checkpoint.checkpoint(self.layer1, x)
        x = checkpoint.checkpoint(self.layer2, x)
        x = self.layer3(x)
        return x

# 2. ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
import torch.cuda

print(f"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
print(f"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB")

# ìƒì„¸ ë¶„ì„
print(torch.cuda.memory_summary())

# 3. ë©”ëª¨ë¦¬ ì •ë¦¬
torch.cuda.empty_cache()
```

---

**ìë£Œ:**
- PyTorch ê³µì‹ íŠœí† ë¦¬ì–¼ ì „ì²´
- "Deep Learning with PyTorch" ì±… (ì„ íƒ)

**ì‹œê°„: ì£¼ 5-7ì‹œê°„**

---

### Week 3-4: CNN ê¹Šì´ íŒŒê¸°

**Phase 1: ViTë§Œ ì‚¬ìš© â†’ ì´ì œ CNNë„**

#### CNN ê¸°ë³¸ êµ¬ì¡° ì´í•´
```python
import torch.nn as nn
import torch.nn.functional as F

# 1. Basic CNN Block
class BasicCNNBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
    
    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        return x

# 2. LeNet (ê¸°ì´ˆ)
class LeNet(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)
    
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

---

#### ResNet êµ¬í˜„ (Skip Connectionì˜ ì¤‘ìš”ì„±)
```python
class ResidualBlock(nn.Module):
    """
    ResNetì˜ í•µì‹¬: Skip Connection
    
    ì™œ ì¤‘ìš”í•œê°€?
    - Gradient vanishing ë°©ì§€
    - ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥
    - Identity mapping í•™ìŠµ ìš©ì´
    """
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        # Main path
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # Skip connection (shortcut)
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.shortcut = nn.Identity()
    
    def forward(self, x):
        identity = self.shortcut(x)
        
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        out += identity  # Skip connection!
        out = F.relu(out)
        
        return out

class ResNet18(nn.Module):
    """
    ResNet-18 êµ¬í˜„
    """
    def __init__(self, num_classes=1000):
        super().__init__()
        
        # Initial layers
        self.conv1 = nn.Conv2d(3, 64, 7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)
        
        # Residual layers
        self.layer1 = self.make_layer(64, 64, 2, stride=1)
        self.layer2 = self.make_layer(64, 128, 2, stride=2)
        self.layer3 = self.make_layer(128, 256, 2, stride=2)
        self.layer4 = self.make_layer(256, 512, 2, stride=2)
        
        # Final layers
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
    
    def make_layer(self, in_channels, out_channels, num_blocks, stride):
        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride))
        for _ in range(1, num_blocks):
            layers.append(ResidualBlock(out_channels, out_channels, 1))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # Initial
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)
        
        # Residual blocks
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        # Classification
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        
        return x
```

---

#### MobileNet (Efficient CNN)
```python
class DepthwiseSeparableConv(nn.Module):
    """
    MobileNetì˜ í•µì‹¬: Depthwise Separable Convolution
    
    ì¼ë°˜ Conv: íŒŒë¼ë¯¸í„° = K Ã— K Ã— C_in Ã— C_out
    DW Conv: íŒŒë¼ë¯¸í„° = K Ã— K Ã— C_in + C_in Ã— C_out
    
    â†’ íŒŒë¼ë¯¸í„° ìˆ˜ ëŒ€í­ ê°ì†Œ!
    """
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        # Depthwise: ê° ì±„ë„ ë…ë¦½ì ìœ¼ë¡œ convolution
        self.depthwise = nn.Conv2d(
            in_channels, in_channels,
            kernel_size=3, stride=stride, padding=1,
            groups=in_channels  # í•µì‹¬!
        )
        self.bn1 = nn.BatchNorm2d(in_channels)
        
        # Pointwise: 1x1 convë¡œ ì±„ë„ ë¯¹ì‹±
        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
    
    def forward(self, x):
        x = F.relu(self.bn1(self.depthwise(x)))
        x = F.relu(self.bn2(self.pointwise(x)))
        return x

class MobileNetV2Block(nn.Module):
    """
    MobileNetV2: Inverted Residual Block
    """
    def __init__(self, in_channels, out_channels, stride, expand_ratio=6):
        super().__init__()
        hidden_dim = in_channels * expand_ratio
        self.use_residual = stride == 1 and in_channels == out_channels
        
        layers = []
        
        # Expansion
        if expand_ratio != 1:
            layers.extend([
                nn.Conv2d(in_channels, hidden_dim, 1),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU6(inplace=True)
            ])
        
        # Depthwise
        layers.extend([
            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True)
        ])
        
        # Projection
        layers.extend([
            nn.Conv2d(hidden_dim, out_channels, 1),
            nn.BatchNorm2d(out_channels)
        ])
        
        self.conv = nn.Sequential(*layers)
    
    def forward(self, x):
        if self.use_residual:
            return x + self.conv(x)
        else:
            return self.conv(x)
```

---

#### EfficientNet ê°œë…
```python
"""
EfficientNetì˜ í•µì‹¬ ì•„ì´ë””ì–´:
1. Compound Scaling
   - Depth (ë ˆì´ì–´ ìˆ˜)
   - Width (ì±„ë„ ìˆ˜)
   - Resolution (ì…ë ¥ í¬ê¸°)
   â†’ ì„¸ ê°€ì§€ë¥¼ ê· í˜•ìˆê²Œ ì¡°ì ˆ!

2. Optimal scaling coefficients
   depth: d = Î±^Ï†
   width: w = Î²^Ï†
   resolution: r = Î³^Ï†
   
   where Î±Â·Î²Â²Â·Î³Â² â‰ˆ 2

3. MBConv (Mobile Inverted Bottleneck)
   - Squeeze-and-Excitation
   - Swish activation
"""

class SEBlock(nn.Module):
    """
    Squeeze-and-Excitation Block
    ì±„ë„ ê°„ ê´€ê³„ ëª¨ë¸ë§
    """
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.squeeze = nn.AdaptiveAvgPool2d(1)
        self.excitation = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        b, c, _, _ = x.size()
        
        # Squeeze
        y = self.squeeze(x).view(b, c)
        
        # Excitation
        y = self.excitation(y).view(b, c, 1, 1)
        
        # Scale
        return x * y.expand_as(x)
```

---

**ìë£Œ:**
- CS231n Lecture 9 (CNN Architectures)
- "Dive into Deep Learning" Chapter 7-8
- Papers: ResNet, MobileNet, EfficientNet

**ì‹œê°„: ì£¼ 5-7ì‹œê°„**

---

### Week 5-6: Computer Vision í•µì‹¬

#### Image Classification
```python
# Transfer Learning ì˜ˆì‹œ
import torchvision.models as models
from torchvision import transforms

# Pre-trained model ë¡œë“œ
model = models.resnet50(pretrained=True)

# ë§ˆì§€ë§‰ layer êµì²´ (Fine-tuning)
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# Feature extraction vs Fine-tuning
# 1. Feature extraction: CNN ë¶€ë¶„ freeze
for param in model.parameters():
    param.requires_grad = False
model.fc.requires_grad = True  # ë§ˆì§€ë§‰ layerë§Œ í•™ìŠµ

# 2. Fine-tuning: ì „ì²´ í•™ìŠµ (ë‚®ì€ LR)
optimizer = torch.optim.Adam([
    {'params': model.layer4.parameters(), 'lr': 1e-4},
    {'params': model.fc.parameters(), 'lr': 1e-3}
])

# Data Augmentation
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])
```

---

#### Object Detection ê¸°ì´ˆ
```python
"""
Object Detectionì˜ ë°œì „:

1. Two-stage detectors (Faster R-CNN)
   - Region Proposal â†’ Classification
   - ëŠë¦¬ì§€ë§Œ ì •í™•

2. One-stage detectors (YOLO, SSD)
   - í•œ ë²ˆì— ì˜ˆì¸¡
   - ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ ë‚®ìŒ

3. Modern (EfficientDet, DETR)
   - Transformer ê¸°ë°˜
   - ë¹ ë¥´ê³  ì •í™•
"""

# YOLO ì‚¬ìš© ì˜ˆì‹œ (ì‹¤ì „)
from ultralytics import YOLO

# Pre-trained model
model = YOLO('yolov8n.pt')

# Fine-tuning on custom data
model.train(
    data='custom_data.yaml',
    epochs=100,
    imgsz=640,
    batch=16,
    name='custom_detector'
)

# Inference
results = model('image.jpg')
boxes = results[0].boxes  # Bounding boxes
```

---

#### Semantic Segmentation
```python
class UNet(nn.Module):
    """
    U-Net: Semantic Segmentationì˜ ê¸°ë³¸
    
    êµ¬ì¡°:
    - Encoder (Contracting path): Feature ì¶”ì¶œ
    - Decoder (Expanding path): Upsampling
    - Skip connections: Detail ë³´ì¡´
    """
    def __init__(self, in_channels=3, num_classes=2):
        super().__init__()
        
        # Encoder
        self.enc1 = self.conv_block(in_channels, 64)
        self.enc2 = self.conv_block(64, 128)
        self.enc3 = self.conv_block(128, 256)
        self.enc4 = self.conv_block(256, 512)
        
        # Bottleneck
        self.bottleneck = self.conv_block(512, 1024)
        
        # Decoder
        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = self.conv_block(1024, 512)  # 1024 = 512 + 512 (skip)
        
        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = self.conv_block(512, 256)
        
        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = self.conv_block(256, 128)
        
        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = self.conv_block(128, 64)
        
        # Output
        self.out = nn.Conv2d(64, num_classes, 1)
        
        self.pool = nn.MaxPool2d(2, 2)
    
    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        # Encoder
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        enc3 = self.enc3(self.pool(enc2))
        enc4 = self.enc4(self.pool(enc3))
        
        # Bottleneck
        bottleneck = self.bottleneck(self.pool(enc4))
        
        # Decoder with skip connections
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat([dec4, enc4], dim=1)  # Skip connection
        dec4 = self.dec4(dec4)
        
        dec3 = self.upconv3(dec4)
        dec3 = torch.cat([dec3, enc3], dim=1)
        dec3 = self.dec3(dec3)
        
        dec2 = self.upconv2(dec3)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.dec2(dec2)
        
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.dec1(dec1)
        
        return self.out(dec1)
```

---

#### ë¯¸ë‹ˆ í”„ë¡œì íŠ¸: ë¬¼ë¥˜ ë°•ìŠ¤ íƒì§€ê¸°
```python
# YOLOv8ë¡œ custom object detector ë§Œë“¤ê¸°

# 1. ë°ì´í„° ì¤€ë¹„
"""
dataset/
  images/
    train/
      img1.jpg
      img2.jpg
    val/
      img3.jpg
  labels/
    train/
      img1.txt  # YOLO format
      img2.txt
    val/
      img3.txt
"""

# 2. data.yaml ì‘ì„±
"""
train: dataset/images/train
val: dataset/images/val

nc: 3  # number of classes
names: ['small_box', 'medium_box', 'large_box']
"""

# 3. Training
from ultralytics import YOLO

model = YOLO('yolov8n.pt')

results = model.train(
    data='boxes.yaml',
    epochs=100,
    imgsz=640,
    batch=16,
    name='box_detector',
    patience=20,  # Early stopping
    save=True,
    plots=True
)

# 4. Evaluation
metrics = model.val()
print(f"mAP50: {metrics.box.map50}")
print(f"mAP50-95: {metrics.box.map}")

# 5. Inference
results = model.predict('test_image.jpg', save=True)

# 6. Export for deployment
model.export(format='onnx')  # For faster inference
```

---

**ìë£Œ:**
- CS231n Lecture 11 (Detection and Segmentation)
- YOLO papers and tutorials
- U-Net paper

**ì‹œê°„: ì£¼ 5-7ì‹œê°„**

---

### Month 3-4 ì™„ë£Œ ì²´í¬
```
âœ… PyTorch ììœ ìì¬ (80%)
âœ… CNN ì•„í‚¤í…ì²˜ ì´í•´ (70%)
âœ… CV ì£¼ìš” task ì´í•´ (60%)
âœ… Transfer Learning ê²½í—˜
âœ… Custom Object Detector êµ¬í˜„

â†’ Transformer ì¤€ë¹„ ì™„ë£Œ!
```

---

## Month 5: Transformer & Multi-modal

### Week 1: Attention ë©”ì»¤ë‹ˆì¦˜ ì™„ì „ ì •ë³µ

#### Scaled Dot-Product Attention
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class ScaledDotProductAttention(nn.Module):
    """
    Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
    
    ì™œ scaling (âˆšd_k)?
    - QK^Tì˜ ê°’ì´ ì»¤ì§€ë©´ softmaxê°€ saturation
    - Gradient vanishing ë°©ì§€
    """
    def __init__(self, d_k):
        super().__init__()
        self.d_k = d_k
    
    def forward(self, Q, K, V, mask=None):
        """
        Args:
            Q: (batch, num_heads, seq_len, d_k)
            K: (batch, num_heads, seq_len, d_k)
            V: (batch, num_heads, seq_len, d_v)
            mask: (batch, 1, seq_len, seq_len) or None
        
        Returns:
            output: (batch, num_heads, seq_len, d_v)
            attention_weights: (batch, num_heads, seq_len, seq_len)
        """
        # Attention scores: QK^T
        scores = torch.matmul(Q, K.transpose(-2, -1))  # (B, H, L, L)
        
        # Scaling
        scores = scores / np.sqrt(self.d_k)
        
        # Masking (for decoder, padding, etc.)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Softmax
        attention_weights = F.softmax(scores, dim=-1)  # (B, H, L, L)
        
        # Weighted sum of values
        output = torch.matmul(attention_weights, V)  # (B, H, L, d_v)
        
        return output, attention_weights

# ì§ê´€ì  ì´í•´ë¥¼ ìœ„í•œ ì˜ˆì‹œ
def attention_example():
    """
    Attentionì˜ ì§ê´€
    """
    # ì˜ˆ: "The cat sat on the mat"
    # Query: "sat"ì´ ë¬´ì—‡ì„ ì£¼ëª©í•´ì•¼ í•˜ëŠ”ê°€?
    
    Q = torch.tensor([
        [0.1, 0.2, 0.9, 0.1, 0.2, 0.1]  # "sat"ì˜ query
    ])
    
    K = torch.tensor([
        [0.9, 0.1, 0.1, 0.1, 0.1, 0.1],  # "The"
        [0.1, 0.9, 0.1, 0.1, 0.1, 0.1],  # "cat"
        [0.1, 0.2, 0.9, 0.1, 0.2, 0.1],  # "sat"
        [0.1, 0.1, 0.1, 0.9, 0.1, 0.1],  # "on"
        [0.1, 0.1, 0.1, 0.1, 0.9, 0.1],  # "the"
        [0.1, 0.1, 0.1, 0.1, 0.1, 0.9],  # "mat"
    ])
    
    # Attention scores
    scores = Q @ K.T
    attention_weights = F.softmax(scores, dim=-1)
    
    print("Attention weights:")
    print("sat attends to:")
    for i, word in enumerate(["The", "cat", "sat", "on", "the", "mat"]):
        print(f"  {word}: {attention_weights[0, i]:.3f}")
    
    # Output: "sat"ì€ "cat"ê³¼ "mat"ì— ì£¼ëª©!
```

---

#### Multi-Head Attention
```python
class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention
    
    ì™œ ì—¬ëŸ¬ head?
    - ë‹¤ì–‘í•œ ê´€ì ì—ì„œ attention
    - Head 1: ë¬¸ë²•ì  ê´€ê³„
    - Head 2: ì˜ë¯¸ì  ê´€ê³„
    - Head 3: ê±°ë¦¬ ì •ë³´
    - ...
    """
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Linear projections
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention(self.d_k)
    
    def split_heads(self, x, batch_size):
        """
        (batch, seq_len, d_model) â†’ (batch, num_heads, seq_len, d_k)
        """
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        return x.transpose(1, 2)
    
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        # 1. Linear projections
        Q = self.W_q(Q)  # (B, L, d_model)
        K = self.W_k(K)
        V = self.W_v(V)
        
        # 2. Split into multiple heads
        Q = self.split_heads(Q, batch_size)  # (B, H, L, d_k)
        K = self.split_heads(K, batch_size)
        V = self.split_heads(V, batch_size)
        
        # 3. Attention
        output, attention_weights = self.attention(Q, K, V, mask)
        
        # 4. Concatenate heads
        output = output.transpose(1, 2).contiguous()  # (B, L, H, d_k)
        output = output.view(batch_size, -1, self.d_model)  # (B, L, d_model)
        
        # 5. Final linear
        output = self.W_o(output)
        
        return output, attention_weights

# ì‹œê°í™”
def visualize_attention():
    """
    Attention weights ì‹œê°í™”
    """
    import matplotlib.pyplot as plt
    
    model = MultiHeadAttention(d_model=512, num_heads=8)
    
    # Dummy input
    seq_len = 10
    x = torch.randn(1, seq_len, 512)
    
    output, attn_weights = model(x, x, x)
    
    # Plot attention for each head
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    for head in range(8):
        ax = axes[head // 4, head % 4]
        attn = attn_weights[0, head].detach().numpy()
        im = ax.imshow(attn, cmap='hot', interpolation='nearest')
        ax.set_title(f'Head {head+1}')
        ax.set_xlabel('Key')
        ax.set_ylabel('Query')
        plt.colorbar(im, ax=ax)
    
    plt.tight_layout()
    plt.savefig('attention_heads.png')
    plt.show()
```

---

#### Position Encoding
```python
class PositionalEncoding(nn.Module):
    """
    ìœ„ì¹˜ ì •ë³´ ì¸ì½”ë”©
    
    ì™œ í•„ìš”?
    - Attentionì€ ìˆœì„œ ì •ë³´ ì—†ìŒ
    - ìœ„ì¹˜ì— ë”°ë¥¸ ê³ ìœ í•œ íŒ¨í„´ ë¶€ì—¬
    
    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    """
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        
        # Create positional encoding matrix
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * 
            (-np.log(10000.0) / d_model)
        )
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, d_model)
        Returns:
            x + positional encoding
        """
        x = x + self.pe[:, :x.size(1), :]
        return x

# ì‹œê°í™”
def visualize_positional_encoding():
    import matplotlib.pyplot as plt
    
    pe = PositionalEncoding(d_model=128, max_len=100)
    
    # Extract PE matrix
    pos_enc = pe.pe[0].numpy()
    
    plt.figure(figsize=(12, 8))
    plt.imshow(pos_enc.T, cmap='RdBu', aspect='auto')
    plt.xlabel('Position')
    plt.ylabel('Dimension')
    plt.title('Positional Encoding')
    plt.colorbar()
    plt.savefig('positional_encoding.png')
    plt.show()
```

---

**ìë£Œ:**
- "Attention is All You Need" ë…¼ë¬¸
- "The Annotated Transformer" (Harvard NLP)
- CS224n Lecture 9

**ì‹œê°„: ì£¼ 8-10ì‹œê°„**

---

### Week 2: Transformer Encoder & Decoder

#### Transformer Encoder Layer
```python
class TransformerEncoderLayer(nn.Module):
    """
    Transformer Encoder Layer
    
    êµ¬ì¡°:
    1. Multi-Head Self-Attention
    2. Add & Norm
    3. Feed-Forward Network
    4. Add & Norm
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # Multi-Head Attention
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        
        # Feed-Forward Network
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        """
        Args:
            x: (batch, seq_len, d_model)
            mask: (batch, 1, seq_len, seq_len) or None
        """
        # 1. Self-Attention + Residual + Norm
        attn_output, _ = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # 2. Feed-Forward + Residual + Norm
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x

class TransformerEncoder(nn.Module):
    """
    Complete Transformer Encoder
    """
    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
    
    def forward(self, x, mask=None):
        for layer in self.layers:
            x = layer(x, mask)
        return x
```

---

#### Transformer Decoder Layer
```python
class TransformerDecoderLayer(nn.Module):
    """
    Transformer Decoder Layer
    
    êµ¬ì¡°:
    1. Masked Multi-Head Self-Attention
    2. Add & Norm
    3. Multi-Head Cross-Attention (with encoder output)
    4. Add & Norm
    5. Feed-Forward Network
    6. Add & Norm
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # Masked Self-Attention
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        
        # Cross-Attention
        self.cross_attn = MultiHeadAttention(d_model, num_heads)
        
        # Feed-Forward
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        # Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        """
        Args:
            x: (batch, tgt_len, d_model) - decoder input
            encoder_output: (batch, src_len, d_model)
            src_mask: mask for encoder output
            tgt_mask: causal mask for decoder (prevents looking ahead)
        """
        # 1. Masked Self-Attention
        self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(self_attn_output))
        
        # 2. Cross-Attention (attend to encoder output)
        cross_attn_output, _ = self.cross_attn(
            x, encoder_output, encoder_output, src_mask
        )
        x = self.norm2(x + self.dropout(cross_attn_output))
        
        # 3. Feed-Forward
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))
        
        return x

def create_causal_mask(seq_len):
    """
    Causal mask for decoder
    - Prevents attending to future tokens
    """
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    mask = mask.masked_fill(mask == 1, float('-inf'))
    return mask
```

---

#### Complete Transformer
```python
class Transformer(nn.Module):
    """
    Complete Transformer for sequence-to-sequence
    """
    def __init__(
        self,
        src_vocab_size,
        tgt_vocab_size,
        d_model=512,
        num_heads=8,
        num_encoder_layers=6,
        num_decoder_layers=6,
        d_ff=2048,
        dropout=0.1,
        max_len=5000
    ):
        super().__init__()
        
        # Embeddings
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        
        # Positional Encoding
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        
        # Encoder
        self.encoder = TransformerEncoder(
            num_encoder_layers, d_model, num_heads, d_ff, dropout
        )
        
        # Decoder
        self.decoder_layers = nn.ModuleList([
            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_decoder_layers)
        ])
        
        # Output projection
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
        self.dropout = nn.Dropout(dropout)
        self.d_model = d_model
    
    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        """
        Args:
            src: (batch, src_len)
            tgt: (batch, tgt_len)
        """
        # Encode
        src_emb = self.dropout(self.pos_encoding(
            self.src_embedding(src) * np.sqrt(self.d_model)
        ))
        encoder_output = self.encoder(src_emb, src_mask)
        
        # Decode
        tgt_emb = self.dropout(self.pos_encoding(
            self.tgt_embedding(tgt) * np.sqrt(self.d_model)
        ))
        
        dec_output = tgt_emb
        for layer in self.decoder_layers:
            dec_output = layer(dec_output, encoder_output, src_mask, tgt_mask)
        
        # Output
        output = self.fc_out(dec_output)
        
        return output

# ê°„ë‹¨í•œ ë²ˆì—­ taskë¡œ í…ŒìŠ¤íŠ¸
def test_transformer():
    # Hyperparameters
    src_vocab_size = 10000
    tgt_vocab_size = 10000
    
    model = Transformer(src_vocab_size, tgt_vocab_size)
    
    # Dummy data
    src = torch.randint(0, src_vocab_size, (32, 20))  # (batch, src_len)
    tgt = torch.randint(0, tgt_vocab_size, (32, 15))  # (batch, tgt_len)
    
    # Masks
    tgt_mask = create_causal_mask(15)
    
    # Forward
    output = model(src, tgt, tgt_mask=tgt_mask)
    
    print(f"Output shape: {output.shape}")  # (32, 15, tgt_vocab_size)
```

---

**í”„ë¡œì íŠ¸: ê°„ë‹¨í•œ ë²ˆì—­ ëª¨ë¸**
```python
# ì˜ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ (ì‘ì€ ë°ì´í„°ì…‹)

import torch
from torch.utils.data import Dataset, DataLoader

class TranslationDataset(Dataset):
    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab):
        self.src_sentences = src_sentences
        self.tgt_sentences = tgt_sentences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
    
    def __len__(self):
        return len(self.src_sentences)
    
    def __getitem__(self, idx):
        src = [self.src_vocab.get(w, self.src_vocab['<unk>']) 
               for w in self.src_sentences[idx].split()]
        tgt = [self.tgt_vocab.get(w, self.tgt_vocab['<unk>']) 
               for w in self.tgt_sentences[idx].split()]
        
        return torch.LongTensor(src), torch.LongTensor(tgt)

# Training
def train_translation_model():
    model = Transformer(src_vocab_size, tgt_vocab_size)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)
    
    for epoch in range(num_epochs):
        for src, tgt in dataloader:
            # Prepare target input and output
            tgt_input = tgt[:, :-1]  # Remove last token
            tgt_output = tgt[:, 1:]  # Remove first token
            
            # Create masks
            tgt_mask = create_causal_mask(tgt_input.size(1))
            
            # Forward
            output = model(src, tgt_input, tgt_mask=tgt_mask)
            
            # Loss
            output = output.reshape(-1, output.size(-1))
            tgt_output = tgt_output.reshape(-1)
            loss = criterion(output, tgt_output)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
        
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```

---

**ì‹œê°„: ì£¼ 8-10ì‹œê°„**

---

### Week 3: Vision Transformer (ViT)
```python
class PatchEmbedding(nn.Module):
    """
    ì´ë¯¸ì§€ â†’ Patch â†’ Embedding
    
    ì˜ˆ: 224x224 ì´ë¯¸ì§€, 16x16 patch
    â†’ 196ê°œ patch (14x14)
    â†’ ê° patchë¥¼ 768ì°¨ì› ë²¡í„°ë¡œ
    """
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        
        # Convolutionìœ¼ë¡œ patch embedding
        # (3, 224, 224) â†’ (768, 14, 14)
        self.proj = nn.Conv2d(
            in_channels, embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )
    
    def forward(self, x):
        """
        Args:
            x: (B, C, H, W)
        Returns:
            patches: (B, num_patches, embed_dim)
        """
        x = self.proj(x)  # (B, embed_dim, H/P, W/P)
        x = x.flatten(2)  # (B, embed_dim, num_patches)
        x = x.transpose(1, 2)  # (B, num_patches, embed_dim)
        return x

class VisionTransformer(nn.Module):
    """
    Vision Transformer (ViT)
    
    êµ¬ì¡°:
    1. Patch Embedding
    2. [CLS] token ì¶”ê°€
    3. Position Embedding
    4. Transformer Encoder
    5. Classification Head ([CLS] token ì‚¬ìš©)
    """
    def __init__(
        self,
        img_size=224,
        patch_size=16,
        num_classes=1000,
        embed_dim=768,
        depth=12,
        num_heads=12,
        mlp_ratio=4.0,
        dropout=0.1
    ):
        super().__init__()
        
        # Patch Embedding
        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)
        num_patches = self.patch_embed.num_patches
        
        # [CLS] token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # Position Embedding
        self.pos_embed = nn.Parameter(
            torch.zeros(1, num_patches + 1, embed_dim)
        )
        
        # Dropout
        self.pos_drop = nn.Dropout(p=dropout)
        
        # Transformer Encoder
        d_ff = int(embed_dim * mlp_ratio)
        self.encoder = TransformerEncoder(
            num_layers=depth,
            d_model=embed_dim,
            num_heads=num_heads,
            d_ff=d_ff,
            dropout=dropout
        )
        
        # Classification head
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)
        
        # Initialize weights
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.cls_token, std=0.02)
    
    def forward(self, x):
        """
        Args:
            x: (B, 3, H, W)
        Returns:
            logits: (B, num_classes)
        """
        B = x.shape[0]
        
        # Patch embedding
        x = self.patch_embed(x)  # (B, num_patches, embed_dim)
        
        # Add [CLS] token
        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)
        x = torch.cat([cls_tokens, x], dim=1)  # (B, num_patches+1, embed_dim)
        
        # Add position embedding
        x = x + self.pos_embed
        x = self.pos_drop(x)
        
        # Transformer Encoder
        x = self.encoder(x)
        
        # Classification (use [CLS] token)
        x = self.norm(x)
        cls_output = x[:, 0]  # (B, embed_dim)
        logits = self.head(cls_output)  # (B, num_classes)
        
        return logits

# ì‚¬ìš© ì˜ˆì‹œ
model = VisionTransformer(
    img_size=224,
    patch_size=16,
    num_classes=1000,
    embed_dim=768,
    depth=12,
    num_heads=12
)

# Pre-trained ViT ì‚¬ìš© (ì‹¤ì „)
from transformers import ViTModel, ViTConfig

# Option 1: From config
config = ViTConfig(
    image_size=224,
    patch_size=16,
    num_channels=3,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12
)
model = ViTModel(config)

# Option 2: Pre-trained
model = ViTModel.from_pretrained('google/vit-base-patch16-224')
```

---

**ìë£Œ:**
- "An Image is Worth 16x16 Words" ë…¼ë¬¸
- ViT ê³µì‹ ì½”ë“œ ë¶„ì„

**ì‹œê°„: ì£¼ 8-10ì‹œê°„**

---

### Week 4-6: Multi-modal Learning

#### CLIP: Contrastive Language-Image Pre-training
```python
class CLIP(nn.Module):
    """
    CLIP: ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ê°™ì€ ê³µê°„ì— ë§¤í•‘
    
    í•µì‹¬ ì•„ì´ë””ì–´:
    - ë§¤ì¹­ë˜ëŠ” (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒ: ê°€ê¹ê²Œ
    - ë§¤ì¹­ ì•ˆ ë˜ëŠ” ìŒ: ë©€ê²Œ
    """
    def __init__(self, image_encoder, text_encoder, embed_dim=512):
        super().__init__()
        
        self.image_encoder = image_encoder
        self.text_encoder = text_encoder
        
        # Projection heads
        self.image_projection = nn.Linear(image_encoder.output_dim, embed_dim)
        self.text_projection = nn.Linear(text_encoder.output_dim, embed_dim)
        
        # Temperature parameter (learnable)
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
    
    def forward(self, images, texts):
        """
        Args:
            images: (B, 3, H, W)
            texts: (B, seq_len)
        Returns:
            logits_per_image: (B, B)
            logits_per_text: (B, B)
        """
        # Encode
        image_features = self.image_encoder(images)
        text_features = self.text_encoder(texts)
        
        # Project to common space
        image_embeds = self.image_projection(image_features)
        text_embeds = self.text_projection(text_features)
        
        # Normalize
        image_embeds = F.normalize(image_embeds, dim=-1)
        text_embeds = F.normalize(text_embeds, dim=-1)
        
        # Compute similarity
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * image_embeds @ text_embeds.t()
        logits_per_text = logits_per_image.t()
        
        return logits_per_image, logits_per_text

def contrastive_loss(logits_per_image, logits_per_text):
    """
    Contrastive Loss (InfoNCE)
    
    ëª©í‘œ: Diagonalì„ maximize, off-diagonalì„ minimize
    """
    batch_size = logits_per_image.shape[0]
    labels = torch.arange(batch_size).to(logits_per_image.device)
    
    # Image to text
    loss_i = F.cross_entropy(logits_per_image, labels)
    
    # Text to image
    loss_t = F.cross_entropy(logits_per_text, labels)
    
    # Total loss
    loss = (loss_i + loss_t) / 2
    
    return loss

# Training
def train_clip():
    model = CLIP(image_encoder, text_encoder)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    for epoch in range(num_epochs):
        for images, texts in dataloader:
            # Forward
            logits_per_image, logits_per_text = model(images, texts)
            
            # Loss
            loss = contrastive_loss(logits_per_image, logits_per_text)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

# Zero-shot classification
def zero_shot_classify(model, image, class_names):
    """
    CLIPì˜ ê°•ë ¥í•œ ê¸°ëŠ¥: Zero-shot classification
    """
    # Encode image
    image_features = model.image_encoder(image.unsqueeze(0))
    image_embeds = F.normalize(
        model.image_projection(image_features), dim=-1
    )
    
    # Encode class names
    text_prompts = [f"a photo of a {name}" for name in class_names]
    text_tokens = tokenizer(text_prompts)
    text_features = model.text_encoder(text_tokens)
    text_embeds = F.normalize(
        model.text_projection(text_features), dim=-1
    )
    
    # Compute similarity
    similarity = (image_embeds @ text_embeds.t()).squeeze(0)
    
    # Softmax
    probs = F.softmax(similarity, dim=0)
    
    return probs

# ì‚¬ìš© ì˜ˆì‹œ
class_names = ["dog", "cat", "bird", "car"]
probs = zero_shot_classify(model, image, class_names)

for name, prob in zip(class_names, probs):
    print(f"{name}: {prob:.2%}")
```

---

#### ìµœì‹  VLM: BLIP, LLaVA
```python
# BLIP ì‚¬ìš© ì˜ˆì‹œ
from transformers import BlipProcessor, BlipForConditionalGeneration

# Model & Processor
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Image Captioning
from PIL import Image

image = Image.open("example.jpg")
inputs = processor(image, return_tensors="pt")

# Generate caption
generated_ids = model.generate(**inputs, max_length=50)
caption = processor.decode(generated_ids[0], skip_special_tokens=True)

print(f"Caption: {caption}")

# Visual Question Answering
question = "What is in the image?"
inputs = processor(image, question, return_tensors="pt")
generated_ids = model.generate(**inputs, max_length=50)
answer = processor.decode(generated_ids[0], skip_special_tokens=True)

print(f"Answer: {answer}")
```
```python
# LLaVA ì‚¬ìš© ì˜ˆì‹œ
from transformers import LlavaForConditionalGeneration, AutoProcessor

model = LlavaForConditionalGeneration.from_pretrained("llava-hf/llava-1.5-7b-hf")
processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")

# Prepare inputs
prompt = "USER: <image>\nWhat is shown in this image?\nASSISTANT:"
inputs = processor(text=prompt, images=image, return_tensors="pt")

# Generate
generated_ids = model.generate(**inputs, max_new_tokens=100)
response = processor.decode(generated_ids[0], skip_special_tokens=True)

print(response)
```

---

**í”„ë¡œì íŠ¸: Mini CLIP**
```python
# ì‘ì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ CLIP í•™ìŠµ

# Dataset
class ImageTextDataset(Dataset):
    def __init__(self, image_paths, captions, transform=None):
        self.image_paths = image_paths
        self.captions = captions
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # Load image
        image = Image.open(self.image_paths[idx]).convert('RGB')
        if self.transform:
            image = self.transform(image)
        
        # Tokenize caption
        caption = self.captions[idx]
        tokens = tokenizer(caption, padding='max_length', max_length=77)
        
        return image, tokens

# Training
def train_mini_clip():
    # Models
    image_encoder = torchvision.models.resnet50(pretrained=True)
    image_encoder.fc = nn.Identity()  # Remove classification head
    
    text_encoder = SimpleTextEncoder()  # Or use BERT
    
    model = CLIP(image_encoder, text_encoder, embed_dim=512)
    
    # Dataset
    dataset = ImageTextDataset(image_paths, captions, transform)
    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
    
    # Training
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    for epoch in range(30):
        for images, texts in dataloader:
            logits_per_image, logits_per_text = model(images, texts)
            loss = contrastive_loss(logits_per_image, logits_per_text)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
    
    return model
```

---

**ìë£Œ:**
- CLIP ë…¼ë¬¸: "Learning Transferable Visual Models From Natural Language Supervision"
- BLIP ë…¼ë¬¸
- LLaVA ë…¼ë¬¸

**ì‹œê°„: ì£¼ 6-8ì‹œê°„**

---

## Month 6: Imitation Learning & RL ê¸°ì´ˆ

### Week 1-2: Imitation Learning ì‹¬í™”

#### Distribution Shift ë¬¸ì œ
```python
"""
Behavioral Cloningì˜ ê·¼ë³¸ì  í•œê³„:

Expert trajectory: s0 â†’ s1 â†’ s2 â†’ s3 (goal)

Learned policyê°€ s1ì—ì„œ ì•½ê°„ ë²—ì–´ë‚¨:
s0 â†’ s1' â†’ ?

Expert dataì—ëŠ” s1'ì—ì„œì˜ í–‰ë™ì´ ì—†ìŒ!
â†’ ëª¨ë¸ì´ ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¦„
â†’ ì—ëŸ¬ê°€ ëˆ„ì ë¨ (compounding error)

í•´ê²°ì±…: DAgger, Behavior Regularization
"""
```

---

#### DAgger (Dataset Aggregation)
```python
class DAgger:
    """
    Interactive Imitation Learning
    
    ê³¼ì •:
    1. BCë¡œ policy í•™ìŠµ
    2. Policyë¡œ rollout
    3. Expertê°€ correction ì œê³µ
    4. ìƒˆ ë°ì´í„° ì¶”ê°€
    5. ë°˜ë³µ
    """
    
    def __init__(self, expert, learner, env):
        self.expert = expert
        self.learner = learner
        self.env = env
        self.dataset = []
    
    def collect_expert_data(self, num_episodes):
        """
        Expert demonstration ìˆ˜ì§‘
        """
        for _ in range(num_episodes):
            episode = []
            state = self.env.reset()
            done = False
            
            while not done:
                action = self.expert.get_action(state)
                next_state, reward, done, _ = self.env.step(action)
                
                episode.append((state, action))
                state = next_state
            
            self.dataset.extend(episode)
        
        print(f"Collected {len(self.dataset)} expert transitions")
    
    def train_iteration(self, num_epochs=10):
        """
        í•œ iterationì˜ DAgger
        """
        # 1. Train policy on current dataset
        self.learner.train(self.dataset, num_epochs)
        
        # 2. Rollout with learned policy
        new_data = []
        num_rollouts = 10
        
        for _ in range(num_rollouts):
            state = self.env.reset()
            done = False
            
            while not done:
                # Learner's action
                learner_action = self.learner.get_action(state)
                
                # But ask expert what to do
                expert_action = self.expert.get_action(state)
                
                # Save (state, expert_action)
                new_data.append((state, expert_action))
                
                # Execute learner's action
                state, _, done, _ = self.env.step(learner_action)
        
        # 3. Add to dataset
        self.dataset.extend(new_data)
        
        print(f"Added {len(new_data)} corrections")
        print(f"Total dataset size: {len(self.dataset)}")
    
    def run(self, num_iterations=10):
        """
        Complete DAgger training
        """
        # Initial expert data
        self.collect_expert_data(num_episodes=50)
        
        # DAgger iterations
        for i in range(num_iterations):
            print(f"\n=== DAgger Iteration {i+1}/{num_iterations} ===")
            self.train_iteration()
            
            # Evaluate
            success_rate = self.evaluate()
            print(f"Success rate: {success_rate:.2%}")
            
            if success_rate > 0.9:
                print("Converged!")
                break
    
    def evaluate(self, num_episodes=10):
        """
        Evaluate learned policy
        """
        successes = 0
        
        for _ in range(num_episodes):
            state = self.env.reset()
            done = False
            
            while not done:
                action = self.learner.get_action(state)
                state, reward, done, info = self.env.step(action)
                
                if info.get('success'):
                    successes += 1
                    break
        
        return successes / num_episodes
```

---

#### Behavior Regularization
```python
class BehaviorRegularizedPolicy(nn.Module):
    """
    Policyë¥¼ reference policyì—ì„œ ë„ˆë¬´ ë©€ì–´ì§€ì§€ ì•Šë„ë¡ ì œì•½
    
    Loss = BC_loss + Î² * KL(Ï€ || Ï€_ref)
    """
    
    def __init__(self, policy, reference_policy, beta=0.1):
        super().__init__()
        self.policy = policy
        self.reference_policy = reference_policy
        self.beta = beta
        
        # Freeze reference policy
        for param in self.reference_policy.parameters():
            param.requires_grad = False
    
    def compute_loss(self, obs, actions):
        # Standard BC loss
        pred_actions = self.policy(obs)
        bc_loss = F.mse_loss(pred_actions, actions)
        
        # KL divergence with reference policy
        ref_actions = self.reference_policy(obs).detach()
        kl_loss = F.mse_loss(pred_actions, ref_actions)
        
        # Combined loss
        total_loss = bc_loss + self.beta * kl_loss
        
        return total_loss, bc_loss, kl_loss
    
    def train_step(self, dataloader, optimizer):
        self.policy.train()
        
        total_loss = 0
        total_bc = 0
        total_kl = 0
        
        for obs, actions in dataloader:
            loss, bc_loss, kl_loss = self.compute_loss(obs, actions)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            total_bc += bc_loss.item()
            total_kl += kl_loss.item()
        
        n = len(dataloader)
        return total_loss/n, total_bc/n, total_kl/n
```

---

#### Inverse Reinforcement Learning (ê°œë…)
```python
"""
Inverse RLì˜ ì•„ì´ë””ì–´:

ê¸°ì¡´ IL: Expertì˜ actionì„ ì§ì ‘ ëª¨ë°©
IRL: Expertì˜ ëª©ì (reward function)ì„ ì¶”ë¡ 

ì˜ˆì‹œ:
- Expertê°€ ì™œ ì´ ê²½ë¡œë¥¼ ì„ íƒí–ˆì„ê¹Œ?
- ì–´ë–¤ rewardë¥¼ ìµœëŒ€í™”í•˜ë ¤ëŠ” ê±¸ê¹Œ?

ì¥ì :
- ë” robustí•œ generalization
- Transfer learning ìš©ì´

ë‹¨ì :
- ê³„ì‚° ë¹„ìš© ë†’ìŒ
- êµ¬í˜„ ë³µì¡

VLAì—ì„œì˜ í™œìš©:
- ì§ì ‘ ì‚¬ìš©ë³´ë‹¤ëŠ” ì•„ì´ë””ì–´ ì°¨ìš©
- Reward designì— insight
- Preference learning
"""

class MaximumEntropyIRL:
    """
    Maximum Entropy IRL (ê°œë…ë§Œ)
    """
    
    def __init__(self, env):
        self.env = env
        self.reward = nn.Linear(state_dim, 1)  # Learned reward
    
    def infer_reward(self, expert_trajectories):
        """
        Expertê°€ maximizeí•˜ëŠ” reward ì¶”ë¡ 
        
        ê³¼ì •:
        1. í˜„ì¬ rewardë¡œ optimal policy ê³„ì‚°
        2. Policyë¡œ trajectories ìƒì„±
        3. Expertì™€ learnedì˜ feature ë¶„í¬ ë¹„êµ
        4. Reward ì—…ë°ì´íŠ¸
        5. ë°˜ë³µ
        """
        for iteration in range(num_iterations):
            # 1. Compute optimal policy under current reward
            policy = self.compute_optimal_policy(self.reward)
            
            # 2. Sample trajectories
            learned_trajs = self.sample_trajectories(policy)
            
            # 3. Compare feature distributions
            expert_features = self.compute_features(expert_trajectories)
            learned_features = self.compute_features(learned_trajs)
            
            # 4. Update reward (gradient ascent)
            feature_diff = expert_features - learned_features
            self.reward.weight += learning_rate * feature_diff
            
            print(f"Iteration {iteration+1}, Reward updated")
        
        return self.reward
```

---

**ìë£Œ:**
- "A Reduction of Imitation Learning" (DAgger ë…¼ë¬¸)
- CS285 Lecture 2-3 (Imitation Learning)

**ì‹œê°„: ì£¼ 6-8ì‹œê°„**

---

### Week 3-4: RL ê¸°ì´ˆ (ìµœì†Œí•œ)

#### MDPì™€ Policy Gradient
```python
"""
Markov Decision Process (MDP):
- State (s): í™˜ê²½ì˜ ìƒíƒœ
- Action (a): Agentì˜ í–‰ë™
- Reward (r): ì¦‰ê°ì  ë³´ìƒ
- Transition (P): s, a â†’ s'
- Policy (Ï€): s â†’ a

ëª©í‘œ: Expected return ìµœëŒ€í™”
J(Ï€) = E[Î£ Î³^t * r_t]

VLAë¥¼ RL ê´€ì ì—ì„œ:
- State: Robot observation (ì´ë¯¸ì§€, proprio)
- Action: Robot control command
- Reward: Task success
- Policy: VLA model ìì²´!
"""
```

---

#### REINFORCE Algorithm
```python
class PolicyGradient:
    """
    REINFORCE: ê°€ì¥ ê¸°ë³¸ì ì¸ Policy Gradient
    
    í•µì‹¬ ì•„ì´ë””ì–´:
    âˆ‡J(Ï€) = E[âˆ‡log Ï€(a|s) * R]
    
    â†’ ë†’ì€ returnì„ ë°›ì€ actionì˜ í™•ë¥  ì¦ê°€
    """
    
    def __init__(self, policy):
        self.policy = policy
        self.optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)
    
    def compute_returns(self, rewards, gamma=0.99):
        """
        Discounted returns ê³„ì‚°
        
        R_t = r_t + Î³*r_{t+1} + Î³Â²*r_{t+2} + ...
        """
        returns = []
        R = 0
        
        for r in reversed(rewards):
            R = r + gamma * R
            returns.insert(0, R)
        
        returns = torch.tensor(returns)
        
        # Normalize (í•™ìŠµ ì•ˆì •í™”)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        return returns
    
    def update(self, trajectory):
        """
        Policy gradient update
        """
        states, actions, rewards = trajectory
        
        # Compute returns
        returns = self.compute_returns(rewards)
        
        # Compute log probabilities
        log_probs = []
        for state, action in zip(states, actions):
            action_dist = self.policy(state)
            log_prob = action_dist.log_prob(action)
            log_probs.append(log_prob)
        
        log_probs = torch.stack(log_probs)
        
        # Policy gradient loss
        loss = -(log_probs * returns).mean()
        
        # Update
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def train(self, env, num_episodes=1000):
        """
        Complete training loop
        """
        for episode in range(num_episodes):
            # Collect trajectory
            trajectory = self.collect_trajectory(env)
            
            # Update policy
            loss = self.update(trajectory)
            
            # Log
            episode_return = sum(trajectory[2])
            print(f"Episode {episode+1}, Return: {episode_return:.2f}, Loss: {loss:.4f}")
    
    def collect_trajectory(self, env):
        """
        í™˜ê²½ì—ì„œ trajectory ìˆ˜ì§‘
        """
        states, actions, rewards = [], [], []
        
        state = env.reset()
        done = False
        
        while not done:
            # Sample action from policy
            action_dist = self.policy(torch.FloatTensor(state))
            action = action_dist.sample()
            
            # Execute
            next_state, reward, done, _ = env.step(action.numpy())
            
            # Record
            states.append(torch.FloatTensor(state))
            actions.append(action)
            rewards.append(reward)
            
            state = next_state
        
        return states, actions, rewards

# Gaussian Policy (for continuous actions)
class GaussianPolicy(nn.Module):
    """
    Continuous actionì„ ìœ„í•œ Gaussian policy
    """
    def __init__(self, state_dim, action_dim):
        super().__init__()
        
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        
        self.mean = nn.Linear(128, action_dim)
        self.log_std = nn.Parameter(torch.zeros(action_dim))
    
    def forward(self, state):
        features = self.net(state)
        mean = self.mean(features)
        std = self.log_std.exp()
        
        return torch.distributions.Normal(mean, std)
```

---

#### PPO ê°œë…ë§Œ
```python
"""
Proximal Policy Optimization (PPO)

REINFORCEì˜ ë¬¸ì œì :
- í•™ìŠµ ë¶ˆì•ˆì • (step size ì¡°ì ˆ ì–´ë ¤ì›€)
- Sample efficiency ë‚®ìŒ

PPOì˜ í•´ê²°ì±…:
1. Importance samplingìœ¼ë¡œ old policy ì¬ì‚¬ìš©
2. Clippingìœ¼ë¡œ policy update ì œí•œ
3. Multiple epochs í•™ìŠµ

í•µì‹¬ ì•„ì´ë””ì–´:
- Policyë¥¼ í¬ê²Œ ë°”ê¾¸ì§€ ì•Šìœ¼ë©´ì„œ ê°œì„ 
- Trust region ê°œë…

Loss:
L = min(
    ratio * advantage,
    clip(ratio, 1-Îµ, 1+Îµ) * advantage
)

where ratio = Ï€_new(a|s) / Ï€_old(a|s)
"""

class PPO:
    """
    PPO ê°œë… ì½”ë“œ (ê°„ì†Œí™”)
    ì‹¤ì œ êµ¬í˜„ì€ ë” ë³µì¡
    """
    
    def __init__(self, policy, clip_epsilon=0.2):
        self.policy = policy
        self.clip_epsilon = clip_epsilon
        self.optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)
    
    def compute_loss(self, states, actions, old_log_probs, advantages):
        """
        PPO clipped objective
        """
        # New log probs
        action_dist = self.policy(states)
        new_log_probs = action_dist.log_prob(actions)
        
        # Importance sampling ratio
        ratio = torch.exp(new_log_probs - old_log_probs)
        
        # Clipped objective
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 
                           1 - self.clip_epsilon, 
                           1 + self.clip_epsilon) * advantages
        
        # Take minimum (pessimistic bound)
        loss = -torch.min(surr1, surr2).mean()
        
        return loss
    
    def update(self, rollouts, num_epochs=4):
        """
        PPO update with multiple epochs
        """
        states, actions, old_log_probs, advantages = rollouts
        
        for epoch in range(num_epochs):
            loss = self.compute_loss(states, actions, old_log_probs, advantages)
            
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
        
        return loss.item()

"""
VLAì—ì„œ PPO í™œìš©:
- BCë¡œ ì´ˆê¸° policy í•™ìŠµ
- PPOë¡œ fine-tuning
- ì˜ˆ: RT-2-Xê°€ ì´ ë°©ì‹

í•˜ì§€ë§Œ:
- Phase 2ì—ì„œëŠ” BCë§Œìœ¼ë¡œ ì¶©ë¶„
- RLì€ "ìˆìœ¼ë©´ ì¢‹ì€" ì •ë„
- Phase 3ì—ì„œ ì„ íƒì ìœ¼ë¡œ ì ìš©
"""
```

---

**VLA + RL Fine-tuning ì „ëµ**
```python
class VLAwithRLFinetuning:
    """
    VLAë¥¼ RLë¡œ fine-tuningí•˜ëŠ” ì „ëµ
    """
    
    def __init__(self, vla_model, env):
        self.vla = vla_model
        self.env = env
        
        # RL algorithm (PPO ë˜ëŠ” SAC)
        self.rl_optimizer = PPO(vla_model)
    
    def stage1_bc_pretraining(self, expert_data):
        """
        Stage 1: BCë¡œ ì´ˆê¸° policy í•™ìŠµ
        """
        print("Stage 1: BC Pre-training")
        
        for epoch in range(100):
            for obs, actions in expert_data:
                pred_actions = self.vla(obs)
                loss = F.mse_loss(pred_actions, actions)
                
                # Update
                loss.backward()
                # ...
        
        print("BC pre-training done!")
        print(f"Success rate: {self.evaluate():.2%}")
    
    def stage2_rl_finetuning(self, num_iterations=1000):
        """
        Stage 2: RLë¡œ self-improvement
        """
        print("\nStage 2: RL Fine-tuning")
        
        for iteration in range(num_iterations):
            # Collect rollouts
            rollouts = self.collect_rollouts()
            
            # PPO update
            loss = self.rl_optimizer.update(rollouts)
            
            # Evaluate
            if iteration % 10 == 0:
                success_rate = self.evaluate()
                print(f"Iteration {iteration}, Success: {success_rate:.2%}")
        
        print("RL fine-tuning done!")
    
    def collect_rollouts(self):
        """
        í™˜ê²½ê³¼ interaction
        """
        # Collect trajectories using current policy
        # ...
        pass
    
    def evaluate(self):
        """
        Evaluation
        """
        # Test in environment
        # ...
        pass

# ì‚¬ìš© ì˜ˆì‹œ
"""
1. Expert dataë¡œ BC í•™ìŠµ (ì•ˆì •ì  baseline)
2. í™˜ê²½ì—ì„œ self-playë¡œ RL fine-tuning
3. ì„±ëŠ¥ ê°œì„  (BC 70% â†’ RL 85%)

ì£¼ì˜:
- RLì€ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
- BC baselineì´ ì¤‘ìš”
- Taskì— ë”°ë¼ íš¨ê³¼ ë‹¤ë¦„
"""
```

---

**ìë£Œ:**
- Spinning Up in Deep RL (OpenAI)
- CS285 Lecture 4-5 (Policy Gradient)

**ì‹œê°„: ì£¼ 6-8ì‹œê°„**

---

## ìˆ˜í•™ ê¸°ì´ˆ (Phase 2 ì „ì²´ ë³‘í–‰)

### Linear Algebra

**ì§„í–‰ ì¤‘ì¸ 3Blue1Brown ì‹œë¦¬ì¦ˆ ì™„ì£¼**

ì¶”ê°€ í•™ìŠµ:
- Gilbert Strangì˜ Linear Algebra (MIT OCW)
- í•µì‹¬ ì£¼ì œ:
  * Eigenvalues/Eigenvectors (PCA, spectral methods)
  * SVD (ë°ì´í„° ì••ì¶•, ì¶”ì²œ ì‹œìŠ¤í…œ)
  * Matrix decomposition

**ì‹œê°„: ì£¼ 2-3ì‹œê°„**

---

### Probability & Statistics

VLAì— í•„ìš”í•œ í™•ë¥ ë¡ :
- í™•ë¥  ë¶„í¬ (Gaussian, Categorical)
- ê¸°ëŒ“ê°’, ë¶„ì‚°
- Bayes' theorem
- Maximum Likelihood Estimation

ìë£Œ:
- "Probability for Machine Learning" (Chris Bishop)
- Khan Academy Statistics

**ì‹œê°„: ì£¼ 2-3ì‹œê°„**

---

## Phase 2 ì™„ë£Œ ì²´í¬
```
âœ… Deep Learning ê¸°ì´ˆ íƒ„íƒ„ (80%)
âœ… PyTorch ììœ ìì¬ (85%)
âœ… CNN ì™„ì „ ì´í•´ (80%)
âœ… Transformer ì™„ì „ ì´í•´ (90%)
âœ… Multi-modal learning ì´í•´ (80%)
âœ… Imitation Learning ì‹¬í™” (70%)
âœ… RL ê¸°ì´ˆ ì´í•´ (60%)
âœ… ìˆ˜í•™ ê¸°ì´ˆ ì¶©ë¶„ (70%)
âœ… ë…¼ë¬¸ ì½ê¸° ìˆ˜ì›”í•¨

â†’ ë³¸ê²© VLA í”„ë¡œì íŠ¸ ì¤€ë¹„ ì™„ë£Œ!
```